This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
argus_track/
  core/
    __init__.py
    detection.py
    gps.py
    stereo.py
    track.py
  detectors/
    __init__.py
    base.py
    yolov11.py
  filters/
    __init__.py
    kalman.py
  stereo/
    __init__.py
    calibration.py
    matching.py
    triangulation.py
  trackers/
    __init__.py
    lightpost_tracker.py
    simplified_lightpost_tracker.py
    stereo_lightpost_tracker.py
  utils/
    __init__.py
    gps_extraction.py
    gps_movement_filter.py
    gps_sync_tracker.py
    gps_utils.py
    io.py
    iou.py
    kalman_gps_filter.py
    motion_compensation.py
    motion_prediction.py
    output_manager.py
    overlap_fixer.py
    performance.py
    smart_track_manager.py
    static_car_detector.py
    visual_feature_extractor.py
    visualization.py
  __init__.py
  __version__.py
  bytetrack_custom.yaml
  config.py
  main.py
  requirements.txt
images/
  bytetrack-workflow-diagram.svg
.gitignore
.repomixignore
LICENSE
README.md
repomix.config.json
run_argus.sh
setup.py
tracker_config.yaml

================================================================
Files
================================================================

================
File: argus_track/trackers/simplified_lightpost_tracker.py
================
# argus_track/trackers/simplified_lightpost_tracker.py (NEW FILE)

"""
Simplified Light Post Tracker - Monocular tracking with ID consolidation
No depth estimation, no geolocation - just tracking with frame data output
"""

import cv2
import numpy as np
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

from ultralytics import YOLO
from ..config import TrackerConfig
from ..core import Detection, GPSData
from ..utils.smart_track_manager import SmartTrackManager
from ..utils.output_manager import OutputManager
from ..utils.gps_sync_tracker import GPSSynchronizer
from ..utils.static_car_detector import StaticCarDetector, StaticCarConfig
from ..utils.visualization import RealTimeVisualizer
from ..utils.overlap_fixer import OverlapFixer

class SimplifiedLightPostTracker:
    """
    Simplified Light Post Tracker for monocular tracking
    
    Features:
    - Single camera processing (no stereo)
    - GPS frame synchronization (every 6th frame)
    - Smart track ID consolidation and reappearance detection
    - JSON + CSV output (no geolocation)
    - Real-time visualization (GPS frames only)
    """

    def __init__(self, 
                config: TrackerConfig,
                model_path: str,
                show_realtime: bool = False,
                display_size: Tuple[int, int] = (1280, 720)):
        """
        Initialize enhanced simplified tracker with motion prediction and visual features
        """
        self.config = config
        self.model_path = model_path
        self.show_realtime = show_realtime
        self.display_size = display_size
        
        # Initialize logger
        self.logger = logging.getLogger(f"{__name__}.SimplifiedLightPostTracker")
        
        # Initialize YOLO model
        self.model = YOLO(model_path)
        
        # Get class names from model
        self.class_names = list(self.model.names.values())
        self.logger.info(f"Model classes: {self.class_names}")
        
        # Initialize ENHANCED smart track manager with motion prediction and visual features
        from .smart_track_manager import EnhancedSmartTrackManager
        self.track_manager = EnhancedSmartTrackManager(
            config=config,
            max_memory_age=300,
            min_detection_count=3,
            similarity_threshold=50.0
        )
        
        # Initialize overlap fixer (from your original solution)
        self.overlap_fixer = OverlapFixer(
            overlap_threshold=0.5,
            distance_threshold=3.0
        )
        
        # Initialize GPS synchronizer (will be set during processing)
        self.gps_synchronizer: Optional[GPSSynchronizer] = None
        
        # Initialize static car detector
        self.static_car_detector = None
        if config.enable_static_car_detection:
            static_config = StaticCarConfig(
                movement_threshold_meters=config.static_movement_threshold_m,
                stationary_time_threshold=config.static_time_threshold_s,
                gps_frame_interval=config.gps_frame_interval
            )
            self.static_car_detector = StaticCarDetector(static_config)
            self.logger.info("Static car detection enabled")
        
        # Initialize real-time visualizer
        self.visualizer = None
        if show_realtime:
            self.visualizer = RealTimeVisualizer(
                window_name="Enhanced Simplified Light Post Tracking (GPS + Motion + Visual)",
                display_size=display_size,
                show_info_panel=True
            )
            self.logger.info("Real-time visualization enabled (GPS frames only)")
        
        # Processing statistics
        self.processing_times = []
        self.frame_count = 0
        self.processed_frame_count = 0
        
        # GPS motion tracking
        self.previous_gps = None
        self.gps_motion_history = []
        
        self.logger.info("Enhanced Simplified Light Post Tracker initialized")
        self.logger.info(f"GPS frame interval: {config.gps_frame_interval}")
        self.logger.info(f"Motion prediction enabled: {config.track_consolidation.enable_motion_prediction}")
        self.logger.info(f"Visual features enabled: {config.track_consolidation.enable_visual_features}")
        self.logger.info(f"Track consolidation enabled: {config.track_consolidation.enable_id_consolidation}")

    def _process_frame(self,
                    frame: np.ndarray,
                    frame_id: int,
                    timestamp: float,
                    gps_data: Optional[GPSData]) -> List[Detection]:
        """Enhanced frame processing with motion prediction and visual features"""
        
        # Calculate GPS motion if available
        gps_motion_matrix = None
        vehicle_speed = 0.0
        
        if gps_data and self.previous_gps and self.track_manager.motion_predictor:
            gps_motion_matrix = self.track_manager.motion_predictor.estimate_motion_from_gps_enhanced(
                gps_data, self.previous_gps
            )
            
            # Calculate vehicle speed
            dt = gps_data.timestamp - self.previous_gps.timestamp
            if dt > 0:
                R = 6378137.0
                lat1, lon1 = np.radians(self.previous_gps.latitude), np.radians(self.previous_gps.longitude)
                lat2, lon2 = np.radians(gps_data.latitude), np.radians(gps_data.longitude)
                
                dlat = lat2 - lat1
                dlon = lon2 - lon1
                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
                c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
                distance = R * c
                vehicle_speed = distance / dt
        
        # Store current GPS for next frame
        self.previous_gps = gps_data
        
        # Run YOLO tracking
        track_params = self.config.get_ultralytics_track_params()
        results = self.model.track(frame, **track_params)
        
        # STEP 1: Apply overlap fixer first
        fixed_detections = self.overlap_fixer.fix_ultralytics_results(
            results[0], gps_data, frame_id
        )
        
        # STEP 2: Convert to Detection objects
        raw_detections = self._convert_fixed_detections(fixed_detections, frame_id)
        
        # STEP 3: Apply ENHANCED smart track management with all features
        processed_detections = self.track_manager.process_frame_detections(
            raw_detections, 
            frame_id, 
            timestamp,
            frame,      # For visual feature extraction and camera motion estimation
            gps_data    # For GPS-enhanced processing
        )
        
        # STEP 4: Apply GPS motion compensation if available
        if (gps_motion_matrix is not None and 
            vehicle_speed > 1.0 and 
            self.track_manager.motion_predictor):
            
            # Convert detections to tracks for compensation
            temp_tracks = []
            for detection in processed_detections:
                if hasattr(detection, 'track_id'):
                    from ..core import Track
                    track = Track(
                        track_id=detection.track_id,
                        detections=[detection],
                        state='confirmed'
                    )
                    temp_tracks.append(track)
            
            # Apply GPS motion compensation
            compensated_tracks = self.track_manager.motion_predictor.compensate_tracks_for_gps_motion(
                temp_tracks, gps_motion_matrix, vehicle_speed
            )
            
            # Convert back to detections
            for i, track in enumerate(compensated_tracks):
                if i < len(processed_detections):
                    compensated_bbox = track.to_tlbr()
                    processed_detections[i].bbox = compensated_bbox
                    processed_detections[i].motion_compensated = True
            
            self.logger.debug(f"Frame {frame_id}: Applied GPS motion compensation (speed: {vehicle_speed:.1f} m/s)")
        
        return processed_detections

    def _visualize_frame(self,
                        frame: np.ndarray,
                        detections: List[Detection],
                        frame_id: int,
                        gps_data: Optional[GPSData],
                        total_frames: int,
                        processed_frames: int) -> bool:
        """Enhanced visualization with motion and visual feature info"""
        
        # Get enhanced statistics
        track_stats = self.track_manager.get_statistics()
        
        # Prepare enhanced frame info
        frame_info = {
            'frame_idx': frame_id,
            'total_frames': total_frames,
            'processed_frames': processed_frames,
            'gps_available': gps_data is not None,
            'track_consolidations': track_stats['total_consolidations'],
            'track_reappearances': track_stats['total_reappearances'],
            # Enhanced info
            'motion_prediction_enabled': track_stats['motion_prediction']['motion_predictor_enabled'],
            'visual_features_enabled': track_stats['visual_features']['feature_extraction_enabled'],
            'camera_motion_detected': track_stats['motion_prediction']['camera_motion_detected'],
            'tracks_with_features': track_stats['visual_features']['tracks_with_features'],
            'avg_prediction_accuracy': track_stats['motion_prediction']['avg_prediction_accuracy'],
            'avg_appearance_stability': track_stats['visual_features']['avg_appearance_stability']
        }
        
        # Prepare GPS info with vehicle motion
        gps_info = None
        if gps_data:
            vehicle_speed = 0.0
            if self.previous_gps:
                dt = gps_data.timestamp - self.previous_gps.timestamp
                if dt > 0:
                    # Calculate speed for display
                    R = 6378137.0
                    lat1, lon1 = np.radians(self.previous_gps.latitude), np.radians(self.previous_gps.longitude)
                    lat2, lon2 = np.radians(gps_data.latitude), np.radians(gps_data.longitude)
                    
                    dlat = lat2 - lat1
                    dlon = lon2 - lon1
                    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
                    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
                    distance = R * c
                    vehicle_speed = distance / dt
            
            gps_info = {
                'latitude': gps_data.latitude,
                'longitude': gps_data.longitude,
                'heading': gps_data.heading,
                'accuracy': gps_data.accuracy,
                'vehicle_speed_ms': vehicle_speed,
                'vehicle_speed_kmh': vehicle_speed * 3.6
            }
        
        # Convert detections to tracks for visualization
        tracks = []
        for detection in detections:
            class MockTrack:
                def __init__(self, detection):
                    self.track_id = getattr(detection, 'track_id', 0)
                    self.state = 'confirmed'
                    self.detections = [detection]
                    self.hits = 1
                    self.age = 1
                    self.time_since_update = 0
                    self._bbox = detection.bbox
                    # Enhanced attributes
                    self.motion_compensated = getattr(detection, 'motion_compensated', False)
                    self.prediction_match = getattr(detection, 'prediction_match', False)
                    self.match_score = getattr(detection, 'match_score', 0.0)
                
                def to_tlbr(self):
                    return self._bbox
            
            track = MockTrack(detection)
            tracks.append(track)
        
        # Visualize with enhanced info
        return self.visualizer.visualize_frame(
            frame, detections, tracks, gps_info, frame_info
        )

    def _create_visualization_frame(self,
                                frame: np.ndarray,
                                detections: List[Detection],
                                gps_data: Optional[GPSData]) -> np.ndarray:
        """Enhanced visualization frame with motion and feature info"""
        
        vis_frame = frame.copy()
        
        # Draw detections with enhanced info
        for detection in detections:
            bbox = detection.bbox.astype(int)
            class_name = self.class_names[detection.class_id] if detection.class_id < len(self.class_names) else f"class_{detection.class_id}"
            
            # Choose color based on detection type
            if getattr(detection, 'motion_compensated', False):
                color = (0, 255, 255)  # Cyan for motion compensated
            elif getattr(detection, 'prediction_match', False):
                color = (255, 0, 255)  # Magenta for prediction match
            elif getattr(detection, 'reappearance_match', False):
                color = (0, 165, 255)  # Orange for reappearance
            else:
                color = (0, 255, 0)    # Green for normal
            
            # Draw bounding box
            cv2.rectangle(vis_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
            
            # Enhanced label with motion/feature info
            label_parts = [f"ID:{getattr(detection, 'track_id', '?')}"]
            label_parts.append(class_name)
            label_parts.append(f"{detection.score:.2f}")
            
            if getattr(detection, 'motion_compensated', False):
                label_parts.append("MC")  # Motion Compensated
            if getattr(detection, 'prediction_match', False):
                match_score = getattr(detection, 'match_score', 0)
                label_parts.append(f"P:{match_score:.2f}")  # Prediction match
            
            label = " ".join(label_parts)
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            
            # Background for text
            cv2.rectangle(vis_frame, 
                        (bbox[0], bbox[1] - label_size[1] - 10),
                        (bbox[0] + label_size[0], bbox[1]),
                        color, -1)
            
            # Text
            cv2.putText(vis_frame, label, (bbox[0], bbox[1] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        # Enhanced GPS and motion info overlay
        if gps_data:
            # Calculate vehicle speed
            vehicle_speed = 0.0
            if hasattr(self, 'previous_gps') and self.previous_gps:
                dt = gps_data.timestamp - self.previous_gps.timestamp
                if dt > 0:
                    R = 6378137.0
                    lat1, lon1 = np.radians(self.previous_gps.latitude), np.radians(self.previous_gps.longitude)
                    lat2, lon2 = np.radians(gps_data.latitude), np.radians(gps_data.longitude)
                    dlat = lat2 - lat1
                    dlon = lon2 - lon1
                    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
                    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
                    distance = R * c
                    vehicle_speed = distance / dt
            
            # GPS info
            gps_text = f"GPS: {gps_data.latitude:.6f}, {gps_data.longitude:.6f}"
            cv2.putText(vis_frame, gps_text, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
            
            # Vehicle speed
            speed_text = f"Speed: {vehicle_speed * 3.6:.1f} km/h"
            cv2.putText(vis_frame, speed_text, (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # Detection count with enhancement info
        motion_compensated = len([d for d in detections if getattr(d, 'motion_compensated', False)])
        prediction_matches = len([d for d in detections if getattr(d, 'prediction_match', False)])
        
        det_text = f"Det: {len(detections)} (MC:{motion_compensated}, P:{prediction_matches})"
        cv2.putText(vis_frame, det_text, (10, 90),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # Enhanced track statistics
        track_stats = self.track_manager.get_statistics()
        stats_text = f"Tracks: {track_stats['active_tracks']} | Features: {track_stats['visual_features']['tracks_with_features']} | Pred: {track_stats['motion_prediction']['avg_prediction_accuracy']:.2f}"
        cv2.putText(vis_frame, stats_text, (10, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        
        return vis_frame

    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """Get comprehensive enhanced tracking statistics"""
        base_stats = self.get_statistics()
        track_stats = self.track_manager.get_statistics()
        
        enhanced_stats = {
            **base_stats,
            'enhanced_features': {
                'motion_prediction': track_stats['motion_prediction'],
                'visual_features': track_stats['visual_features'],
                'gps_enhanced_motion': track_stats.get('gps_enhanced', {}),
                'track_consolidation': {
                    'total_consolidations': track_stats['total_consolidations'],
                    'total_reappearances': track_stats['total_reappearances'],
                    'recovery_rate': track_stats['recovery_rate']
                }
            },
            'performance_metrics': {
                'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,
                'gps_frames_processed': len([1 for _ in self.processing_times]),  # Simplified
                'motion_compensation_active': track_stats['motion_prediction']['camera_motion_detected'],
                'visual_matching_active': track_stats['visual_features']['feature_extraction_enabled']
            }
        }
        
        return enhanced_stats

    def _add_info_panel(self, frame_info: Dict) -> Dict:
        """Add motion prediction info to visualization panel"""
        enhanced_info = frame_info.copy()
        
        # Add motion prediction statistics
        motion_stats = self.track_manager.get_motion_statistics()
        
        enhanced_info.update({
            'motion_detected': motion_stats.get('motion_detection', {}).get('motion_detected', False),
            'prediction_matches': motion_stats.get('prediction_matches', 0),
            'camera_motion': motion_stats.get('motion_detection', {}).get('avg_translation', 0),
            'motion_compensation': 'ACTIVE' if motion_stats.get('motion_detection', {}).get('motion_detected') else 'INACTIVE'
        })
        
        return enhanced_info

    def process_video(self,
                     video_path: str,
                     gps_data: Optional[List[GPSData]] = None,
                     output_path: Optional[str] = None,
                     save_results: bool = True) -> Dict[str, Any]:
        """
        Process video with simplified tracking
        
        Args:
            video_path: Path to input video
            gps_data: Optional GPS data for synchronization
            output_path: Optional output video path
            save_results: Whether to save results
            
        Returns:
            Processing results dictionary
        """
        self.logger.info(f"Starting simplified tracking: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Could not open video: {video_path}")
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        self.logger.info(f"Video: {total_frames} frames, {fps:.1f} FPS, {width}x{height}")
        
        # Initialize GPS synchronizer
        if gps_data:
            self.gps_synchronizer = GPSSynchronizer(gps_data, fps, gps_fps=10.0)
            sync_stats = self.gps_synchronizer.get_processing_statistics()
            self.logger.info(f"GPS sync: {sync_stats['sync_frames']} frames to process")
        else:
            self.logger.warning("No GPS data - processing all frames")
        
        # Initialize output manager
        output_manager = OutputManager(video_path, self.class_names)
        
        # Setup video writer (if requested)
        out_writer = None
        if output_path and self.show_realtime:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            # Write only GPS frames, so use GPS frequency for output
            output_fps = fps / self.config.gps_frame_interval if self.gps_synchronizer else fps
            out_writer = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))
        
        # Processing loop
        current_frame_idx = 0
        processed_frames = 0
        skipped_frames_gps = 0
        skipped_frames_static = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                start_time = time.time()
                
                # Check GPS synchronization
                should_process_gps = True
                current_gps = None
                
                if self.gps_synchronizer:
                    should_process_gps = self.gps_synchronizer.should_process_frame(current_frame_idx)
                    if should_process_gps:
                        current_gps = self.gps_synchronizer.get_gps_for_frame(current_frame_idx)
                    else:
                        skipped_frames_gps += 1
                
                # Check static car detection
                should_process_static = True
                if should_process_gps and self.static_car_detector and current_gps:
                    should_process_static = self.static_car_detector.should_process_frame(
                        current_gps, current_frame_idx
                    )
                    if not should_process_static:
                        skipped_frames_static += 1
                
                # Final decision: process frame?
                should_process = should_process_gps and should_process_static
                
                if not should_process:
                    current_frame_idx += 1
                    continue
                
                # Process frame
                frame_timestamp = current_frame_idx / fps
                detections = self._process_frame(frame, current_frame_idx, frame_timestamp, current_gps)
                
                # Add to output manager
                output_manager.add_frame_data(
                    frame_id=current_frame_idx,
                    timestamp=frame_timestamp,
                    detections=detections,
                    gps_data=current_gps
                )
                
                # Real-time visualization (GPS frames only)
                if self.show_realtime and self.visualizer:
                    should_continue = self._visualize_frame(
                        frame, detections, current_frame_idx, current_gps, 
                        total_frames, processed_frames
                    )
                    
                    if not should_continue:
                        self.logger.info("User requested quit")
                        break
                
                # Save frame to output video
                if out_writer and self.show_realtime:
                    vis_frame = self._create_visualization_frame(frame, detections, current_gps)
                    out_writer.write(vis_frame)
                
                # Performance tracking
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                processed_frames += 1
                
                # Progress logging
                if processed_frames % 30 == 0:  # Every 30 processed frames
                    avg_time = np.mean(self.processing_times[-30:])
                    progress = current_frame_idx / total_frames * 100
                    
                    self.logger.info(
                        f"Progress: {progress:.1f}% | "
                        f"Processed: {processed_frames} | "
                        f"Skipped (GPS): {skipped_frames_gps} | "
                        f"Skipped (Static): {skipped_frames_static} | "
                        f"Avg time: {avg_time*1000:.1f}ms"
                    )
                
                current_frame_idx += 1
                
        except KeyboardInterrupt:
            self.logger.info("Processing interrupted by user")
        except Exception as e:
            self.logger.error(f"Error during processing: {e}")
            raise
        finally:
            # Cleanup
            cap.release()
            if out_writer:
                out_writer.release()
            if self.visualizer:
                self.visualizer.close()
            cv2.destroyAllWindows()
        
        # Processing summary
        total_time = sum(self.processing_times)
        avg_fps = processed_frames / total_time if total_time > 0 else 0
        
        results = {
            'total_frames': total_frames,
            'processed_frames': processed_frames,
            'skipped_frames_gps': skipped_frames_gps,
            'skipped_frames_static': skipped_frames_static,
            'processing_time': total_time,
            'avg_fps': avg_fps,
            'track_manager_stats': self.track_manager.get_statistics(),
            'output_summary': output_manager.get_processing_summary()
        }
        
        self.logger.info("=== PROCESSING COMPLETE ===")
        self.logger.info(f"Total frames: {total_frames}")
        self.logger.info(f"Processed frames: {processed_frames}")
        self.logger.info(f"Processing time: {total_time:.1f}s")
        self.logger.info(f"Average FPS: {avg_fps:.1f}")
        
        # Track manager statistics
        track_stats = self.track_manager.get_statistics()
        self.logger.info(f"Track consolidations: {track_stats['total_consolidations']}")
        self.logger.info(f"Track reappearances: {track_stats['total_reappearances']}")
        
        # Save results
        if save_results:
            json_path, csv_path = output_manager.export_both()
            results['json_output'] = json_path
            results['csv_output'] = csv_path
            
            # Print summary
            output_manager.print_summary()
        
        return results
    
    def _convert_fixed_detections(self, fixed_detections: List[Dict], frame_id: int) -> List[Detection]:
        """Convert fixed detections to Detection objects"""
        detections = []
        
        for det_dict in fixed_detections:
            detection = Detection(
                bbox=np.array(det_dict['bbox']),
                score=float(det_dict['score']),
                class_id=int(det_dict['class_id']),
                frame_id=frame_id
            )
            detection.track_id = int(det_dict['track_id'])
            detections.append(detection)
        
        return detections
    
    def _extract_detections_from_results(self, results, frame_id: int) -> List[Detection]:
        """Extract Detection objects from YOLO results"""
        detections = []
        
        if not results or not results[0].boxes:
            return detections
        
        result = results[0]
        
        # Check if tracking is available
        if not hasattr(result.boxes, 'id') or result.boxes.id is None:
            self.logger.warning(f"Frame {frame_id}: No tracking IDs available")
            return detections
        
        # Extract data
        boxes = result.boxes.xyxy.cpu().numpy()
        scores = result.boxes.conf.cpu().numpy()
        classes = result.boxes.cls.cpu().numpy().astype(int)
        track_ids = result.boxes.id.cpu().numpy().astype(int)
        
        # Create Detection objects
        for i, (box, score, cls_id, track_id) in enumerate(zip(boxes, scores, classes, track_ids)):
            detection = Detection(
                bbox=box,
                score=float(score),
                class_id=int(cls_id),
                frame_id=frame_id
            )
            detection.track_id = int(track_id)  # Add track ID
            detections.append(detection)
        
        return detections
    
    def _visualize_frame(self,
                        frame: np.ndarray,
                        detections: List[Detection],
                        frame_id: int,
                        gps_data: Optional[GPSData],
                        total_frames: int,
                        processed_frames: int) -> bool:
        """Visualize frame with real-time display"""
        
        # Prepare frame info
        frame_info = {
            'frame_idx': frame_id,
            'total_frames': total_frames,
            'processed_frames': processed_frames,
            'gps_available': gps_data is not None,
            'track_consolidations': self.track_manager.get_statistics()['total_consolidations'],
            'track_reappearances': self.track_manager.get_statistics()['total_reappearances']
        }
        
        # Prepare GPS info
        gps_info = None
        if gps_data:
            gps_info = {
                'latitude': gps_data.latitude,
                'longitude': gps_data.longitude,
                'heading': gps_data.heading,
                'accuracy': gps_data.accuracy
            }
        
        # Convert detections to tracks for visualization
        tracks = []
        for detection in detections:
            # Create minimal track object for visualization
            class MockTrack:
                def __init__(self, detection):
                    self.track_id = detection.track_id
                    self.state = 'confirmed'
                    self.detections = [detection]
                    self.hits = 1  # Required by visualizer
                    self.age = 1   # Required by visualizer
                    self.time_since_update = 0  # Required by visualizer
                    self._bbox = detection.bbox
                
                def to_tlbr(self):
                    return self._bbox
            
            track = MockTrack(detection)
            tracks.append(track)
        
        # Visualize
        return self.visualizer.visualize_frame(
            frame, detections, tracks, gps_info, frame_info
        )
    
    def _create_visualization_frame(self,
                                   frame: np.ndarray,
                                   detections: List[Detection],
                                   gps_data: Optional[GPSData]) -> np.ndarray:
        """Create visualization frame for video output"""
        
        vis_frame = frame.copy()
        
        # Draw detections
        for detection in detections:
            bbox = detection.bbox.astype(int)
            class_name = self.class_names[detection.class_id] if detection.class_id < len(self.class_names) else f"class_{detection.class_id}"
            
            # Draw bounding box
            cv2.rectangle(vis_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
            
            # Draw label
            label = f"ID:{detection.track_id} {class_name} {detection.score:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            
            # Background for text
            cv2.rectangle(vis_frame, 
                         (bbox[0], bbox[1] - label_size[1] - 10),
                         (bbox[0] + label_size[0], bbox[1]),
                         (0, 255, 0), -1)
            
            # Text
            cv2.putText(vis_frame, label, (bbox[0], bbox[1] - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        # Add GPS info overlay
        if gps_data:
            gps_text = f"GPS: {gps_data.latitude:.6f}, {gps_data.longitude:.6f}"
            cv2.putText(vis_frame, gps_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # Add detection count
        det_text = f"Detections: {len(detections)}"
        cv2.putText(vis_frame, det_text, (10, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # Add track consolidation info
        track_stats = self.track_manager.get_statistics()
        consol_text = f"Consolidations: {track_stats['total_consolidations']} | Reappearances: {track_stats['total_reappearances']}"
        cv2.putText(vis_frame, consol_text, (10, 90),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        
        return vis_frame
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive tracking statistics"""
        track_stats = self.track_manager.get_statistics()
        
        return {
            'processed_frames': self.processed_frame_count,
            'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,
            'track_manager': track_stats,
            'model_classes': self.class_names
        }

================
File: argus_track/utils/motion_prediction.py
================
"""
Motion Predictor for Enhanced Tracking
=====================================
Provides advanced motion prediction capabilities for object tracking,
including camera motion compensation and track position prediction.
"""

import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import cv2
import logging

from ..core import Detection, Track, GPSData

@dataclass
class CameraMotion:
    """Represents camera motion between frames"""
    translation: np.ndarray  # [dx, dy]
    rotation: float         # rotation angle in radians
    scale: float           # scale factor
    confidence: float      # motion estimation confidence
    timestamp: float       # when motion was detected

@dataclass 
class PredictedPosition:
    """Predicted position for an object"""
    bbox: np.ndarray       # predicted bounding box
    confidence: float      # prediction confidence
    velocity: np.ndarray   # estimated velocity [vx, vy]
    timestamp: float       # prediction timestamp

class MotionPredictor:
    """
    Advanced motion predictor for object tracking with camera motion compensation
    """
    
    def __init__(self, 
                 history_length: int = 10,
                 motion_threshold: float = 2.0,
                 prediction_steps: int = 5):
        """
        Initialize motion predictor
        
        Args:
            history_length: Number of frames to maintain in history
            motion_threshold: Minimum motion to consider significant
            prediction_steps: Number of frames to predict ahead
        """
        self.history_length = history_length
        self.motion_threshold = motion_threshold
        self.prediction_steps = prediction_steps
        self.logger = logging.getLogger(f"{__name__}.MotionPredictor")
        
        # Motion history
        self.camera_motion_history: List[CameraMotion] = []
        self.track_histories: Dict[int, List[Detection]] = {}
        
        # Feature detector for motion estimation
        self.feature_detector = cv2.ORB_create(nfeatures=500)
        self.matcher = cv2.BFMatcher()
        
        # Previous frame data for motion estimation
        self.prev_frame = None
        self.prev_keypoints = None
        self.prev_descriptors = None
        
        self.logger.info("Motion Predictor initialized")
    
    def update_camera_motion(self, current_frame: np.ndarray, timestamp: float) -> CameraMotion:
        """
        Estimate camera motion from consecutive frames
        
        Args:
            current_frame: Current video frame
            timestamp: Frame timestamp
            
        Returns:
            Estimated camera motion
        """
        if self.prev_frame is None:
            self._update_reference_frame(current_frame)
            return CameraMotion(
                translation=np.array([0.0, 0.0]),
                rotation=0.0,
                scale=1.0,
                confidence=1.0,
                timestamp=timestamp
            )
        
        try:
            # Detect and match features
            keypoints, descriptors = self.feature_detector.detectAndCompute(current_frame, None)
            
            if descriptors is None or self.prev_descriptors is None:
                self.logger.warning("No features detected for motion estimation")
                return self._create_zero_motion(timestamp)
            
            # Match features
            matches = self.matcher.knnMatch(self.prev_descriptors, descriptors, k=2)
            
            # Apply ratio test
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.7 * n.distance:
                        good_matches.append(m)
            
            if len(good_matches) < 10:
                self.logger.warning(f"Insufficient matches for motion estimation: {len(good_matches)}")
                return self._create_zero_motion(timestamp)
            
            # Extract matched points
            prev_pts = np.float32([self.prev_keypoints[m.queryIdx].pt for m in good_matches])
            curr_pts = np.float32([keypoints[m.trainIdx].pt for m in good_matches])
            
            # Estimate affine transformation
            transform_matrix = cv2.estimateAffinePartial2D(prev_pts, curr_pts)[0]
            
            if transform_matrix is not None:
                motion = self._extract_motion_parameters(transform_matrix, timestamp)
                
                # Update reference frame
                self._update_reference_frame(current_frame, keypoints, descriptors)
                
                # Add to history
                self.camera_motion_history.append(motion)
                if len(self.camera_motion_history) > self.history_length:
                    self.camera_motion_history = self.camera_motion_history[-self.history_length:]
                
                return motion
            else:
                return self._create_zero_motion(timestamp)
                
        except Exception as e:
            self.logger.error(f"Error in camera motion estimation: {e}")
            return self._create_zero_motion(timestamp)
        
    def estimate_camera_motion(self, frame: np.ndarray) -> CameraMotion:
        """Backward compatibility method"""
        import time
        timestamp = time.time()
        return self.update_camera_motion(frame, timestamp)

    def predict_track_position(self, track: Track, steps_ahead: int = 1) -> PredictedPosition:
        """
        Predict future position of a track
        
        Args:
            track: Track to predict
            steps_ahead: Number of frames to predict ahead
            
        Returns:
            Predicted position
        """
        if track.track_id not in self.track_histories:
            self.track_histories[track.track_id] = []
        
        # Update track history
        if track.detections:
            self.track_histories[track.track_id].extend(track.detections)
            # Keep only recent history
            if len(self.track_histories[track.track_id]) > self.history_length:
                self.track_histories[track.track_id] = self.track_histories[track.track_id][-self.history_length:]
        
        history = self.track_histories[track.track_id]
        
        if len(history) < 2:
            # Not enough history for prediction
            current_bbox = track.to_tlbr()
            return PredictedPosition(
                bbox=current_bbox,
                confidence=0.5,
                velocity=np.array([0.0, 0.0]),
                timestamp=0.0
            )
        
        # Calculate velocity from recent positions
        velocity = self._calculate_track_velocity(history)
        
        # Apply camera motion compensation
        compensated_velocity = self._compensate_velocity_for_camera_motion(velocity)
        
        # Predict future position
        current_bbox = track.to_tlbr()
        current_center = np.array([(current_bbox[0] + current_bbox[2]) / 2,
                                  (current_bbox[1] + current_bbox[3]) / 2])
        
        predicted_center = current_center + compensated_velocity * steps_ahead
        
        # Maintain bbox size
        bbox_width = current_bbox[2] - current_bbox[0]
        bbox_height = current_bbox[3] - current_bbox[1]
        
        predicted_bbox = np.array([
            predicted_center[0] - bbox_width / 2,
            predicted_center[1] - bbox_height / 2,
            predicted_center[0] + bbox_width / 2,
            predicted_center[1] + bbox_height / 2
        ])
        
        # Calculate prediction confidence
        confidence = self._calculate_prediction_confidence(history, velocity)
        
        return PredictedPosition(
            bbox=predicted_bbox,
            confidence=confidence,
            velocity=compensated_velocity,
            timestamp=0.0
        )
    
    def compensate_tracks_for_motion(self, tracks: List[Track]) -> List[Track]:
        """
        Compensate track positions for camera motion
        
        Args:
            tracks: List of tracks to compensate
            
        Returns:
            List of motion-compensated tracks
        """
        if not self.camera_motion_history:
            return tracks
        
        recent_motion = self.camera_motion_history[-1]
        
        compensated_tracks = []
        for track in tracks:
            compensated_track = self._compensate_track_for_motion(track, recent_motion)
            compensated_tracks.append(compensated_track)
        
        return compensated_tracks
    
    def _extract_motion_parameters(self, transform_matrix: np.ndarray, timestamp: float) -> CameraMotion:
        """Extract motion parameters from transformation matrix"""
        # Extract translation
        translation = transform_matrix[:2, 2]
        
        # Extract rotation and scale
        a = transform_matrix[0, 0]
        b = transform_matrix[0, 1]
        
        scale = np.sqrt(a*a + b*b)
        rotation = np.arctan2(b, a)
        
        # Calculate confidence based on motion magnitude
        motion_magnitude = np.linalg.norm(translation)
        confidence = min(1.0, max(0.1, 1.0 / (1.0 + motion_magnitude / 10.0)))
        
        return CameraMotion(
            translation=translation,
            rotation=rotation,
            scale=scale,
            confidence=confidence,
            timestamp=timestamp
        )
    
    def _calculate_track_velocity(self, history: List[Detection]) -> np.ndarray:
        """Calculate track velocity from detection history"""
        if len(history) < 2:
            return np.array([0.0, 0.0])
        
        # Get recent positions
        recent_positions = []
        for detection in history[-5:]:  # Last 5 detections
            center = detection.center
            recent_positions.append(center)
        
        if len(recent_positions) < 2:
            return np.array([0.0, 0.0])
        
        # Calculate average velocity
        velocities = []
        for i in range(1, len(recent_positions)):
            vel = recent_positions[i] - recent_positions[i-1]
            velocities.append(vel)
        
        return np.mean(velocities, axis=0)
    
    def _compensate_velocity_for_camera_motion(self, object_velocity: np.ndarray) -> np.ndarray:
        """Compensate object velocity for camera motion"""
        if not self.camera_motion_history:
            return object_velocity
        
        # Get recent camera motion
        recent_motion = self.camera_motion_history[-1]
        camera_velocity = recent_motion.translation
        
        # Subtract camera motion from object motion
        compensated_velocity = object_velocity - camera_velocity
        
        return compensated_velocity
    
    def _calculate_prediction_confidence(self, history: List[Detection], velocity: np.ndarray) -> float:
        """Calculate confidence for motion prediction"""
        if len(history) < 3:
            return 0.5
        
        # Calculate motion consistency
        recent_velocities = []
        positions = [det.center for det in history[-5:]]
        
        for i in range(1, len(positions)):
            vel = positions[i] - positions[i-1]
            recent_velocities.append(vel)
        
        if len(recent_velocities) < 2:
            return 0.5
        
        # Calculate velocity standard deviation
        velocities_array = np.array(recent_velocities)
        velocity_std = np.std(velocities_array, axis=0)
        max_std = np.max(velocity_std)
        
        # Higher consistency = higher confidence
        consistency = 1.0 / (1.0 + max_std)
        
        # Factor in motion magnitude (very fast motion is less reliable)
        velocity_magnitude = np.linalg.norm(velocity)
        magnitude_factor = 1.0 / (1.0 + velocity_magnitude / 20.0)
        
        confidence = consistency * magnitude_factor
        return np.clip(confidence, 0.1, 1.0)
    
    def _compensate_track_for_motion(self, track: Track, camera_motion: CameraMotion) -> Track:
        """Compensate individual track for camera motion"""
        # This is a simplified implementation
        # In practice, you'd modify the track's internal state
        return track
    
    def _create_zero_motion(self, timestamp: float) -> CameraMotion:
        """Create a zero motion object"""
        return CameraMotion(
            translation=np.array([0.0, 0.0]),
            rotation=0.0,
            scale=1.0,
            confidence=1.0,
            timestamp=timestamp
        )
    
    def _update_reference_frame(self, frame: np.ndarray, 
                               keypoints: Optional[List] = None,
                               descriptors: Optional[np.ndarray] = None):
        """Update reference frame for motion tracking"""
        self.prev_frame = frame.copy()
        
        if keypoints is not None and descriptors is not None:
            self.prev_keypoints = keypoints
            self.prev_descriptors = descriptors
        else:
            # Detect features if not provided
            self.prev_keypoints, self.prev_descriptors = self.feature_detector.detectAndCompute(frame, None)
    
    def get_motion_statistics(self) -> Dict[str, Any]:
        """Get motion tracking statistics"""
        if not self.camera_motion_history:
            return {"motion_detected": False}
        
        # Analyze recent motion
        recent_motions = self.camera_motion_history[-5:]
        translations = [motion.translation for motion in recent_motions]
        rotations = [motion.rotation for motion in recent_motions]
        confidences = [motion.confidence for motion in recent_motions]
        
        return {
            "motion_detected": True,
            "avg_translation": np.mean([np.linalg.norm(t) for t in translations]),
            "max_translation": np.max([np.linalg.norm(t) for t in translations]),
            "avg_rotation": np.mean([abs(r) for r in rotations]),
            "avg_confidence": np.mean(confidences),
            "motion_frames": len(self.camera_motion_history)
        }

class EnhancedTrackMatcher:
    """
    Enhanced track matching using motion prediction
    """
    
    def __init__(self, motion_predictor: MotionPredictor):
        """
        Initialize enhanced track matcher
        
        Args:
            motion_predictor: Motion predictor instance
        """
        self.motion_predictor = motion_predictor
        self.logger = logging.getLogger(f"{__name__}.EnhancedTrackMatcher")
    
    def match_tracks_with_predictions(self, 
                                    tracks: List[Track], 
                                    detections: List[Detection]) -> List[Tuple[int, int]]:
        """
        Match tracks to detections using motion predictions
        
        Args:
            tracks: List of existing tracks
            detections: List of new detections
            
        Returns:
            List of (track_idx, detection_idx) matches
        """
        if not tracks or not detections:
            return []
        
        # Generate predictions for all tracks
        predictions = []
        for track in tracks:
            prediction = self.motion_predictor.predict_track_position(track)
            predictions.append(prediction)
        
        # Calculate cost matrix using predictions
        cost_matrix = self._calculate_prediction_cost_matrix(predictions, detections)
        
        # Use Hungarian algorithm for optimal assignment
        from scipy.optimize import linear_sum_assignment
        track_indices, detection_indices = linear_sum_assignment(cost_matrix)
        
        # Filter matches by cost threshold
        matches = []
        for track_idx, det_idx in zip(track_indices, detection_indices):
            if cost_matrix[track_idx, det_idx] < 0.5:  # Cost threshold
                matches.append((track_idx, det_idx))
        
        return matches
    
    def _calculate_prediction_cost_matrix(self, 
                                        predictions: List[PredictedPosition],
                                        detections: List[Detection]) -> np.ndarray:
        """Calculate cost matrix using motion predictions"""
        cost_matrix = np.ones((len(predictions), len(detections)))
        
        for i, prediction in enumerate(predictions):
            for j, detection in enumerate(detections):
                # Calculate IoU between predicted and actual position
                iou = self._calculate_iou(prediction.bbox, detection.bbox)
                
                # Calculate distance between centers
                pred_center = np.array([(prediction.bbox[0] + prediction.bbox[2]) / 2,
                                       (prediction.bbox[1] + prediction.bbox[3]) / 2])
                det_center = detection.center
                distance = np.linalg.norm(pred_center - det_center)
                
                # Combine IoU and distance with prediction confidence
                cost = (1.0 - iou) + (distance / 100.0)  # Normalize distance
                cost = cost * (2.0 - prediction.confidence)  # Weight by confidence
                
                cost_matrix[i, j] = cost
        
        return cost_matrix
    
    def _calculate_iou(self, bbox1: np.ndarray, bbox2: np.ndarray) -> float:
        """Calculate IoU between two bounding boxes"""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0

# Factory functions for easy creation
def create_motion_predictor(history_length: int = 10,
                           motion_threshold: float = 2.0,
                           prediction_steps: int = 5) -> MotionPredictor:
    """Create a motion predictor with specified parameters"""
    return MotionPredictor(history_length, motion_threshold, prediction_steps)

def create_enhanced_track_matcher(motion_predictor: MotionPredictor) -> EnhancedTrackMatcher:
    """Create an enhanced track matcher"""
    return EnhancedTrackMatcher(motion_predictor)

================
File: argus_track/utils/output_manager.py
================
# argus_track/utils/output_manager.py (NEW FILE)

"""
Output Manager - Handles JSON and CSV export for simplified tracking
"""

import json
import csv
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

from ..core import Detection, GPSData


@dataclass
class FrameData:
    """Data for a single processed frame"""
    frame_id: int
    timestamp: float
    detections: List[Detection]
    gps_data: Optional[GPSData] = None


class OutputManager:
    """
    Manages output file generation for simplified tracking
    
    Outputs:
    1. JSON file: Frame-by-frame detection data
    2. CSV file: GPS data synchronized with frame IDs
    """
    
    def __init__(self, video_path: str, class_names: List[str]):
        """
        Initialize output manager
        
        Args:
            video_path: Path to input video (for output naming)
            class_names: List of class names from YOLO model
        """
        self.video_path = Path(video_path)
        self.class_names = class_names
        self.logger = logging.getLogger(f"{__name__}.OutputManager")
        
        # Storage for output data
        self.frame_data: Dict[int, FrameData] = {}
        self.processing_stats = {
            'total_frames_processed': 0,
            'total_detections': 0,
            'unique_track_ids': set(),
            'class_distribution': {},
            'processing_time': 0.0
        }
        
        self.logger.info(f"Output Manager initialized for video: {video_path}")
        self.logger.info(f"Available classes: {class_names}")
    
    def add_frame_data(self, 
                      frame_id: int,
                      timestamp: float,
                      detections: List[Detection],
                      gps_data: Optional[GPSData] = None):
        """
        Add data for a processed frame
        
        Args:
            frame_id: Frame identifier
            timestamp: Timestamp in original video
            detections: List of detections for this frame
            gps_data: GPS data for this frame (if available)
        """
        self.frame_data[frame_id] = FrameData(
            frame_id=frame_id,
            timestamp=timestamp,
            detections=detections.copy() if detections else [],
            gps_data=gps_data
        )
        
        # Update statistics
        self.processing_stats['total_frames_processed'] += 1
        self.processing_stats['total_detections'] += len(detections) if detections else 0
        
        for detection in detections or []:
            self.processing_stats['unique_track_ids'].add(detection.track_id)
            
            # Update class distribution
            class_name = self._get_class_name(detection.class_id)
            if class_name not in self.processing_stats['class_distribution']:
                self.processing_stats['class_distribution'][class_name] = 0
            self.processing_stats['class_distribution'][class_name] += 1
    
    def export_json(self, output_path: Optional[str] = None) -> str:
        """
        Export frame data to JSON file
        
        Args:
            output_path: Custom output path (optional)
            
        Returns:
            Path to exported JSON file
        """
        if output_path is None:
            output_path = self.video_path.with_suffix('.json')
        
        # Prepare JSON data
        json_data = {
            "metadata": {
                "video_file": str(self.video_path),
                "total_frames": self.processing_stats['total_frames_processed'],
                "total_detections": self.processing_stats['total_detections'],
                "unique_tracks": len(self.processing_stats['unique_track_ids']),
                "class_names": self.class_names,
                "class_distribution": self.processing_stats['class_distribution'],
                "processing_method": "simplified_tracking_with_consolidation"
            },
            "frames": {}
        }
        
        # Add frame data
        for frame_id, data in sorted(self.frame_data.items()):
            frame_key = f"frame_{frame_id}"
            json_data["frames"][frame_key] = {
                "timestamp": data.timestamp,
                "detections": [
                    {
                        "track_id": det.track_id,
                        "class": self._get_class_name(det.class_id),
                        "class_id": det.class_id,
                        "bbox": [
                            float(det.bbox[0]),  # x1
                            float(det.bbox[1]),  # y1
                            float(det.bbox[2]),  # x2
                            float(det.bbox[3])   # y2
                        ],
                        "confidence": float(det.score)
                    }
                    for det in data.detections
                ]
            }
        
        # Write JSON file
        with open(output_path, 'w') as f:
            json.dump(json_data, f, indent=2)
        
        self.logger.info(f"📄 Exported frame data to JSON: {output_path}")
        self.logger.info(f"   Frames: {len(self.frame_data)}")
        self.logger.info(f"   Total detections: {self.processing_stats['total_detections']}")
        self.logger.info(f"   Unique tracks: {len(self.processing_stats['unique_track_ids'])}")
        
        return str(output_path)
    
    def export_csv(self, output_path: Optional[str] = None) -> str:
        """
        Export GPS data to CSV file
        
        Args:
            output_path: Custom output path (optional)
            
        Returns:
            Path to exported CSV file
        """
        if output_path is None:
            output_path = self.video_path.with_suffix('.csv')
        
        # Collect GPS data with frame IDs
        gps_rows = []
        for frame_id, data in sorted(self.frame_data.items()):
            if data.gps_data is not None:
                gps_rows.append({
                    'frame_id': frame_id,
                    'timestamp': data.timestamp,
                    'latitude': data.gps_data.latitude,
                    'longitude': data.gps_data.longitude,
                    'altitude': data.gps_data.altitude,
                    'heading': data.gps_data.heading,
                    'accuracy': data.gps_data.accuracy
                })
        
        # Write CSV file
        if gps_rows:
            with open(output_path, 'w', newline='') as f:
                fieldnames = ['frame_id', 'timestamp', 'latitude', 'longitude', 
                             'altitude', 'heading', 'accuracy']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(gps_rows)
            
            self.logger.info(f"📍 Exported GPS data to CSV: {output_path}")
            self.logger.info(f"   GPS entries: {len(gps_rows)}")
        else:
            # Create empty CSV with headers
            with open(output_path, 'w', newline='') as f:
                fieldnames = ['frame_id', 'timestamp', 'latitude', 'longitude', 
                             'altitude', 'heading', 'accuracy']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
            
            self.logger.warning(f"⚠️  No GPS data available - created empty CSV: {output_path}")
        
        return str(output_path)
    
    def export_both(self, 
                   json_path: Optional[str] = None,
                   csv_path: Optional[str] = None) -> tuple[str, str]:
        """
        Export both JSON and CSV files
        
        Args:
            json_path: Custom JSON output path (optional)
            csv_path: Custom CSV output path (optional)
            
        Returns:
            Tuple of (json_path, csv_path)
        """
        json_output = self.export_json(json_path)
        csv_output = self.export_csv(csv_path)
        
        return json_output, csv_output
    
    def _get_class_name(self, class_id: int) -> str:
        """Get class name from class ID"""
        if 0 <= class_id < len(self.class_names):
            return self.class_names[class_id]
        else:
            return f"unknown_class_{class_id}"
    
    def get_processing_summary(self) -> Dict[str, Any]:
        """Get processing summary statistics"""
        return {
            'frames_processed': self.processing_stats['total_frames_processed'],
            'total_detections': self.processing_stats['total_detections'],
            'unique_tracks': len(self.processing_stats['unique_track_ids']),
            'avg_detections_per_frame': (
                self.processing_stats['total_detections'] / 
                max(1, self.processing_stats['total_frames_processed'])
            ),
            'class_distribution': dict(self.processing_stats['class_distribution']),
            'frames_with_gps': len([
                data for data in self.frame_data.values() 
                if data.gps_data is not None
            ])
        }
    
    def print_summary(self):
        """Print processing summary to console"""
        summary = self.get_processing_summary()
        
        print("\n" + "="*50)
        print("📊 PROCESSING SUMMARY")
        print("="*50)
        print(f"📹 Video: {self.video_path.name}")
        print(f"🎬 Frames processed: {summary['frames_processed']}")
        print(f"🎯 Total detections: {summary['total_detections']}")
        print(f"🏷️  Unique tracks: {summary['unique_tracks']}")
        print(f"📊 Avg detections/frame: {summary['avg_detections_per_frame']:.1f}")
        print(f"📍 Frames with GPS: {summary['frames_with_gps']}")
        
        if summary['class_distribution']:
            print("\n🏷️  Class Distribution:")
            for class_name, count in summary['class_distribution'].items():
                print(f"   {class_name}: {count}")
        
        print("="*50)
    
    def create_sample_output(self) -> Dict[str, Any]:
        """Create sample output structure for documentation"""
        return {
            "sample_json_structure": {
                "metadata": {
                    "video_file": "example_video.mp4",
                    "total_frames": 150,
                    "total_detections": 45,
                    "unique_tracks": 8,
                    "class_names": ["LED", "traffic_light", "street_lamp"],
                    "class_distribution": {"LED": 30, "traffic_light": 15}
                },
                "frames": {
                    "frame_6": {
                        "timestamp": 0.1,
                        "detections": [
                            {
                                "track_id": 1,
                                "class": "LED",
                                "class_id": 0,
                                "bbox": [100.5, 200.3, 150.8, 280.1],
                                "confidence": 0.85
                            }
                        ]
                    }
                }
            },
            "sample_csv_structure": [
                {
                    "frame_id": 6,
                    "timestamp": 0.1,
                    "latitude": -34.758432,
                    "longitude": -58.635219,
                    "altitude": 25.4,
                    "heading": 45.2,
                    "accuracy": 1.0
                }
            ]
        }

================
File: argus_track/utils/smart_track_manager.py
================
# Complete enhanced SmartTrackManager - replace the existing class

import numpy as np
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass, field
import logging
import time

from ..core import Detection, Track, GPSData
from .motion_prediction import MotionPredictor, CameraMotion, PredictedPosition, EnhancedTrackMatcher
from .visual_feature_extractor import VisualFeatureExtractor, VisualFeatures

@dataclass
class TrackMemory:
    """Enhanced memory container with motion prediction and visual features"""
    track_id: int
    last_seen_frame: int
    last_position: np.ndarray
    velocity_history: List[np.ndarray] = field(default_factory=list)
    confidence_history: List[float] = field(default_factory=list)
    detection_count: int = 0
    creation_time: float = field(default_factory=time.time)
    last_update_time: float = field(default_factory=time.time)
    gps_positions: List[Tuple[float, float]] = field(default_factory=list)
    is_static: bool = False
    reliability_score: float = 0.5
    
    # Motion prediction attributes
    predicted_position: Optional[np.ndarray] = None
    prediction_confidence: float = 0.0
    prediction_accuracy_history: List[float] = field(default_factory=list)
    motion_compensated: bool = False
    camera_motion_history: List[Dict] = field(default_factory=list)
    
    # Visual feature attributes
    visual_features: Optional[VisualFeatures] = None
    feature_history: List[VisualFeatures] = field(default_factory=list)
    feature_match_history: List[float] = field(default_factory=list)
    appearance_stability: float = 0.5
    
    # GPS-enhanced motion attributes
    gps_velocity_history: List[Tuple[float, float]] = field(default_factory=list)  # (speed, heading)
    world_motion_compensation: bool = False
    
    def update(self, detection: Detection, frame_id: int, 
               gps_pos: Optional[Tuple[float, float]] = None,
               gps_velocity: Optional[Tuple[float, float]] = None,
               visual_features: Optional[VisualFeatures] = None):
        """Enhanced update with visual features and GPS motion"""
        
        # === MOTION UPDATES ===
        if self.last_position is not None:
            velocity = detection.center - self.last_position
            self.velocity_history.append(velocity)
            if len(self.velocity_history) > 10:
                self.velocity_history = self.velocity_history[-10:]
        
        # GPS velocity tracking
        if gps_velocity:
            self.gps_velocity_history.append(gps_velocity)
            if len(self.gps_velocity_history) > 20:
                self.gps_velocity_history = self.gps_velocity_history[-20:]
        
        # === VISUAL FEATURE UPDATES ===
        if visual_features:
            # Calculate feature similarity with previous features
            if self.visual_features:
                from .visual_feature_extractor import VisualFeatureExtractor
                feature_extractor = VisualFeatureExtractor()
                similarity = feature_extractor.compare_features(self.visual_features, visual_features)
                self.feature_match_history.append(similarity)
                
                if len(self.feature_match_history) > 15:
                    self.feature_match_history = self.feature_match_history[-15:]
                
                # Update appearance stability
                if len(self.feature_match_history) >= 3:
                    self.appearance_stability = np.mean(self.feature_match_history[-5:])
            
            # Store current features
            self.visual_features = visual_features
            self.feature_history.append(visual_features)
            if len(self.feature_history) > 5:
                self.feature_history = self.feature_history[-5:]
        
        # === PREDICTION ACCURACY ===
        if self.predicted_position is not None:
            predicted_center = np.array([
                (self.predicted_position[0] + self.predicted_position[2]) / 2,
                (self.predicted_position[1] + self.predicted_position[3]) / 2
            ])
            actual_center = detection.center
            prediction_error = np.linalg.norm(actual_center - predicted_center)
            
            normalized_error = prediction_error / 1000.0
            accuracy = max(0.0, 1.0 - normalized_error)
            self.prediction_accuracy_history.append(accuracy)
            
            if len(self.prediction_accuracy_history) > 20:
                self.prediction_accuracy_history = self.prediction_accuracy_history[-20:]
        
        # === STANDARD UPDATES ===
        self.last_position = detection.center
        self.last_seen_frame = frame_id
        self.detection_count += 1
        self.last_update_time = time.time()
        
        self.confidence_history.append(detection.score)
        if len(self.confidence_history) > 20:
            self.confidence_history = self.confidence_history[-20:]
        
        if gps_pos:
            self.gps_positions.append(gps_pos)
            if len(self.gps_positions) > 50:
                self.gps_positions = self.gps_positions[-50:]
        
        if hasattr(detection, 'motion_compensated'):
            self.motion_compensated = detection.motion_compensated
        
        self._update_static_analysis()
        self._update_reliability_score()
        
        # Clear prediction for next frame
        self.predicted_position = None
        self.prediction_confidence = 0.0
    
    def get_combined_similarity(self, other_features: VisualFeatures, 
                               motion_distance: float, config) -> float:
        """Calculate combined visual + motion similarity"""
        visual_sim = 0.0
        motion_sim = 0.0
        
        # Visual similarity
        if self.visual_features and other_features:
            from .visual_feature_extractor import VisualFeatureExtractor
            feature_extractor = VisualFeatureExtractor()
            visual_sim = feature_extractor.compare_features(self.visual_features, other_features)
        
        # Motion similarity (inverse of distance, normalized)
        if motion_distance < 200:  # Within reasonable range
            motion_sim = max(0.0, 1.0 - motion_distance / 200.0)
        
        # Weighted combination
        visual_weight = config.feature_weight if config.enable_visual_features else 0.0
        motion_weight = config.motion_weight if config.enable_motion_prediction else 1.0
        
        # Normalize weights
        total_weight = visual_weight + motion_weight
        if total_weight > 0:
            visual_weight /= total_weight
            motion_weight /= total_weight
        
        combined_similarity = visual_weight * visual_sim + motion_weight * motion_sim
        return combined_similarity
    
    def _update_static_analysis(self):
        """Enhanced static analysis with GPS motion"""
        if len(self.velocity_history) < 5:
            return
        
        # Camera motion analysis
        recent_velocities = self.velocity_history[-5:]
        velocity_magnitudes = [np.linalg.norm(v) for v in recent_velocities]
        avg_camera_velocity = np.mean(velocity_magnitudes)
        
        # GPS motion analysis
        gps_motion_stable = True
        if len(self.gps_velocity_history) >= 3:
            recent_gps_speeds = [v[0] for v in self.gps_velocity_history[-3:]]
            gps_speed_variation = np.std(recent_gps_speeds)
            gps_motion_stable = gps_speed_variation < 2.0  # m/s
        
        # Object is static if camera motion is low AND GPS motion is stable
        self.is_static = avg_camera_velocity < 2.0 and gps_motion_stable
    
    def _update_reliability_score(self):
        """Enhanced reliability with visual and motion factors"""
        score = 0.0
        
        # Base factors (40%)
        detection_factor = min(1.0, self.detection_count / 10.0)
        score += 0.2 * detection_factor
        
        if self.confidence_history:
            avg_confidence = np.mean(self.confidence_history)
            score += 0.2 * avg_confidence
        
        # Motion consistency (30%)
        if len(self.velocity_history) > 3:
            velocities = np.array(self.velocity_history[-5:])
            velocity_std = np.std(velocities, axis=0)
            max_std = np.max(velocity_std)
            motion_consistency = 1.0 / (1.0 + max_std / 5.0)
            score += 0.15 * motion_consistency
        
        # Prediction accuracy (15%)
        if self.prediction_accuracy_history:
            avg_prediction_accuracy = np.mean(self.prediction_accuracy_history)
            score += 0.15 * avg_prediction_accuracy
        
        # Visual stability (10%)
        score += 0.1 * self.appearance_stability
        
        # GPS factor (5%)
        if self.gps_positions:
            gps_factor = min(1.0, len(self.gps_positions) / 5.0)
            score += 0.05 * gps_factor
        
        self.reliability_score = min(1.0, score)


class EnhancedSmartTrackManager:
    """Enhanced Smart Track Manager with Motion Prediction and Visual Features"""
    
    def __init__(self, 
                 config,
                 max_memory_age: int = 300,
                 min_detection_count: int = 3,
                 similarity_threshold: float = 50.0):
        """Initialize enhanced track manager"""
        self.config = config
        self.max_memory_age = max_memory_age
        self.min_detection_count = min_detection_count
        self.similarity_threshold = similarity_threshold
        
        self.logger = logging.getLogger(f"{__name__}.EnhancedSmartTrackManager")
        
        # Track memory storage
        self.track_memories: Dict[int, EnhancedTrackMemory] = {}
        self.active_track_ids: Set[int] = set()
        self.lost_track_ids: Set[int] = set()
        
        # Motion prediction
        try:
            self.motion_predictor = MotionPredictor(history_length=10)
            self.enhanced_matcher = EnhancedTrackMatcher(self.motion_predictor)
            self.logger.info("Motion predictor enabled")
        except Exception as e:
            self.logger.warning(f"Failed to initialize motion predictor: {e}")
            self.motion_predictor = None
            self.enhanced_matcher = None
        
        # Visual feature extraction
        try:
            self.feature_extractor = VisualFeatureExtractor(
                min_bbox_size=20,
                hist_bins=32
            )
            self.logger.info("Visual feature extractor enabled")
        except Exception as e:
            self.logger.warning(f"Failed to initialize feature extractor: {e}")
            self.feature_extractor = None
        
        # Current frame data
        self.current_camera_motion = None
        self.current_gps_velocity = None
        
        # Statistics
        self.total_tracks_created = 0
        self.total_tracks_lost = 0
        self.total_tracks_recovered = 0
        self.total_consolidations = 0
        self.total_reappearances = 0
        
        self.logger.info("Enhanced Smart Track Manager initialized")
    
    def process_frame_detections(self, 
                                detections: List[Detection],
                                frame_id: int,
                                timestamp: float = None,
                                frame: Optional[np.ndarray] = None,
                                current_gps: Optional[GPSData] = None) -> List[Detection]:
        """Enhanced frame processing with motion and visual features"""
        
        # Update camera motion
        if self.motion_predictor and frame is not None:
            frame_timestamp = current_gps.timestamp if current_gps else frame_id / 30.0
            self.current_camera_motion = self.motion_predictor.update_camera_motion(frame, frame_timestamp)
        
        # Calculate GPS velocity
        self.current_gps_velocity = self._calculate_gps_velocity(current_gps)
        
        # Update position predictions
        self._update_position_predictions()
        
        # Extract visual features for all detections
        detection_features = {}
        if self.feature_extractor and frame is not None:
            for i, detection in enumerate(detections):
                features = self.feature_extractor.extract_features(frame, detection)
                if features:
                    detection_features[i] = features
        
        # Enhanced matching with motion and visual features
        processed_detections = self._enhanced_detection_matching(
            detections, detection_features, frame_id
        )
        
        # Update track memories
        self._update_track_memories(processed_detections, frame_id, current_gps, frame)
        
        # Handle lost tracks and cleanup
        self._handle_lost_tracks(frame_id)
        self._cleanup_old_memories(frame_id)
        
        return processed_detections
    
    def _calculate_gps_velocity(self, current_gps: Optional[GPSData]) -> Optional[Tuple[float, float]]:
        """Calculate GPS velocity (speed, heading) from GPS history"""
        if not current_gps or not hasattr(self, 'previous_gps'):
            self.previous_gps = current_gps
            return None
        
        if self.previous_gps is None:
            self.previous_gps = current_gps
            return None
        
        # Calculate time difference
        dt = current_gps.timestamp - self.previous_gps.timestamp
        if dt <= 0:
            return None
        
        # Calculate distance (simplified)
        R = 6378137.0  # Earth radius
        lat1, lon1 = np.radians(self.previous_gps.latitude), np.radians(self.previous_gps.longitude)
        lat2, lon2 = np.radians(current_gps.latitude), np.radians(current_gps.longitude)
        
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        distance = R * c
        
        # Speed in m/s
        speed = distance / dt
        
        # Use GPS heading
        heading = current_gps.heading
        
        self.previous_gps = current_gps
        return (speed, heading)
    
    def _update_position_predictions(self):
        """Update position predictions for active tracks"""
        if not self.motion_predictor:
            return
        
        for track_id in self.active_track_ids:
            if track_id in self.track_memories:
                memory = self.track_memories[track_id]
                mock_track = self._create_mock_track_from_memory(memory)
                
                if mock_track:
                    prediction = self.motion_predictor.predict_track_position(mock_track, steps_ahead=1)
                    memory.predicted_position = prediction.bbox
                    memory.prediction_confidence = prediction.confidence
    
    def _enhanced_detection_matching(self, 
                                   detections: List[Detection], 
                                   detection_features: Dict[int, VisualFeatures],
                                   frame_id: int) -> List[Detection]:
        """Enhanced matching using motion prediction and visual features"""
        
        matched_detections = []
        unmatched_detections = list(enumerate(detections))
        
        # Phase 1: Match with active tracks using predictions and features
        for track_id in self.active_track_ids.copy():
            if track_id not in self.track_memories:
                continue
            
            memory = self.track_memories[track_id]
            best_match = None
            best_score = 0.0
            best_detection_idx = None
            
            for det_idx, detection in unmatched_detections:
                # Calculate motion-based similarity
                motion_distance = float('inf')
                if memory.predicted_position is not None:
                    predicted_center = np.array([
                        (memory.predicted_position[0] + memory.predicted_position[2]) / 2,
                        (memory.predicted_position[1] + memory.predicted_position[3]) / 2
                    ])
                    motion_distance = np.linalg.norm(detection.center - predicted_center)
                
                # Get visual features for this detection
                visual_features = detection_features.get(det_idx)
                
                # Calculate combined similarity
                if self.config.track_consolidation.enable_visual_features or self.config.track_consolidation.enable_motion_prediction:
                    similarity = memory.get_combined_similarity(
                        visual_features, motion_distance, self.config.track_consolidation
                    )
                else:
                    # Fallback to distance-based matching
                    similarity = max(0.0, 1.0 - motion_distance / self.similarity_threshold) if motion_distance != float('inf') else 0.0
                
                # Check thresholds
                motion_ok = motion_distance < self.similarity_threshold
                visual_ok = True
                if visual_features and memory.visual_features and self.config.track_consolidation.enable_visual_features:
                    visual_similarity = self.feature_extractor.compare_features(memory.visual_features, visual_features)
                    visual_ok = visual_similarity > self.config.track_consolidation.feature_similarity_threshold
                
                if motion_ok and visual_ok and similarity > best_score:
                    best_score = similarity
                    best_match = detection
                    best_detection_idx = det_idx
            
            # Assign best match
            if best_match and best_score > 0.5:  # Minimum similarity threshold
                enhanced_detection = Detection(
                    bbox=best_match.bbox,
                    score=best_match.score,
                    class_id=best_match.class_id,
                    frame_id=frame_id
                )
                enhanced_detection.track_id = track_id
                enhanced_detection.prediction_match = True
                enhanced_detection.match_score = best_score
                enhanced_detection.motion_compensated = self.current_camera_motion is not None
                
                matched_detections.append(enhanced_detection)
                unmatched_detections = [(i, d) for i, d in unmatched_detections if i != best_detection_idx]
                
                self.logger.debug(f"Track {track_id}: Matched with score {best_score:.3f}")
        
        # Phase 2: Handle unmatched detections (new tracks or reappearances)
        for det_idx, detection in unmatched_detections:
            # Check for track reappearance
            reappeared_track_id = self._check_track_reappearance(detection, detection_features.get(det_idx))
            
            if reappeared_track_id:
                enhanced_detection = Detection(
                    bbox=detection.bbox,
                    score=detection.score,
                    class_id=detection.class_id,
                    frame_id=frame_id
                )
                enhanced_detection.track_id = reappeared_track_id
                enhanced_detection.reappearance_match = True
                enhanced_detection.motion_compensated = self.current_camera_motion is not None
                
                matched_detections.append(enhanced_detection)
                self.total_reappearances += 1
                
                # Move track back to active
                self.lost_track_ids.discard(reappeared_track_id)
                self.active_track_ids.add(reappeared_track_id)
                
                self.logger.info(f"Track {reappeared_track_id}: Reappeared at frame {frame_id}")
            else:
                # New track
                new_track_id = self._get_next_track_id()
                enhanced_detection = Detection(
                    bbox=detection.bbox,
                    score=detection.score,
                    class_id=detection.class_id,
                    frame_id=frame_id
                )
                enhanced_detection.track_id = new_track_id
                enhanced_detection.new_track = True
                enhanced_detection.motion_compensated = self.current_camera_motion is not None
                
                matched_detections.append(enhanced_detection)
        
        return matched_detections
    
    def _check_track_reappearance(self, detection: Detection, visual_features: Optional[VisualFeatures]) -> Optional[int]:
        """Check if detection matches a recently lost track"""
        if not self.config.track_consolidation.enable_reappearance_detection:
            return None
        
        best_track_id = None
        best_score = 0.0
        
        for track_id in self.lost_track_ids.copy():
            if track_id not in self.track_memories:
                continue
            
            memory = self.track_memories[track_id]
            
            # Check if track was lost recently enough
            frames_since_lost = abs(detection.frame_id - memory.last_seen_frame)
            if frames_since_lost > self.config.track_consolidation.max_gap_frames:
                continue
            
            # Calculate spatial distance
            if memory.last_position is not None:
                spatial_distance = np.linalg.norm(detection.center - memory.last_position)
                if spatial_distance > self.config.track_consolidation.reappearance_spatial_threshold:
                    continue
            
            # Calculate combined similarity for reappearance
            motion_distance = spatial_distance if memory.last_position is not None else float('inf')
            similarity = memory.get_combined_similarity(
                visual_features, motion_distance, self.config.track_consolidation
            )
            
            if similarity > best_score:
                best_score = similarity
                best_track_id = track_id
        
        # Return track ID if similarity is high enough
        reappearance_threshold = 0.6  # Higher threshold for reappearance
        return best_track_id if best_score > reappearance_threshold else None
    
    def _update_track_memories(self, detections: List[Detection], frame_id: int, 
                              current_gps: Optional[GPSData], frame: Optional[np.ndarray]):
        """Update track memories with enhanced features"""
        
        for detection in detections:
            if hasattr(detection, 'track_id') and detection.track_id is not None:
                track_id = detection.track_id
                
                # Calculate GPS position
                gps_pos = None
                if current_gps:
                    gps_pos = self._calculate_gps_position(detection, current_gps)
                
                # Extract visual features
                visual_features = None
                if self.feature_extractor and frame is not None:
                    visual_features = self.feature_extractor.extract_features(frame, detection)
                
                # Create or update memory
                if track_id not in self.track_memories:
                    self.track_memories[track_id] = EnhancedTrackMemory(
                        track_id=track_id,
                        last_seen_frame=frame_id,
                        last_position=detection.center
                    )
                    self.total_tracks_created += 1
                    self.active_track_ids.add(track_id)
                
                # Update memory with all features
                self.track_memories[track_id].update(
                    detection, frame_id, gps_pos, self.current_gps_velocity, visual_features
                )
    
    def _calculate_gps_position(self, detection: Detection, gps_data: GPSData) -> Optional[Tuple[float, float]]:
        """Calculate GPS position for detection using enhanced depth estimation"""
        try:
            bbox = detection.bbox
            bbox_height = bbox[3] - bbox[1]
            
            if bbox_height <= 0:
                return None
            
            # Enhanced depth estimation considering camera motion
            focal_length = 1400
            object_height = 4.0  # meters
            base_depth = (object_height * focal_length) / bbox_height
            
            # Adjust depth based on camera motion
            if self.current_camera_motion and self.current_camera_motion.translation is not None:
                motion_magnitude = np.linalg.norm(self.current_camera_motion.translation)
                motion_factor = 1.0 + motion_magnitude * 0.001  # Small adjustment
                estimated_depth = base_depth * motion_factor
            else:
                estimated_depth = base_depth
            
            # Enhanced bearing calculation with GPS velocity
            bbox_center_x = (bbox[0] + bbox[2]) / 2
            image_width = 1920
            pixels_from_center = bbox_center_x - (image_width / 2)
            degrees_per_pixel = 60.0 / image_width
            bearing_offset = pixels_from_center * degrees_per_pixel
            
            # Adjust bearing based on GPS heading and camera motion
            base_bearing = gps_data.heading + bearing_offset
            
            if self.current_gps_velocity:
                speed, gps_heading = self.current_gps_velocity
                # Use GPS heading for moving vehicle, camera heading for stationary
                if speed > 1.0:  # Moving
                    object_bearing = gps_heading + bearing_offset
                else:
                    object_bearing = base_bearing
            else:
                object_bearing = base_bearing
            
            # Convert to GPS coordinates
            import math
            lat_offset = (estimated_depth * math.cos(math.radians(object_bearing))) / 111000
            lon_offset = (estimated_depth * math.sin(math.radians(object_bearing))) / (111000 * math.cos(math.radians(gps_data.latitude)))
            
            object_lat = gps_data.latitude + lat_offset
            object_lon = gps_data.longitude + lon_offset
            
            return (object_lat, object_lon)
            
        except Exception as e:
            self.logger.error(f"Error calculating GPS position: {e}")
            return None
    
    def _get_next_track_id(self) -> int:
        """Get next available track ID"""
        if not hasattr(self, '_next_track_id'):
            self._next_track_id = 1
        
        self._next_track_id += 1
        return self._next_track_id
    
    def _create_mock_track_from_memory(self, memory: EnhancedTrackMemory):
        """Create mock track for motion prediction"""
        from ..core import Track, Detection
        
        if memory.last_position is not None:
            estimated_size = 50
            center = memory.last_position
            
            bbox = np.array([
                center[0] - estimated_size/2,
                center[1] - estimated_size/2,
                center[0] + estimated_size/2,
                center[1] + estimated_size/2
            ])
            
            detection = Detection(
                bbox=bbox,
                score=0.8,
                class_id=0,
                frame_id=memory.last_seen_frame
            )
            
            track = Track(
                track_id=memory.track_id,
                detections=[detection],
                state='confirmed'
            )
            
            return track
        
        return None
    
    def _handle_lost_tracks(self, frame_id: int):
        """Handle tracks that weren't detected this frame"""
        current_active = set()
        
        for track_id, memory in self.track_memories.items():
            if memory.last_seen_frame == frame_id:
                current_active.add(track_id)
        
        lost_tracks = self.active_track_ids - current_active
        for lost_track_id in lost_tracks:
            self.active_track_ids.discard(lost_track_id)
            self.lost_track_ids.add(lost_track_id)
            self.total_tracks_lost += 1
            self.logger.debug(f"Track {lost_track_id} lost at frame {frame_id}")
    
    def _cleanup_old_memories(self, current_frame: int):
        """Cleanup old track memories"""
        to_remove = []
        
        for track_id, memory in self.track_memories.items():
            age = current_frame - memory.last_seen_frame
            if age > self.max_memory_age:
                to_remove.append(track_id)
        
        for track_id in to_remove:
            del self.track_memories[track_id]
            self.lost_track_ids.discard(track_id)
            self.active_track_ids.discard(track_id)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive enhanced statistics"""
        exportable_tracks = len([m for m in self.track_memories.values() 
                                if m.detection_count >= self.min_detection_count and m.reliability_score > 0.3])
        
        # Visual feature statistics
        tracks_with_features = len([m for m in self.track_memories.values() if m.visual_features])
        avg_feature_stability = np.mean([m.appearance_stability for m in self.track_memories.values()]) if self.track_memories else 0
        
        # Motion prediction statistics
        tracks_with_predictions = len([m for m in self.track_memories.values() if m.prediction_accuracy_history])
        avg_prediction_accuracy = np.mean([
            np.mean(m.prediction_accuracy_history) for m in self.track_memories.values() 
            if m.prediction_accuracy_history
        ]) if tracks_with_predictions > 0 else 0
        
        return {
            'total_tracks_created': self.total_tracks_created,
            'total_tracks_lost': self.total_tracks_lost,
            'total_tracks_recovered': self.total_tracks_recovered,
            'total_consolidations': self.total_consolidations,
            'total_reappearances': self.total_reappearances,
            'active_tracks': len(self.active_track_ids),
            'lost_tracks': len(self.lost_track_ids),
            'tracks_in_memory': len(self.track_memories),
            'exportable_tracks': exportable_tracks,
            'recovery_rate': self.total_tracks_recovered / max(1, self.total_tracks_lost),
            'visual_features': {
                'tracks_with_features': tracks_with_features,
                'avg_appearance_stability': avg_feature_stability,
                'feature_extraction_enabled': self.feature_extractor is not None
            },
            'motion_prediction': {
                'tracks_with_predictions': tracks_with_predictions,
                'avg_prediction_accuracy': avg_prediction_accuracy,
                'motion_predictor_enabled': self.motion_predictor is not None,
                'camera_motion_detected': self.current_camera_motion is not None
            },
            'gps_enhanced': {
                'gps_velocity_tracking': self.current_gps_velocity is not None,
                'world_motion_compensation': True
            }
        }

================
File: argus_track/utils/visual_feature_extractor.py
================
# argus_track/utils/visual_feature_extractor.py (NEW FILE)

"""
Visual Feature Extractor - Extract and compare visual features for object matching
Uses multiple feature descriptors to match objects based on appearance
"""

import cv2
import numpy as np
import logging
from typing import List, Tuple, Optional, Dict
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity

from ..core import Detection


@dataclass
class VisualFeatures:
    """Container for visual features of an object"""
    histogram_hsv: np.ndarray          # HSV color histogram
    histogram_gray: np.ndarray         # Grayscale histogram  
    texture_features: np.ndarray       # LBP texture descriptor
    shape_features: np.ndarray         # Contour-based shape features
    size_features: np.ndarray          # Size and aspect ratio
    detection_bbox: np.ndarray         # Original bounding box
    extraction_quality: float         # Quality score (0-1)


class VisualFeatureExtractor:
    """
    Extract visual features from object detections for appearance-based matching
    
    Features extracted:
    1. Color histograms (HSV and grayscale)
    2. Texture features (Local Binary Patterns)
    3. Shape features (contours, aspect ratio)
    4. Size features (normalized)
    """
    
    def __init__(self, 
                 min_bbox_size: int = 20,
                 hist_bins: int = 32,
                 lbp_radius: int = 1,
                 lbp_points: int = 8):
        """
        Initialize visual feature extractor
        
        Args:
            min_bbox_size: Minimum bbox size for feature extraction
            hist_bins: Number of bins for histograms
            lbp_radius: Radius for LBP texture analysis
            lbp_points: Number of points for LBP
        """
        self.min_bbox_size = min_bbox_size
        self.hist_bins = hist_bins
        self.lbp_radius = lbp_radius
        self.lbp_points = lbp_points
        self.logger = logging.getLogger(f"{__name__}.VisualFeatureExtractor")
        
        # Feature weights for similarity calculation
        self.feature_weights = {
            'color_hsv': 0.3,
            'color_gray': 0.2,
            'texture': 0.3,
            'shape': 0.1,
            'size': 0.1
        }
        
        self.logger.info("Visual Feature Extractor initialized")
        self.logger.info(f"  Histogram bins: {hist_bins}")
        self.logger.info(f"  LBP parameters: radius={lbp_radius}, points={lbp_points}")
    
    def extract_features(self, frame: np.ndarray, detection: Detection) -> Optional[VisualFeatures]:
        """
        Extract visual features from a detection
        
        Args:
            frame: Full frame image
            detection: Detection object with bounding box
            
        Returns:
            VisualFeatures object or None if extraction failed
        """
        try:
            bbox = detection.bbox.astype(int)
            x1, y1, x2, y2 = bbox
            
            # Validate bounding box
            if not self._validate_bbox(bbox, frame.shape):
                return None
            
            # Extract region of interest
            roi = frame[y1:y2, x1:x2]
            
            if roi.size == 0 or min(roi.shape[:2]) < self.min_bbox_size:
                return None
            
            # Extract different types of features
            hist_hsv = self._extract_hsv_histogram(roi)
            hist_gray = self._extract_gray_histogram(roi)
            texture_features = self._extract_texture_features(roi)
            shape_features = self._extract_shape_features(roi)
            size_features = self._extract_size_features(bbox)
            
            # Calculate extraction quality
            quality = self._calculate_extraction_quality(roi, bbox)
            
            features = VisualFeatures(
                histogram_hsv=hist_hsv,
                histogram_gray=hist_gray,
                texture_features=texture_features,
                shape_features=shape_features,
                size_features=size_features,
                detection_bbox=bbox,
                extraction_quality=quality
            )
            
            return features
            
        except Exception as e:
            self.logger.warning(f"Feature extraction failed: {e}")
            return None
    
    def compare_features(self, features1: VisualFeatures, features2: VisualFeatures) -> float:
        """
        Compare two sets of visual features
        
        Args:
            features1: First set of features
            features2: Second set of features
            
        Returns:
            Similarity score (0-1, higher is more similar)
        """
        try:
            similarities = {}
            
            # Color similarity (HSV)
            if features1.histogram_hsv is not None and features2.histogram_hsv is not None:
                similarities['color_hsv'] = self._compare_histograms(
                    features1.histogram_hsv, features2.histogram_hsv
                )
            
            # Color similarity (Grayscale)
            if features1.histogram_gray is not None and features2.histogram_gray is not None:
                similarities['color_gray'] = self._compare_histograms(
                    features1.histogram_gray, features2.histogram_gray
                )
            
            # Texture similarity
            if features1.texture_features is not None and features2.texture_features is not None:
                similarities['texture'] = self._compare_feature_vectors(
                    features1.texture_features, features2.texture_features
                )
            
            # Shape similarity
            if features1.shape_features is not None and features2.shape_features is not None:
                similarities['shape'] = self._compare_feature_vectors(
                    features1.shape_features, features2.shape_features
                )
            
            # Size similarity
            if features1.size_features is not None and features2.size_features is not None:
                similarities['size'] = self._compare_feature_vectors(
                    features1.size_features, features2.size_features
                )
            
            # Weighted average of similarities
            total_weight = 0
            weighted_sum = 0
            
            for feature_type, similarity in similarities.items():
                if similarity is not None:
                    weight = self.feature_weights.get(feature_type, 0.1)
                    weighted_sum += similarity * weight
                    total_weight += weight
            
            if total_weight > 0:
                final_similarity = weighted_sum / total_weight
                
                # Quality adjustment
                quality_factor = min(features1.extraction_quality, features2.extraction_quality)
                final_similarity *= quality_factor
                
                return min(1.0, max(0.0, final_similarity))
            
            return 0.0
            
        except Exception as e:
            self.logger.warning(f"Feature comparison failed: {e}")
            return 0.0
    
    def _extract_hsv_histogram(self, roi: np.ndarray) -> Optional[np.ndarray]:
        """Extract HSV color histogram"""
        try:
            hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            
            # Calculate histograms for H, S, V channels
            hist_h = cv2.calcHist([hsv], [0], None, [self.hist_bins], [0, 180])
            hist_s = cv2.calcHist([hsv], [1], None, [self.hist_bins], [0, 256])
            hist_v = cv2.calcHist([hsv], [2], None, [self.hist_bins], [0, 256])
            
            # Normalize and concatenate
            hist_h = cv2.normalize(hist_h, hist_h).flatten()
            hist_s = cv2.normalize(hist_s, hist_s).flatten()
            hist_v = cv2.normalize(hist_v, hist_v).flatten()
            
            return np.concatenate([hist_h, hist_s, hist_v])
            
        except Exception as e:
            self.logger.warning(f"HSV histogram extraction failed: {e}")
            return None
    
    def _extract_gray_histogram(self, roi: np.ndarray) -> Optional[np.ndarray]:
        """Extract grayscale histogram"""
        try:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            hist = cv2.calcHist([gray], [0], None, [self.hist_bins], [0, 256])
            hist = cv2.normalize(hist, hist).flatten()
            return hist
            
        except Exception as e:
            self.logger.warning(f"Gray histogram extraction failed: {e}")
            return None
    
    def _extract_texture_features(self, roi: np.ndarray) -> Optional[np.ndarray]:
        """Extract texture features using Local Binary Patterns"""
        try:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            # Simple LBP implementation
            lbp = self._local_binary_pattern(gray, self.lbp_points, self.lbp_radius)
            
            # Calculate LBP histogram
            hist, _ = np.histogram(lbp.ravel(), bins=self.lbp_points + 2, 
                                 range=(0, self.lbp_points + 2), density=True)
            
            return hist
            
        except Exception as e:
            self.logger.warning(f"Texture feature extraction failed: {e}")
            return None
    
    def _local_binary_pattern(self, image: np.ndarray, points: int, radius: int) -> np.ndarray:
        """Simplified Local Binary Pattern implementation"""
        rows, cols = image.shape
        lbp = np.zeros_like(image)
        
        for i in range(radius, rows - radius):
            for j in range(radius, cols - radius):
                center = image[i, j]
                pattern = 0
                
                # Sample points around the center
                for p in range(points):
                    angle = 2 * np.pi * p / points
                    x = int(i + radius * np.cos(angle))
                    y = int(j + radius * np.sin(angle))
                    
                    if 0 <= x < rows and 0 <= y < cols:
                        if image[x, y] > center:
                            pattern |= (1 << p)
                
                lbp[i, j] = pattern
        
        return lbp
    
    def _extract_shape_features(self, roi: np.ndarray) -> Optional[np.ndarray]:
        """Extract shape-based features"""
        try:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            # Find contours
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if not contours:
                return np.array([0.0, 0.0, 0.0, 0.0])  # Default shape features
            
            # Get largest contour
            largest_contour = max(contours, key=cv2.contourArea)
            
            # Calculate shape features
            area = cv2.contourArea(largest_contour)
            perimeter = cv2.arcLength(largest_contour, True)
            
            # Aspect ratio
            x, y, w, h = cv2.boundingRect(largest_contour)
            aspect_ratio = w / h if h > 0 else 1.0
            
            # Extent (area / bounding box area)
            extent = area / (w * h) if w * h > 0 else 0.0
            
            # Solidity (area / convex hull area)
            hull = cv2.convexHull(largest_contour)
            hull_area = cv2.contourArea(hull)
            solidity = area / hull_area if hull_area > 0 else 0.0
            
            return np.array([aspect_ratio, extent, solidity, perimeter / area if area > 0 else 0.0])
            
        except Exception as e:
            self.logger.warning(f"Shape feature extraction failed: {e}")
            return np.array([0.0, 0.0, 0.0, 0.0])
    
    def _extract_size_features(self, bbox: np.ndarray) -> np.ndarray:
        """Extract normalized size features"""
        x1, y1, x2, y2 = bbox
        width = x2 - x1
        height = y2 - y1
        area = width * height
        aspect_ratio = width / height if height > 0 else 1.0
        
        # Normalize features (assuming typical image size of 1920x1080)
        normalized_width = width / 1920.0
        normalized_height = height / 1080.0
        normalized_area = area / (1920.0 * 1080.0)
        
        return np.array([normalized_width, normalized_height, normalized_area, aspect_ratio])
    
    def _calculate_extraction_quality(self, roi: np.ndarray, bbox: np.ndarray) -> float:
        """Calculate quality score for feature extraction"""
        try:
            # Size quality (larger regions are better)
            size = roi.shape[0] * roi.shape[1]
            size_quality = min(1.0, size / (self.min_bbox_size * self.min_bbox_size * 4))
            
            # Contrast quality (higher contrast is better)
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            contrast = np.std(gray) / 255.0
            contrast_quality = min(1.0, contrast * 2)
            
            # Sharpness quality (edge strength)
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            sharpness_quality = min(1.0, edge_density * 10)
            
            # Combined quality
            quality = (size_quality + contrast_quality + sharpness_quality) / 3.0
            return max(0.1, min(1.0, quality))  # Clamp between 0.1 and 1.0
            
        except Exception:
            return 0.5  # Default quality
    
    def _validate_bbox(self, bbox: np.ndarray, frame_shape: tuple) -> bool:
        """Validate bounding box dimensions"""
        x1, y1, x2, y2 = bbox
        height, width = frame_shape[:2]
        
        # Check bounds
        if x1 < 0 or y1 < 0 or x2 >= width or y2 >= height:
            return False
        
        # Check minimum size
        if (x2 - x1) < self.min_bbox_size or (y2 - y1) < self.min_bbox_size:
            return False
        
        return True
    
    def _compare_histograms(self, hist1: np.ndarray, hist2: np.ndarray) -> float:
        """Compare two histograms using correlation"""
        try:
            # Use correlation method (returns -1 to 1, we convert to 0 to 1)
            correlation = cv2.compareHist(hist1.astype(np.float32), 
                                        hist2.astype(np.float32), 
                                        cv2.HISTCMP_CORREL)
            return max(0.0, correlation)  # Convert to 0-1 range
            
        except Exception:
            return 0.0
    
    def _compare_feature_vectors(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Compare two feature vectors using cosine similarity"""
        try:
            if len(vec1) != len(vec2):
                return 0.0
            
            vec1 = vec1.reshape(1, -1)
            vec2 = vec2.reshape(1, -1)
            
            # Cosine similarity
            similarity = cosine_similarity(vec1, vec2)[0, 0]
            return max(0.0, similarity)  # Ensure non-negative
            
        except Exception:
            return 0.0
    
    def get_feature_statistics(self) -> Dict[str, str]:
        """Get statistics about feature extraction"""
        return {
            'histogram_bins': self.hist_bins,
            'lbp_parameters': f"radius={self.lbp_radius}, points={self.lbp_points}",
            'min_bbox_size': self.min_bbox_size,
            'feature_weights': str(self.feature_weights)
        }

================
File: argus_track/core/__init__.py
================
"""Core data structures for ByteTrack Light Post Tracking System"""

from .detection import Detection
from .track import Track
from .gps import GPSData

__all__ = ["Detection", "Track", "GPSData"]

================
File: argus_track/core/detection.py
================
"""Detection data structure"""

from dataclasses import dataclass
import numpy as np


@dataclass
class Detection:
    """Single object detection"""
    bbox: np.ndarray                   # [x1, y1, x2, y2] format
    score: float                       # Confidence score [0, 1]
    class_id: int                      # Object class ID
    frame_id: int                      # Frame number
    
    @property
    def tlbr(self) -> np.ndarray:
        """Get bounding box in top-left, bottom-right format"""
        return self.bbox
    
    @property
    def xywh(self) -> np.ndarray:
        """Get bounding box in center-x, center-y, width, height format"""
        x1, y1, x2, y2 = self.bbox
        return np.array([
            (x1 + x2) / 2,  # center x
            (y1 + y2) / 2,  # center y
            x2 - x1,        # width
            y2 - y1         # height
        ])
    
    @property
    def area(self) -> float:
        """Calculate bounding box area"""
        x1, y1, x2, y2 = self.bbox
        return (x2 - x1) * (y2 - y1)
    
    @property
    def center(self) -> np.ndarray:
        """Get center point of bounding box"""
        x1, y1, x2, y2 = self.bbox
        return np.array([(x1 + x2) / 2, (y1 + y2) / 2])
    
    def to_dict(self) -> dict:
        """Convert to dictionary representation"""
        return {
            'bbox': self.bbox.tolist(),
            'score': self.score,
            'class_id': self.class_id,
            'frame_id': self.frame_id
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'Detection':
        """Create from dictionary representation"""
        return cls(
            bbox=np.array(data['bbox']),
            score=data['score'],
            class_id=data['class_id'],
            frame_id=data['frame_id']
        )

================
File: argus_track/core/gps.py
================
"""GPS data structure"""

from dataclasses import dataclass
from typing import Dict, Any


@dataclass
class GPSData:
    """GPS data for a single frame"""
    timestamp: float
    latitude: float
    longitude: float
    altitude: float
    heading: float
    accuracy: float = 1.0              # GPS accuracy in meters
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'timestamp': self.timestamp,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'altitude': self.altitude,
            'heading': self.heading,
            'accuracy': self.accuracy
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'GPSData':
        """Create from dictionary representation"""
        return cls(**data)
    
    @classmethod
    def from_csv_line(cls, line: str) -> 'GPSData':
        """Create from CSV line"""
        parts = line.strip().split(',')
        if len(parts) < 5:
            raise ValueError(f"Invalid GPS data line: {line}")
        
        return cls(
            timestamp=float(parts[0]),
            latitude=float(parts[1]),
            longitude=float(parts[2]),
            altitude=float(parts[3]),
            heading=float(parts[4]),
            accuracy=float(parts[5]) if len(parts) > 5 else 1.0
        )

================
File: argus_track/core/stereo.py
================
# argus_track/core/stereo.py (NEW FILE)

"""Stereo vision data structures and utilities"""

from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Any
import numpy as np

from .detection import Detection


@dataclass
class StereoDetection:
    """Stereo detection pair from left and right cameras"""
    left_detection: Detection
    right_detection: Detection
    disparity: float                   # Pixel disparity between left/right
    depth: float                       # Estimated depth in meters
    world_coordinates: np.ndarray      # 3D coordinates in camera frame
    stereo_confidence: float           # Confidence of stereo match [0,1]
    
    @property
    def center_3d(self) -> np.ndarray:
        """Get 3D center point"""
        return self.world_coordinates
    
    @property
    def left_center(self) -> np.ndarray:
        """Get left camera center point"""
        return self.left_detection.center
    
    @property
    def right_center(self) -> np.ndarray:
        """Get right camera center point"""
        return self.right_detection.center
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'left_detection': self.left_detection.to_dict(),
            'right_detection': self.right_detection.to_dict(),
            'disparity': self.disparity,
            'depth': self.depth,
            'world_coordinates': self.world_coordinates.tolist(),
            'stereo_confidence': self.stereo_confidence
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StereoDetection':
        """Create from dictionary representation"""
        return cls(
            left_detection=Detection.from_dict(data['left_detection']),
            right_detection=Detection.from_dict(data['right_detection']),
            disparity=data['disparity'],
            depth=data['depth'],
            world_coordinates=np.array(data['world_coordinates']),
            stereo_confidence=data['stereo_confidence']
        )


@dataclass
class StereoFrame:
    """Stereo frame pair with synchronized detections"""
    frame_id: int
    timestamp: float
    left_frame: np.ndarray
    right_frame: np.ndarray
    left_detections: List[Detection]
    right_detections: List[Detection]
    stereo_detections: List[StereoDetection]
    gps_data: Optional['GPSData'] = None
    
    @property
    def has_gps(self) -> bool:
        """Check if frame has GPS data"""
        return self.gps_data is not None
    
    def get_stereo_count(self) -> int:
        """Get number of successful stereo matches"""
        return len(self.stereo_detections)


@dataclass
class StereoTrack:
    """Extended track with stereo 3D information"""
    track_id: int
    stereo_detections: List[StereoDetection]
    world_trajectory: List[np.ndarray]  # 3D trajectory in world coordinates
    gps_trajectory: List[np.ndarray]    # GPS coordinate trajectory
    estimated_location: Optional['GeoLocation'] = None
    depth_consistency: float = 0.0      # Measure of depth consistency
    
    @property
    def is_static_3d(self) -> bool:
        """Check if object is static in 3D space"""
        if len(self.world_trajectory) < 3:
            return False
        
        positions = np.array(self.world_trajectory)
        std_dev = np.std(positions, axis=0)
        
        # Object is static if movement in any axis is < 1 meter
        return np.all(std_dev < 1.0)
    
    @property
    def average_depth(self) -> float:
        """Get average depth of all detections"""
        if not self.stereo_detections:
            return 0.0
        return np.mean([det.depth for det in self.stereo_detections])
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'track_id': self.track_id,
            'stereo_detections': [det.to_dict() for det in self.stereo_detections[-10:]],  # Last 10
            'world_trajectory': [pos.tolist() for pos in self.world_trajectory[-20:]],     # Last 20
            'gps_trajectory': [pos.tolist() for pos in self.gps_trajectory[-20:]],        # Last 20
            'estimated_location': self.estimated_location.__dict__ if self.estimated_location else None,
            'depth_consistency': self.depth_consistency,
            'is_static_3d': self.is_static_3d,
            'average_depth': self.average_depth
        }

================
File: argus_track/detectors/base.py
================
"""Base detector interface"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any
import numpy as np


class ObjectDetector(ABC):
    """Abstract base class for object detection modules"""
    
    @abstractmethod
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect objects in a frame
        
        Args:
            frame: Input image as numpy array
            
        Returns:
            List of detections with keys: bbox, score, class_name, class_id
        """
        pass
    
    @abstractmethod
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        pass
    
    def set_target_classes(self, target_classes: List[str]) -> None:
        """Set specific classes to detect"""
        self.target_classes = target_classes

================
File: argus_track/stereo/__init__.py
================
"""Stereo vision processing modules"""

from .matching import StereoMatcher
from .triangulation import StereoTriangulator
from .calibration import StereoCalibrationManager

__all__ = ["StereoMatcher", "StereoTriangulator", "StereoCalibrationManager"]

================
File: argus_track/stereo/calibration.py
================
# argus_track/stereo/calibration.py (NEW FILE)

"""Stereo camera calibration management"""

import cv2
import numpy as np
import pickle
from typing import Optional, Tuple, List
import logging
from pathlib import Path

from ..config import StereoCalibrationConfig


class StereoCalibrationManager:
    """
    Manages stereo camera calibration data and provides rectification utilities
    """
    
    def __init__(self, calibration: Optional[StereoCalibrationConfig] = None):
        """
        Initialize calibration manager
        
        Args:
            calibration: Pre-loaded calibration data
        """
        self.calibration = calibration
        self.logger = logging.getLogger(f"{__name__}.StereoCalibrationManager")
        
        # Rectification maps (computed when needed)
        self.left_map1 = None
        self.left_map2 = None
        self.right_map1 = None
        self.right_map2 = None
        
    @classmethod
    def from_pickle_file(cls, calibration_path: str) -> 'StereoCalibrationManager':
        """
        Load calibration from pickle file
        
        Args:
            calibration_path: Path to calibration pickle file
            
        Returns:
            StereoCalibrationManager instance
        """
        calibration = StereoCalibrationConfig.from_pickle(calibration_path)
        return cls(calibration)
    
    def compute_rectification_maps(self, 
                                  image_size: Optional[Tuple[int, int]] = None,
                                  alpha: float = 0.0) -> bool:
        """
        Compute rectification maps for stereo pair
        
        Args:
            image_size: (width, height) of images, uses calibration size if None
            alpha: Free scaling parameter (0=crop, 1=no crop)
            
        Returns:
            True if successful
        """
        if self.calibration is None:
            self.logger.error("No calibration data available")
            return False
        
        if image_size is None:
            image_size = (self.calibration.image_width, self.calibration.image_height)
        
        try:
            # Compute rectification transforms
            R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
                self.calibration.camera_matrix_left,
                self.calibration.dist_coeffs_left,
                self.calibration.camera_matrix_right,
                self.calibration.dist_coeffs_right,
                image_size,
                self.calibration.R,
                self.calibration.T,
                alpha=alpha
            )
            
            # Update calibration with computed matrices
            self.calibration.P1 = P1
            self.calibration.P2 = P2
            self.calibration.Q = Q
            
            # Compute rectification maps
            self.left_map1, self.left_map2 = cv2.initUndistortRectifyMap(
                self.calibration.camera_matrix_left,
                self.calibration.dist_coeffs_left,
                R1, P1, image_size,
                cv2.CV_32FC1
            )
            
            self.right_map1, self.right_map2 = cv2.initUndistortRectifyMap(
                self.calibration.camera_matrix_right,
                self.calibration.dist_coeffs_right,
                R2, P2, image_size,
                cv2.CV_32FC1
            )
            
            self.logger.info("Successfully computed rectification maps")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to compute rectification maps: {e}")
            return False
    
    def rectify_image_pair(self, 
                          left_image: np.ndarray, 
                          right_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Rectify stereo image pair
        
        Args:
            left_image: Left camera image
            right_image: Right camera image
            
        Returns:
            (rectified_left, rectified_right) images
        """
        if self.left_map1 is None or self.left_map2 is None:
            # Compute maps if not available
            image_size = (left_image.shape[1], left_image.shape[0])
            if not self.compute_rectification_maps(image_size):
                self.logger.warning("Using non-rectified images")
                return left_image, right_image
        
        # Apply rectification
        left_rectified = cv2.remap(
            left_image, self.left_map1, self.left_map2, cv2.INTER_LINEAR
        )
        right_rectified = cv2.remap(
            right_image, self.right_map1, self.right_map2, cv2.INTER_LINEAR
        )
        
        return left_rectified, right_rectified
    
    def validate_calibration(self) -> Tuple[bool, List[str]]:
        """
        Validate calibration data
        
        Returns:
            (is_valid, error_messages)
        """
        if self.calibration is None:
            return False, ["No calibration data loaded"]
        
        errors = []
        
        # Check camera matrices
        if self.calibration.camera_matrix_left.shape != (3, 3):
            errors.append("Invalid left camera matrix shape")
        
        if self.calibration.camera_matrix_right.shape != (3, 3):
            errors.append("Invalid right camera matrix shape")
        
        # Check distortion coefficients
        if len(self.calibration.dist_coeffs_left) < 4:
            errors.append("Invalid left distortion coefficients")
        
        if len(self.calibration.dist_coeffs_right) < 4:
            errors.append("Invalid right distortion coefficients")
        
        # Check rotation and translation
        if self.calibration.R.shape != (3, 3):
            errors.append("Invalid rotation matrix shape")
        
        if self.calibration.T.shape != (3, 1) and self.calibration.T.shape != (3,):
            errors.append("Invalid translation vector shape")
        
        # Check baseline
        if self.calibration.baseline <= 0:
            # Try to compute from translation vector
            if self.calibration.T.shape == (3, 1):
                baseline = float(np.linalg.norm(self.calibration.T))
            else:
                baseline = float(np.linalg.norm(self.calibration.T))
            
            if baseline <= 0:
                errors.append("Invalid baseline distance")
            else:
                self.calibration.baseline = baseline
                self.logger.info(f"Computed baseline: {baseline:.3f}m")
        
        # Check image dimensions
        if self.calibration.image_width <= 0 or self.calibration.image_height <= 0:
            errors.append("Invalid image dimensions")
        
        is_valid = len(errors) == 0
        
        if is_valid:
            self.logger.info("Calibration validation passed")
        else:
            self.logger.error(f"Calibration validation failed: {errors}")
        
        return is_valid, errors
    
    def get_calibration_summary(self) -> dict:
        """Get summary of calibration parameters"""
        if self.calibration is None:
            return {"status": "No calibration loaded"}
        
        return {
            "baseline": f"{self.calibration.baseline:.3f}m",
            "image_size": f"{self.calibration.image_width}x{self.calibration.image_height}",
            "left_focal_length": f"{self.calibration.camera_matrix_left[0,0]:.1f}px",
            "right_focal_length": f"{self.calibration.camera_matrix_right[0,0]:.1f}px",
            "has_rectification": self.calibration.P1 is not None,
            "has_maps": self.left_map1 is not None
        }
    
    def create_sample_calibration(self, 
                                 image_width: int = 1920,
                                 image_height: int = 1080,
                                 baseline: float = 0.12) -> StereoCalibrationConfig:
        """
        Create sample calibration for testing (GoPro Hero 11 approximate values)
        
        Args:
            image_width: Image width in pixels
            image_height: Image height in pixels
            baseline: Baseline distance in meters
            
        Returns:
            Sample calibration configuration
        """
        # Approximate GoPro Hero 11 parameters
        focal_length = 1400  # pixels
        cx = image_width / 2
        cy = image_height / 2
        
        # Camera matrices
        camera_matrix = np.array([
            [focal_length, 0, cx],
            [0, focal_length, cy],
            [0, 0, 1]
        ], dtype=np.float64)
        
        # Distortion coefficients (approximate for GoPro)
        dist_coeffs = np.array([-0.3, 0.1, 0, 0, 0], dtype=np.float64)
        
        # Stereo parameters (assuming cameras are aligned horizontally)
        R = np.eye(3, dtype=np.float64)  # No rotation between cameras
        T = np.array([[baseline], [0], [0]], dtype=np.float64)  # Horizontal translation
        
        calibration = StereoCalibrationConfig(
            camera_matrix_left=camera_matrix,
            camera_matrix_right=camera_matrix,
            dist_coeffs_left=dist_coeffs,
            dist_coeffs_right=dist_coeffs,
            R=R,
            T=T,
            baseline=baseline,
            image_width=image_width,
            image_height=image_height
        )
        
        self.logger.info(f"Created sample calibration with {baseline}m baseline")
        return calibration
    
    def save_calibration(self, output_path: str) -> bool:
        """
        Save calibration to pickle file
        
        Args:
            output_path: Path for output file
            
        Returns:
            True if successful
        """
        if self.calibration is None:
            self.logger.error("No calibration to save")
            return False
        
        try:
            self.calibration.save_pickle(output_path)
            self.logger.info(f"Saved calibration to {output_path}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to save calibration: {e}")
            return False

================
File: argus_track/stereo/triangulation.py
================
# argus_track/stereo/triangulation.py (NEW FILE)

"""3D triangulation and coordinate transformation"""

import cv2
import numpy as np
from typing import List, Tuple, Optional
import logging

from ..core.stereo import StereoDetection
from ..config import StereoCalibrationConfig
from ..core import GPSData
from ..utils.gps_utils import CoordinateTransformer, GeoLocation


class StereoTriangulator:
    """
    Handles 3D triangulation and coordinate system transformations
    from camera coordinates to world coordinates to GPS coordinates.
    """
    
    def __init__(self, 
                 calibration: StereoCalibrationConfig,
                 coordinate_transformer: Optional[CoordinateTransformer] = None):
        """
        Initialize triangulator
        
        Args:
            calibration: Stereo camera calibration
            coordinate_transformer: GPS coordinate transformer
        """
        self.calibration = calibration
        self.coordinate_transformer = coordinate_transformer
        self.logger = logging.getLogger(f"{__name__}.StereoTriangulator")
        
        # Camera extrinsics (if available)
        self.camera_position = None  # GPS position of camera
        self.camera_orientation = None  # Camera orientation relative to world
        
    def set_camera_pose(self, 
                       gps_position: GPSData, 
                       orientation_angles: Optional[Tuple[float, float, float]] = None):
        """
        Set camera pose for world coordinate transformation
        
        Args:
            gps_position: GPS position of the camera
            orientation_angles: (roll, pitch, yaw) in degrees
        """
        self.camera_position = gps_position
        if orientation_angles:
            self.camera_orientation = np.array(orientation_angles) * np.pi / 180  # Convert to radians
        
        # Update coordinate transformer
        if self.coordinate_transformer is None:
            self.coordinate_transformer = CoordinateTransformer(
                reference_lat=gps_position.latitude,
                reference_lon=gps_position.longitude
            )
    
    def triangulate_points(self, stereo_detections: List[StereoDetection]) -> List[np.ndarray]:
        """
        Triangulate 3D points from stereo detections
        
        Args:
            stereo_detections: List of stereo detection pairs
            
        Returns:
            List of 3D points in camera coordinate system
        """
        points_3d = []
        
        for stereo_det in stereo_detections:
            # Get 2D points
            left_point = stereo_det.left_detection.center
            right_point = stereo_det.right_detection.center
            
            # Triangulate
            point_3d = self._triangulate_single_point(left_point, right_point)
            points_3d.append(point_3d)
        
        return points_3d
    
    def _triangulate_single_point(self, 
                                 left_point: np.ndarray, 
                                 right_point: np.ndarray) -> np.ndarray:
        """Triangulate single 3D point from stereo pair"""
        
        # Prepare points for OpenCV triangulation
        left_pt = left_point.reshape(2, 1).astype(np.float32)
        right_pt = right_point.reshape(2, 1).astype(np.float32)
        
        # Use projection matrices if available
        if self.calibration.P1 is not None and self.calibration.P2 is not None:
            points_4d = cv2.triangulatePoints(
                self.calibration.P1,
                self.calibration.P2,
                left_pt,
                right_pt
            )
            
            # Convert from homogeneous coordinates
            if points_4d[3, 0] != 0:
                point_3d = points_4d[:3, 0] / points_4d[3, 0]
            else:
                point_3d = points_4d[:3, 0]
                
            return point_3d
        else:
            # Fallback triangulation using basic stereo geometry
            return self._basic_triangulation(left_point, right_point)
    
    def _basic_triangulation(self, left_point: np.ndarray, right_point: np.ndarray) -> np.ndarray:
        """Basic triangulation without projection matrices"""
        # Calculate disparity
        disparity = left_point[0] - right_point[0]
        
        if disparity <= 0:
            return np.array([0, 0, float('inf')])
        
        # Camera parameters
        fx = self.calibration.camera_matrix_left[0, 0]
        fy = self.calibration.camera_matrix_left[1, 1]
        cx = self.calibration.camera_matrix_left[0, 2]
        cy = self.calibration.camera_matrix_left[1, 2]
        baseline = self.calibration.baseline
        
        # Calculate depth
        depth = (baseline * fx) / disparity
        
        # Calculate 3D coordinates
        x = (left_point[0] - cx) * depth / fx
        y = (left_point[1] - cy) * depth / fy
        z = depth
        
        return np.array([x, y, z])
    
    def camera_to_world_coordinates(self, 
                                   camera_points: List[np.ndarray],
                                   gps_data: GPSData) -> List[np.ndarray]:
        """
        Transform camera coordinates to world coordinates
        
        Args:
            camera_points: 3D points in camera coordinate system
            gps_data: GPS data for camera pose
            
        Returns:
            3D points in world coordinate system
        """
        world_points = []
        
        for cam_point in camera_points:
            # Apply camera rotation and translation
            world_point = self._transform_camera_to_world(cam_point, gps_data)
            world_points.append(world_point)
        
        return world_points
    
    def _transform_camera_to_world(self, 
                                  camera_point: np.ndarray, 
                                  gps_data: GPSData) -> np.ndarray:
        """Transform single point from camera to world coordinates"""
        
        # If we have camera orientation, apply rotation
        if self.camera_orientation is not None:
            # Create rotation matrix from Euler angles
            roll, pitch, yaw = self.camera_orientation
            
            # Rotation matrices
            Rx = np.array([[1, 0, 0],
                          [0, np.cos(roll), -np.sin(roll)],
                          [0, np.sin(roll), np.cos(roll)]])
            
            Ry = np.array([[np.cos(pitch), 0, np.sin(pitch)],
                          [0, 1, 0],
                          [-np.sin(pitch), 0, np.cos(pitch)]])
            
            Rz = np.array([[np.cos(yaw), -np.sin(yaw), 0],
                          [np.sin(yaw), np.cos(yaw), 0],
                          [0, 0, 1]])
            
            # Combined rotation matrix
            R = Rz @ Ry @ Rx
            
            # Apply rotation
            world_point = R @ camera_point
        else:
            # Assume camera is level and facing forward
            # Simple transformation: camera Z -> world X, camera X -> world Y, camera Y -> world Z
            world_point = np.array([camera_point[2], camera_point[0], -camera_point[1]])
        
        return world_point
    
    def world_to_gps_coordinates(self, 
                                world_points: List[np.ndarray],
                                reference_gps: GPSData) -> List[GeoLocation]:
        """
        Convert world coordinates to GPS coordinates
        
        Args:
            world_points: 3D points in world coordinate system
            reference_gps: Reference GPS position
            
        Returns:
            List of GPS locations
        """
        if self.coordinate_transformer is None:
            self.coordinate_transformer = CoordinateTransformer(
                reference_lat=reference_gps.latitude,
                reference_lon=reference_gps.longitude
            )
        
        gps_locations = []
        
        for world_point in world_points:
            # Use X, Y coordinates for GPS conversion (ignore Z/altitude)
            local_x = world_point[0]
            local_y = world_point[1]
            
            # Convert to GPS
            lat, lon = self.coordinate_transformer.local_to_gps(local_x, local_y)
            
            # Create GeoLocation with estimated accuracy
            location = GeoLocation(
                latitude=lat,
                longitude=lon,
                accuracy=self._estimate_gps_accuracy(world_point),
                reliability=0.8,  # Base reliability for stereo triangulation
                timestamp=reference_gps.timestamp
            )
            
            gps_locations.append(location)
        
        return gps_locations
    
    def _estimate_gps_accuracy(self, world_point: np.ndarray) -> float:
        """Estimate GPS accuracy based on triangulation quality"""
        # Accuracy degrades with distance
        distance = np.linalg.norm(world_point)
        
        # Base accuracy (1m) + distance-dependent error
        base_accuracy = 1.0
        distance_error = distance * 0.01  # 1cm per meter of distance
        
        estimated_accuracy = base_accuracy + distance_error
        
        # Cap at reasonable maximum
        return min(estimated_accuracy, 10.0)
    
    def estimate_object_location(self, 
                                stereo_track: 'StereoTrack',
                                gps_history: List[GPSData]) -> Optional[GeoLocation]:
        """
        Estimate final GPS location for a static object track
        
        Args:
            stereo_track: Stereo track with 3D trajectory
            gps_history: GPS data history for the track
            
        Returns:
            Estimated GPS location or None
        """
        if not stereo_track.is_static_3d or len(gps_history) < 3:
            return None
        
        # Get all world coordinates for the track
        world_coords = stereo_track.world_trajectory
        
        if len(world_coords) < 3:
            return None
        
        # Calculate average world position
        avg_world_pos = np.mean(world_coords, axis=0)
        
        # Use middle GPS point as reference
        mid_gps = gps_history[len(gps_history) // 2]
        
        # Convert to GPS
        gps_locations = self.world_to_gps_coordinates([avg_world_pos], mid_gps)
        
        if gps_locations:
            location = gps_locations[0]
            
            # Calculate reliability based on trajectory consistency
            if len(world_coords) > 1:
                positions = np.array(world_coords)
                std_dev = np.std(positions, axis=0)
                max_std = np.max(std_dev)
                
                # High reliability if standard deviation is low
                reliability = 1.0 / (1.0 + max_std)
                location.reliability = min(1.0, max(0.1, reliability))
            
            return location
        
        return None
    
    def validate_triangulation(self, 
                              stereo_detection: StereoDetection,
                              max_depth: float = 100.0,
                              min_depth: float = 1.0) -> bool:
        """
        Validate triangulation result
        
        Args:
            stereo_detection: Stereo detection to validate
            max_depth: Maximum reasonable depth
            min_depth: Minimum reasonable depth
            
        Returns:
            True if triangulation is valid
        """
        depth = stereo_detection.depth
        
        # Check depth range
        if not (min_depth <= depth <= max_depth):
            return False
        
        # Check if 3D coordinates are reasonable
        world_coords = stereo_detection.world_coordinates
        
        # Check for NaN or infinite values
        if not np.all(np.isfinite(world_coords)):
            return False
        
        # Check if coordinates are within reasonable bounds
        max_coord = 1000.0  # 1km from camera
        if np.any(np.abs(world_coords) > max_coord):
            return False
        
        return True

================
File: argus_track/utils/gps_movement_filter.py
================
# argus_track/utils/gps_movement_filter.py - NEW FILE

"""
GPS Movement Filter - Skip frames when GPS position doesn't change significantly
"""

import numpy as np
import logging
from typing import List, Optional, Tuple
from dataclasses import dataclass

from ..core import GPSData

@dataclass
class MovementFilterConfig:
    """Configuration for GPS movement filtering"""
    min_movement_meters: float = 1.0      # Minimum movement to consider as "moving"
    stationary_frame_threshold: int = 10   # Frames before considering stationary
    max_stationary_skip: int = 300         # Maximum frames to skip when stationary
    enable_altitude_check: bool = False    # Whether to include altitude in movement calculation
    movement_smoothing_window: int = 3     # Frames to average for movement calculation


class GPSMovementFilter:
    """
    Filter GPS data to skip frames when camera is stationary
    
    This helps focus processing on frames where the camera is moving,
    which provides better triangulation angles for static object detection.
    """
    
    def __init__(self, config: MovementFilterConfig):
        """
        Initialize GPS movement filter
        
        Args:
            config: Movement filter configuration
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.GPSMovementFilter")
        
        # State tracking
        self.gps_history: List[GPSData] = []
        self.movement_history: List[float] = []
        self.stationary_frame_count = 0
        self.total_frames_processed = 0
        self.total_frames_skipped = 0
        
        # Statistics
        self.movement_stats = {
            'total_distance': 0.0,
            'max_movement': 0.0,
            'avg_movement': 0.0,
            'stationary_periods': 0,
            'moving_periods': 0
        }
        
        self.logger.info(f"GPS Movement Filter initialized:")
        self.logger.info(f"  Min movement: {config.min_movement_meters}m")
        self.logger.info(f"  Stationary threshold: {config.stationary_frame_threshold} frames")
        self.logger.info(f"  Max skip frames: {config.max_stationary_skip}")
    
    def should_process_frame(self, gps_data: GPSData, frame_id: int) -> bool:
        """
        Determine if frame should be processed based on GPS movement
        
        Args:
            gps_data: Current GPS data
            frame_id: Current frame number
            
        Returns:
            True if frame should be processed, False if it should be skipped
        """
        # Always process first few frames
        if len(self.gps_history) < 2:
            self._add_gps_data(gps_data)
            self.total_frames_processed += 1
            return True
        
        # Calculate movement from recent position
        movement_distance = self._calculate_movement_distance(gps_data)
        self.movement_history.append(movement_distance)
        
        # Keep movement history manageable
        if len(self.movement_history) > 50:
            self.movement_history = self.movement_history[-50:]
        
        # Update statistics
        self.movement_stats['total_distance'] += movement_distance
        self.movement_stats['max_movement'] = max(self.movement_stats['max_movement'], movement_distance)
        
        # Calculate smoothed movement (average over recent frames)
        recent_movements = self.movement_history[-self.config.movement_smoothing_window:]
        smoothed_movement = np.mean(recent_movements)
        
        # Determine if camera is moving significantly
        is_moving = smoothed_movement >= self.config.min_movement_meters
        
        if is_moving:
            # Camera is moving - always process these frames
            if self.stationary_frame_count > 0:
                self.logger.debug(f"Frame {frame_id}: Movement detected ({smoothed_movement:.2f}m) - "
                               f"ending stationary period of {self.stationary_frame_count} frames")
                self.movement_stats['stationary_periods'] += 1
                self.stationary_frame_count = 0
            
            self.movement_stats['moving_periods'] += 1
            self._add_gps_data(gps_data)
            self.total_frames_processed += 1
            return True
        
        else:
            # Camera is stationary or moving very little
            self.stationary_frame_count += 1
            
            if self.stationary_frame_count <= self.config.stationary_frame_threshold:
                # Still within threshold - continue processing
                self.logger.debug(f"Frame {frame_id}: Low movement ({smoothed_movement:.2f}m) - "
                               f"stationary count: {self.stationary_frame_count}/{self.config.stationary_frame_threshold}")
                self._add_gps_data(gps_data)
                self.total_frames_processed += 1
                return True
            
            elif self.stationary_frame_count <= self.config.max_stationary_skip:
                # Skip this frame - camera has been stationary too long
                if self.stationary_frame_count == self.config.stationary_frame_threshold + 1:
                    self.logger.info(f"Frame {frame_id}: Camera stationary - skipping frames until movement detected")
                
                self.total_frames_skipped += 1
                return False
            
            else:
                # Force process a frame occasionally even when stationary
                if self.stationary_frame_count % self.config.max_stationary_skip == 0:
                    self.logger.debug(f"Frame {frame_id}: Forced processing after {self.stationary_frame_count} stationary frames")
                    self._add_gps_data(gps_data)
                    self.total_frames_processed += 1
                    return True
                else:
                    self.total_frames_skipped += 1
                    return False
    
    def _calculate_movement_distance(self, current_gps: GPSData) -> float:
        """Calculate movement distance from previous GPS position"""
        if not self.gps_history:
            return 0.0
        
        # Use most recent GPS position for comparison
        previous_gps = self.gps_history[-1]
        
        # Calculate distance using Haversine formula
        distance = self._haversine_distance(
            previous_gps.latitude, previous_gps.longitude,
            current_gps.latitude, current_gps.longitude
        )
        
        # Include altitude if enabled
        if self.config.enable_altitude_check:
            altitude_diff = abs(current_gps.altitude - previous_gps.altitude)
            distance = np.sqrt(distance**2 + altitude_diff**2)
        
        return distance
    
    def _haversine_distance(self, lat1: float, lon1: float, 
                           lat2: float, lon2: float) -> float:
        """
        Calculate the great circle distance between two points on Earth
        
        Args:
            lat1, lon1: First point coordinates
            lat2, lon2: Second point coordinates
            
        Returns:
            Distance in meters
        """
        # Earth's radius in meters
        R = 6378137.0
        
        # Convert to radians
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        # Haversine formula
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c
    
    def _add_gps_data(self, gps_data: GPSData):
        """Add GPS data to history"""
        self.gps_history.append(gps_data)
        
        # Keep history manageable
        if len(self.gps_history) > 100:
            self.gps_history = self.gps_history[-100:]
    
    def get_movement_statistics(self) -> dict:
        """Get movement filtering statistics"""
        total_frames = self.total_frames_processed + self.total_frames_skipped
        
        if self.movement_history:
            self.movement_stats['avg_movement'] = np.mean(self.movement_history)
        
        return {
            'total_frames': total_frames,
            'processed_frames': self.total_frames_processed,
            'skipped_frames': self.total_frames_skipped,
            'skip_ratio': self.total_frames_skipped / total_frames if total_frames > 0 else 0,
            'movement_stats': self.movement_stats,
            'current_stationary_count': self.stationary_frame_count,
            'efficiency_gain': f"{self.total_frames_skipped / total_frames * 100:.1f}%" if total_frames > 0 else "0%"
        }
    
    def reset(self):
        """Reset filter state"""
        self.gps_history.clear()
        self.movement_history.clear()
        self.stationary_frame_count = 0
        self.total_frames_processed = 0
        self.total_frames_skipped = 0
        self.movement_stats = {
            'total_distance': 0.0,
            'max_movement': 0.0,
            'avg_movement': 0.0,
            'stationary_periods': 0,
            'moving_periods': 0
        }


def create_movement_filter(min_movement_meters: float = 1.0,
                          stationary_threshold: int = 10,
                          max_skip_frames: int = 300) -> GPSMovementFilter:
    """
    Create a GPS movement filter with specified parameters
    
    Args:
        min_movement_meters: Minimum movement to consider as moving (default: 1.0m)
        stationary_threshold: Frames before considering stationary (default: 10)
        max_skip_frames: Maximum frames to skip when stationary (default: 300)
        
    Returns:
        Configured GPS movement filter
    """
    config = MovementFilterConfig(
        min_movement_meters=min_movement_meters,
        stationary_frame_threshold=stationary_threshold,
        max_stationary_skip=max_skip_frames
    )
    
    return GPSMovementFilter(config)

================
File: argus_track/utils/gps_sync_tracker.py
================
"""
GPS-synchronized frame processing implementation for Argus Track
FIXED: Only process frames when GPS data is actually available
"""

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Any, Set
import cv2
import time

from ..core import GPSData

# Configure logging
logger = logging.getLogger(__name__)

class GPSSynchronizer:
    """
    Synchronizes video frame processing with actual GPS data points
    ONLY processes frames that have real GPS measurements
    """
    
    def __init__(self, 
                 gps_data: List[GPSData], 
                 video_fps: float, 
                 gps_fps: float = 10.0):
        """
        Initialize GPS synchronizer - FIXED VERSION
        
        Args:
            gps_data: List of actual GPS data points
            video_fps: Video frame rate in FPS
            gps_fps: Expected GPS data rate in Hz (for validation only)
        """
        if not gps_data:
            self.gps_data = []
            self.sync_frames = set()
            self.frame_to_gps = {}
            return
            
        self.gps_data = sorted(gps_data, key=lambda x: x.timestamp)
        self.video_fps = video_fps
        self.gps_fps = gps_fps
        
        # Calculate video start time (assume GPS and video start together)
        self.video_start_time = self.gps_data[0].timestamp
        
        # FIXED: Only create mappings for frames where we have actual GPS data
        self.frame_to_gps: Dict[int, int] = {}  # frame_idx -> gps_data_idx
        self.sync_frames: Set[int] = set()  # Only frames with GPS data
        
        # Generate the mapping - ONLY for actual GPS points
        self._generate_gps_frame_mapping()
        
        logger.info(f"GPS Synchronizer initialized:")
        logger.info(f"  📊 GPS points available: {len(self.gps_data)}")
        logger.info(f"  🎬 Video FPS: {video_fps}")
        logger.info(f"  📍 Frames to process: {len(self.sync_frames)}")
        logger.info(f"  ⏱️  Processing ratio: {len(self.sync_frames)}/{int(video_fps * self._get_video_duration()):.0f} frames")
        
    def _get_video_duration(self) -> float:
        """Calculate video duration based on GPS data"""
        if len(self.gps_data) < 2:
            return 0.0
        return self.gps_data[-1].timestamp - self.gps_data[0].timestamp
    
    def _generate_gps_frame_mapping(self) -> None:
        """Generate mapping ONLY for frames with actual GPS data"""
        if not self.gps_data:
            return
        
        for gps_idx, gps_point in enumerate(self.gps_data):
            # Calculate which video frame corresponds to this GPS timestamp
            time_offset = gps_point.timestamp - self.video_start_time
            frame_number = int(time_offset * self.video_fps)
            
            # Only map frames that are valid
            if frame_number >= 0:
                self.frame_to_gps[frame_number] = gps_idx
                self.sync_frames.add(frame_number)
        
        logger.info(f"GPS-Video Mapping:")
        logger.info(f"  📍 GPS points: {len(self.gps_data)}")
        logger.info(f"  🎬 Mapped frames: {len(self.sync_frames)}")
        
        if self.sync_frames:
            min_frame = min(self.sync_frames)
            max_frame = max(self.sync_frames)
            logger.info(f"  📊 Frame range: {min_frame} to {max_frame}")
            
            # Show actual GPS frequency
            frame_intervals = []
            sorted_frames = sorted(self.sync_frames)
            for i in range(1, len(sorted_frames)):
                interval = sorted_frames[i] - sorted_frames[i-1]
                frame_intervals.append(interval)
            
            if frame_intervals:
                avg_interval = np.mean(frame_intervals)
                actual_gps_freq = self.video_fps / avg_interval if avg_interval > 0 else 0
                logger.info(f"  🔄 Actual GPS frequency: {actual_gps_freq:.1f} Hz")
                logger.info(f"  📏 Average frame interval: {avg_interval:.1f} frames")
    
    def should_process_frame(self, frame_idx: int) -> bool:
        """
        FIXED: Only process frames that have actual GPS data
        
        Args:
            frame_idx: Frame index
            
        Returns:
            True ONLY if frame has GPS data, False otherwise
        """
        return frame_idx in self.sync_frames
    
    def get_gps_for_frame(self, frame_idx: int) -> Optional[GPSData]:
        """
        Get GPS data for a specific frame
        
        Args:
            frame_idx: Frame index
            
        Returns:
            GPS data for the frame or None if not available
        """
        gps_idx = self.frame_to_gps.get(frame_idx)
        if gps_idx is not None and gps_idx < len(self.gps_data):
            return self.gps_data[gps_idx]
        return None
    
    def get_all_sync_frames(self) -> List[int]:
        """
        Get all frames that should be processed (only GPS frames)
        
        Returns:
            Sorted list of frame indices with GPS data
        """
        return sorted(list(self.sync_frames))
    
    def get_sync_frames_count(self) -> int:
        """
        Get number of frames to process (only GPS frames)
        
        Returns:
            Number of frames with GPS data
        """
        return len(self.sync_frames)
    
    def get_next_sync_frame(self, current_frame: int) -> Optional[int]:
        """
        Get the next frame with GPS data
        
        Args:
            current_frame: Current frame index
            
        Returns:
            Next frame index with GPS data or None if no more
        """
        sync_frames = sorted(list(self.sync_frames))
        for frame in sync_frames:
            if frame > current_frame:
                return frame
        return None
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about GPS-synchronized processing
        
        Returns:
            Dictionary with processing statistics
        """
        if not self.sync_frames:
            return {
                'gps_points': 0,
                'sync_frames': 0,
                'processing_ratio': 0.0,
                'avg_gps_frequency': 0.0
            }
        
        sync_frames = sorted(list(self.sync_frames))
        frame_intervals = np.diff(sync_frames) if len(sync_frames) > 1 else [0]
        
        avg_interval = np.mean(frame_intervals) if len(frame_intervals) > 0 else 0
        actual_gps_freq = self.video_fps / avg_interval if avg_interval > 0 else 0
        
        # Calculate total video frames in the GPS time range
        if len(sync_frames) >= 2:
            frame_span = max(sync_frames) - min(sync_frames)
            processing_ratio = len(sync_frames) / (frame_span + 1) if frame_span > 0 else 1.0
        else:
            processing_ratio = 1.0 if sync_frames else 0.0
        
        return {
            'gps_points': len(self.gps_data),
            'sync_frames': len(self.sync_frames),
            'processing_ratio': processing_ratio,
            'avg_gps_frequency': actual_gps_freq,
            'frame_range': (min(sync_frames), max(sync_frames)) if sync_frames else (0, 0),
            'avg_frame_interval': avg_interval
        }


def create_gps_synchronizer(gps_data: List[GPSData], 
                           video_fps: float, 
                           gps_fps: float = 10.0) -> GPSSynchronizer:
    """
    Create a GPS synchronizer for frame processing
    FIXED: Only processes frames with actual GPS data
    
    Args:
        gps_data: List of actual GPS data points
        video_fps: Video frame rate in FPS
        gps_fps: Expected GPS data rate in Hz (for validation)
        
    Returns:
        GPS synchronizer instance
    """
    return GPSSynchronizer(gps_data, video_fps, gps_fps)

================
File: argus_track/utils/gps_utils.py
================
# argus_track/utils/gps_utils.py

"""Enhanced GPS utilities for tracking"""

import numpy as np
from typing import List, Tuple, Optional, Dict
from scipy.interpolate import interp1d
import pyproj
from dataclasses import dataclass

from ..core import GPSData


class GPSInterpolator:
    """Interpolate GPS data between frames"""
    
    def __init__(self, gps_data: List[GPSData]):
        """
        Initialize GPS interpolator
        
        Args:
            gps_data: List of GPS data points
        """
        self.gps_data = sorted(gps_data, key=lambda x: x.timestamp)
        self.timestamps = np.array([gps.timestamp for gps in self.gps_data])
        
        # Create interpolation functions
        self.lat_interp = interp1d(
            self.timestamps,
            [gps.latitude for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
        self.lon_interp = interp1d(
            self.timestamps,
            [gps.longitude for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
        self.heading_interp = interp1d(
            self.timestamps,
            [gps.heading for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
    
    def interpolate(self, timestamp: float) -> GPSData:
        """
        Interpolate GPS data for a specific timestamp
        
        Args:
            timestamp: Target timestamp
            
        Returns:
            Interpolated GPS data
        """
        return GPSData(
            timestamp=timestamp,
            latitude=float(self.lat_interp(timestamp)),
            longitude=float(self.lon_interp(timestamp)),
            altitude=0.0,  # We're not focusing on altitude
            heading=float(self.heading_interp(timestamp)),
            accuracy=1.0  # Interpolated accuracy
        )
    
    def get_range(self) -> Tuple[float, float]:
        """Get timestamp range of GPS data"""
        return self.timestamps[0], self.timestamps[-1]


class CoordinateTransformer:
    """Transform between GPS coordinates and local coordinate systems"""
    
    def __init__(self, reference_lat: float, reference_lon: float):
        """
        Initialize transformer with reference point
        
        Args:
            reference_lat: Reference latitude
            reference_lon: Reference longitude
        """
        self.reference_lat = reference_lat
        self.reference_lon = reference_lon
        
        # Setup projections
        self.wgs84 = pyproj.CRS("EPSG:4326")  # GPS coordinates
        self.utm = pyproj.CRS(f"EPSG:{self._get_utm_zone()}")
        self.transformer = pyproj.Transformer.from_crs(
            self.wgs84, self.utm, always_xy=True
        )
        self.inverse_transformer = pyproj.Transformer.from_crs(
            self.utm, self.wgs84, always_xy=True
        )
        
        # Calculate reference point in UTM
        self.ref_x, self.ref_y = self.transformer.transform(
            reference_lon, reference_lat
        )
    
    def _get_utm_zone(self) -> int:
        """Get UTM zone for reference point"""
        zone = int((self.reference_lon + 180) / 6) + 1
        if self.reference_lat >= 0:
            return 32600 + zone  # Northern hemisphere
        else:
            return 32700 + zone  # Southern hemisphere
    
    def gps_to_local(self, lat: float, lon: float) -> Tuple[float, float]:
        """
        Convert GPS coordinates to local coordinate system
        
        Args:
            lat: Latitude
            lon: Longitude
            
        Returns:
            (x, y) in meters from reference point
        """
        utm_x, utm_y = self.transformer.transform(lon, lat)
        return utm_x - self.ref_x, utm_y - self.ref_y
    
    def local_to_gps(self, x: float, y: float) -> Tuple[float, float]:
        """
        Convert local coordinates to GPS
        
        Args:
            x: X coordinate in meters from reference
            y: Y coordinate in meters from reference
            
        Returns:
            (latitude, longitude)
        """
        utm_x = x + self.ref_x
        utm_y = y + self.ref_y
        lon, lat = self.inverse_transformer.transform(utm_x, utm_y)
        return lat, lon
    
    def distance(self, lat1: float, lon1: float, 
                 lat2: float, lon2: float) -> float:
        """
        Calculate distance between two GPS points
        
        Args:
            lat1, lon1: First point
            lat2, lon2: Second point
            
        Returns:
            Distance in meters
        """
        x1, y1 = self.gps_to_local(lat1, lon1)
        x2, y2 = self.gps_to_local(lat2, lon2)
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)


@dataclass
class GeoLocation:
    """Represents a geographic location with reliability information"""
    latitude: float
    longitude: float
    accuracy: float = 1.0  # Accuracy in meters
    reliability: float = 1.0  # Value between 0 and 1
    timestamp: Optional[float] = None


def sync_gps_with_frames(gps_data: List[GPSData], 
                         video_fps: float,
                         start_timestamp: Optional[float] = None) -> List[GPSData]:
    """
    Synchronize GPS data with video frames
    
    Args:
        gps_data: List of GPS data points
        video_fps: Video frame rate
        start_timestamp: Optional start timestamp
        
    Returns:
        List of GPS data aligned with frames
    """
    if not gps_data:
        return []
    
    # Sort GPS data by timestamp
    gps_data = sorted(gps_data, key=lambda x: x.timestamp)
    
    # Determine start timestamp
    if start_timestamp is None:
        start_timestamp = gps_data[0].timestamp
    
    # Create interpolator
    interpolator = GPSInterpolator(gps_data)
    
    # Generate frame-aligned GPS data
    frame_gps = []
    frame_duration = 1.0 / video_fps
    
    timestamp = start_timestamp
    while timestamp <= gps_data[-1].timestamp:
        frame_gps.append(interpolator.interpolate(timestamp))
        timestamp += frame_duration
    
    return frame_gps


def compute_average_location(locations: List[GPSData]) -> GeoLocation:
    """
    Compute the average location from multiple GPS points
    
    Args:
        locations: List of GPS data points
        
    Returns:
        Average location with reliability score
    """
    if not locations:
        return GeoLocation(0.0, 0.0, 0.0, 0.0)
    
    # Simple weighted average based on accuracy
    weights = np.array([1.0 / max(loc.accuracy, 0.1) for loc in locations])
    weights = weights / np.sum(weights)  # Normalize
    
    avg_lat = np.sum([loc.latitude * w for loc, w in zip(locations, weights)])
    avg_lon = np.sum([loc.longitude * w for loc, w in zip(locations, weights)])
    
    # Calculate reliability based on consistency of points
    if len(locations) > 1:
        # Create transformer using the first point as reference
        transformer = CoordinateTransformer(locations[0].latitude, locations[0].longitude)
        
        # Calculate standard deviation in meters
        distances = []
        for loc in locations:
            dist = transformer.distance(loc.latitude, loc.longitude, avg_lat, avg_lon)
            distances.append(dist)
        
        std_dev = np.std(distances)
        reliability = 1.0 / (1.0 + std_dev / 10.0)  # Decreases with higher standard deviation
        reliability = min(1.0, max(0.1, reliability))  # Clamp between 0.1 and 1.0
    else:
        reliability = 0.5  # Only one point, medium reliability
    
    # Average accuracy is the weighted average of individual accuracies
    avg_accuracy = np.sum([loc.accuracy * w for loc, w in zip(locations, weights)])
    
    # Use the latest timestamp
    latest_timestamp = max([loc.timestamp for loc in locations])
    
    return GeoLocation(
        latitude=avg_lat,
        longitude=avg_lon,
        accuracy=avg_accuracy,
        reliability=reliability,
        timestamp=latest_timestamp
    )


def filter_gps_outliers(locations: List[GPSData], 
                       threshold_meters: float = 30.0) -> List[GPSData]:
    """
    Filter outliers from GPS data using DBSCAN clustering
    
    Args:
        locations: List of GPS data points
        threshold_meters: Distance threshold for outlier detection
        
    Returns:
        Filtered list of GPS data points
    """
    if len(locations) <= 2:
        return locations
    
    from sklearn.cluster import DBSCAN
    
    # Create transformer using the first point as reference
    transformer = CoordinateTransformer(locations[0].latitude, locations[0].longitude)
    
    # Convert to local coordinates
    local_points = []
    for loc in locations:
        x, y = transformer.gps_to_local(loc.latitude, loc.longitude)
        local_points.append([x, y])
    
    # Cluster points
    clustering = DBSCAN(eps=threshold_meters, min_samples=1).fit(local_points)
    
    # Find the largest cluster
    labels = clustering.labels_
    unique_labels, counts = np.unique(labels, return_counts=True)
    largest_cluster = unique_labels[np.argmax(counts)]
    
    # Keep only points from the largest cluster
    filtered_locations = [loc for i, loc in enumerate(locations) if labels[i] == largest_cluster]
    
    return filtered_locations

================
File: argus_track/utils/io.py
================
# argus_track/utils/io.py

"""I/O utilities for loading and saving tracking data"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import csv

import numpy as np

from ..core import GPSData, Track


def setup_logging(log_file: Optional[str] = None, 
                 level: int = logging.INFO) -> None:
    """
    Setup logging configuration
    
    Args:
        log_file: Optional log file path
        level: Logging level
    """
    handlers = [logging.StreamHandler()]
    
    if log_file:
        handlers.append(logging.FileHandler(log_file))
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


def save_tracking_results(tracks: Dict[int, List[Dict]], 
                         output_path: Path,
                         metadata: Optional[Dict[str, Any]] = None,
                         gps_tracks: Optional[Dict[int, List[GPSData]]] = None,
                         track_locations: Optional[Dict[int, Dict]] = None) -> None:
    """
    Save tracking results to JSON file
    
    Args:
        tracks: Dictionary of track histories
        output_path: Path for output file
        metadata: Optional metadata to include
        gps_tracks: Optional GPS data for tracks
        track_locations: Optional estimated locations for tracks
    """
    results = {
        'metadata': metadata or {},
        'tracks': tracks
    }
    
    # Add GPS data if provided
    if gps_tracks:
        results['gps_tracks'] = {
            track_id: [gps.to_dict() for gps in gps_list]
            for track_id, gps_list in gps_tracks.items()
        }
        
    # Add track locations if provided
    if track_locations:
        results['track_locations'] = track_locations
    
    # Save to file
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    logging.info(f"Saved tracking results to {output_path}")


def load_tracking_results(input_path: Path) -> Dict[str, Any]:
    """
    Load tracking results from JSON file
    
    Args:
        input_path: Path to input file
        
    Returns:
        Dictionary with tracking results
    """
    with open(input_path, 'r') as f:
        results = json.load(f)
    
    logging.info(f"Loaded tracking results from {input_path}")
    return results


def load_gps_data(gps_file: str) -> List[GPSData]:
    """
    Load GPS data from file
    
    Args:
        gps_file: Path to GPS data file (CSV format)
        
    Returns:
        List of GPS data points
    """
    gps_data = []
    
    with open(gps_file, 'r') as f:
        reader = csv.reader(f)
        # Skip header if exists
        header = next(reader, None)
        
        try:
            for row in reader:
                if len(row) >= 5:
                    gps_data.append(GPSData(
                        timestamp=float(row[0]),
                        latitude=float(row[1]),
                        longitude=float(row[2]),
                        altitude=float(row[3]) if len(row) > 3 else 0.0,
                        heading=float(row[4]) if len(row) > 4 else 0.0,
                        accuracy=float(row[5]) if len(row) > 5 else 1.0
                    ))
        except ValueError as e:
            logging.error(f"Error parsing GPS data: {e}")
            logging.error(f"Problematic row: {row}")
            raise
    
    logging.info(f"Loaded {len(gps_data)} GPS data points from {gps_file}")
    return gps_data


def save_gps_data(gps_data: List[GPSData], output_path: str) -> None:
    """
    Save GPS data to CSV file
    
    Args:
        gps_data: List of GPS data points
        output_path: Path for output file
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # Write header
        writer.writerow(['timestamp', 'latitude', 'longitude', 
                        'altitude', 'heading', 'accuracy'])
        
        # Write data
        for gps in gps_data:
            writer.writerow([
                gps.timestamp,
                gps.latitude,
                gps.longitude,
                gps.altitude,
                gps.heading,
                gps.accuracy
            ])
    
    logging.info(f"Saved {len(gps_data)} GPS data points to {output_path}")


def export_locations_to_csv(track_locations: Dict[int, Dict],
                           output_path: str) -> None:
    """
    Export estimated track locations to CSV
    
    Args:
        track_locations: Dictionary of track locations
        output_path: Output CSV path
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['track_id', 'latitude', 'longitude', 
                         'accuracy', 'reliability', 'timestamp'])
        
        for track_id, location in track_locations.items():
            writer.writerow([
                track_id,
                location['latitude'],
                location['longitude'],
                location.get('accuracy', 1.0),
                location.get('reliability', 1.0),
                location.get('timestamp', '')
            ])
    
    logging.info(f"Exported {len(track_locations)} locations to CSV: {output_path}")


def export_tracks_to_csv(tracks: Dict[int, Track], 
                        output_path: str) -> None:
    """
    Export track data to CSV format
    
    Args:
        tracks: Dictionary of tracks
        output_path: Path for output CSV file
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # Write header
        writer.writerow(['track_id', 'frame', 'x1', 'y1', 'x2', 'y2', 
                        'state', 'hits', 'age'])
        
        # Write track data
        for track_id, track in tracks.items():
            for detection in track.detections:
                x1, y1, x2, y2 = detection.tlbr
                writer.writerow([
                    track_id,
                    detection.frame_id,
                    x1, y1, x2, y2,
                    track.state,
                    track.hits,
                    track.age
                ])
    
    logging.info(f"Exported tracks to CSV: {output_path}")


def load_config_from_file(config_path: str) -> Dict[str, Any]:
    """
    Load configuration from YAML or JSON file
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Configuration dictionary
    """
    path = Path(config_path)
    
# argus_track/utils/io.py (continued)

    if path.suffix == '.yaml' or path.suffix == '.yml':
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    elif path.suffix == '.json':
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        raise ValueError(f"Unsupported config file format: {path.suffix}")
    
    logging.info(f"Loaded configuration from {config_path}")
    return config


def export_to_geojson(track_locations: Dict[int, Dict], 
                     output_path: str,
                     properties: Optional[Dict[int, Dict]] = None) -> None:
    """
    Export track locations to GeoJSON format
    
    Args:
        track_locations: Dictionary of track locations
        output_path: Path for output GeoJSON file
        properties: Optional additional properties for each feature
    """
    features = []
    
    for track_id, location in track_locations.items():
        # Create basic properties
        feature_props = {
            'track_id': track_id,
            'accuracy': location.get('accuracy', 1.0),
            'reliability': location.get('reliability', 1.0)
        }
        
        # Add additional properties if provided
        if properties and track_id in properties:
            feature_props.update(properties[track_id])
            
        feature = {
            'type': 'Feature',
            'geometry': {
                'type': 'Point',
                'coordinates': [location['longitude'], location['latitude']]
            },
            'properties': feature_props
        }
        
        features.append(feature)
    
    geojson = {
        'type': 'FeatureCollection',
        'features': features
    }
    
    with open(output_path, 'w') as f:
        json.dump(geojson, f, indent=2)
    
    logging.info(f"Exported {len(features)} locations to GeoJSON: {output_path}")

================
File: argus_track/utils/iou.py
================
# argus_track/utils/iou.py

"""IoU (Intersection over Union) utilities for tracking"""

import numpy as np
from typing import List, Union
from numba import jit

from ..core import Track, Detection


@jit(nopython=True)
def calculate_iou_jit(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    """
    Calculate IoU between two bounding boxes (numba accelerated)
    
    Args:
        bbox1: First bbox in [x1, y1, x2, y2] format
        bbox2: Second bbox in [x1, y1, x2, y2] format
        
    Returns:
        IoU value between 0 and 1
    """
    # Get intersection coordinates
    x1 = max(bbox1[0], bbox2[0])
    y1 = max(bbox1[1], bbox2[1])
    x2 = min(bbox1[2], bbox2[2])
    y2 = min(bbox1[3], bbox2[3])
    
    # Calculate intersection area
    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)
    
    # Calculate union area
    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
    bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
    union_area = bbox1_area + bbox2_area - intersection_area
    
    # Avoid division by zero
    if union_area == 0:
        return 0.0
    
    return intersection_area / union_area


def calculate_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    """
    Calculate IoU between two bounding boxes
    
    Args:
        bbox1: First bbox in [x1, y1, x2, y2] format
        bbox2: Second bbox in [x1, y1, x2, y2] format
        
    Returns:
        IoU value between 0 and 1
    """
    return calculate_iou_jit(bbox1, bbox2)


@jit(nopython=True)
def calculate_iou_matrix_jit(bboxes1: np.ndarray, bboxes2: np.ndarray) -> np.ndarray:
    """
    Calculate IoU matrix between two sets of bounding boxes (numba accelerated)
    
    Args:
        bboxes1: First set of bboxes in [N, 4] format
        bboxes2: Second set of bboxes in [M, 4] format
        
    Returns:
        IoU matrix of shape [N, M]
    """
    n_bbox1 = bboxes1.shape[0]
    n_bbox2 = bboxes2.shape[0]
    iou_matrix = np.zeros((n_bbox1, n_bbox2))
    
    for i in range(n_bbox1):
        for j in range(n_bbox2):
            iou_matrix[i, j] = calculate_iou_jit(bboxes1[i], bboxes2[j])
    
    return iou_matrix


def calculate_iou_matrix(tracks_or_bboxes1: Union[List[Track], np.ndarray], 
                         detections_or_bboxes2: Union[List[Detection], np.ndarray]) -> np.ndarray:
    """
    Calculate IoU matrix between tracks and detections
    
    Args:
        tracks_or_bboxes1: List of tracks or array of bboxes
        detections_or_bboxes2: List of detections or array of bboxes
        
    Returns:
        IoU matrix of shape (len(tracks_or_bboxes1), len(detections_or_bboxes2))
    """
    # Handle different input types
    if isinstance(tracks_or_bboxes1, np.ndarray):
        bboxes1 = tracks_or_bboxes1
    else:
        bboxes1 = np.array([track.to_tlbr() for track in tracks_or_bboxes1])
    
    if isinstance(detections_or_bboxes2, np.ndarray):
        bboxes2 = detections_or_bboxes2
    else:
        bboxes2 = np.array([det.tlbr for det in detections_or_bboxes2])
    
    # Calculate IoU matrix
    return calculate_iou_matrix_jit(bboxes1, bboxes2)

================
File: argus_track/utils/kalman_gps_filter.py
================
# Add this to argus_track/utils/kalman_gps_filter.py - NEW FILE

"""
Kalman Filter for GPS Location Deduplication
"""

import numpy as np
from typing import List, Dict, Tuple
import logging
from dataclasses import dataclass
from filterpy.kalman import KalmanFilter


@dataclass 
class GPSMeasurement:
    """Single GPS measurement for a LED"""
    latitude: float
    longitude: float
    detection_count: int
    confidence: float
    distance_m: float
    track_id: int
    

class LEDLocationKalmanFilter:
    """
    Kalman filter for estimating true LED position from multiple GPS measurements
    
    State: [latitude, longitude, lat_velocity, lon_velocity]
    Measurements: [latitude, longitude]
    """
    
    def __init__(self, initial_lat: float, initial_lon: float):
        """
        Initialize Kalman filter for LED location estimation
        
        Args:
            initial_lat: Initial latitude estimate
            initial_lon: Initial longitude estimate
        """
        self.kf = KalmanFilter(dim_x=4, dim_z=2)
        
        # State transition matrix (constant velocity model)
        dt = 1.0  # Time step (not critical for static objects)
        self.kf.F = np.array([
            [1, 0, dt, 0],   # lat = lat + lat_vel*dt
            [0, 1, 0, dt],   # lon = lon + lon_vel*dt  
            [0, 0, 1, 0],    # lat_vel = lat_vel
            [0, 0, 0, 1]     # lon_vel = lon_vel
        ])
        
        # Measurement matrix (we measure lat, lon directly)
        self.kf.H = np.array([
            [1, 0, 0, 0],    # measure latitude
            [0, 1, 0, 0]     # measure longitude
        ])
        
        # Initial state [lat, lon, lat_vel, lon_vel]
        self.kf.x = np.array([initial_lat, initial_lon, 0, 0])
        
        # Initial uncertainty (higher for velocities since LEDs are static)
        self.kf.P = np.diag([1e-6, 1e-6, 1e-8, 1e-8])  # Very low uncertainty for static objects
        
        # Process noise (very low for static LEDs)
        self.kf.Q = np.diag([1e-8, 1e-8, 1e-10, 1e-10])
        
        # Measurement noise (will be updated based on measurement quality)
        self.kf.R = np.diag([1e-6, 1e-6])  # Base measurement noise
        
        self.measurements: List[GPSMeasurement] = []
        self.logger = logging.getLogger(f"{__name__}.LEDLocationKalmanFilter")
    
    def add_measurement(self, measurement: GPSMeasurement):
        """
        Add a GPS measurement and update Kalman filter
        
        Args:
            measurement: GPS measurement for this LED
        """
        self.measurements.append(measurement)
        
        # Calculate measurement noise based on quality
        measurement_noise = self._calculate_measurement_noise(measurement)
        
        # Update measurement noise matrix
        self.kf.R = np.diag([measurement_noise, measurement_noise])
        
        # Kalman predict step
        self.kf.predict()
        
        # Kalman update step
        z = np.array([measurement.latitude, measurement.longitude])
        self.kf.update(z)
        
        self.logger.debug(f"Added measurement for track {measurement.track_id}: "
                         f"({measurement.latitude:.6f}, {measurement.longitude:.6f}), "
                         f"noise: {measurement_noise:.2e}")
    
    def _calculate_measurement_noise(self, measurement: GPSMeasurement) -> float:
        """
        Calculate measurement noise based on measurement quality
        
        Lower noise = more trusted measurement
        """
        # Base noise level
        base_noise = 1e-6
        
        # Factors that increase noise (less reliable):
        # 1. Lower detection count
        detection_factor = 1.0 / max(1, measurement.detection_count)
        
        # 2. Lower confidence
        confidence_factor = 1.0 / max(0.1, measurement.confidence)
        
        # 3. Greater distance from estimated center (outlier detection)
        if len(self.measurements) > 0:
            # Distance from current filter estimate
            current_lat, current_lon = self.kf.x[0], self.kf.x[1]
            distance_to_estimate = self._gps_distance(
                current_lat, current_lon,
                measurement.latitude, measurement.longitude
            )
            # Penalize measurements far from current estimate
            distance_factor = 1.0 + distance_to_estimate * 1000  # Convert to penalty
        else:
            distance_factor = 1.0
        
        # Combined noise
        total_noise = base_noise * detection_factor * confidence_factor * distance_factor
        
        # Cap the noise (don't completely ignore any measurement)
        return min(total_noise, 1e-4)
    
    def get_estimated_location(self) -> Tuple[float, float, float]:
        """
        Get final estimated location with uncertainty
        
        Returns:
            (latitude, longitude, uncertainty_meters)
        """
        if len(self.measurements) == 0:
            return 0.0, 0.0, float('inf')
        
        lat_estimate = float(self.kf.x[0])
        lon_estimate = float(self.kf.x[1])
        
        # Calculate uncertainty in meters
        lat_variance = float(self.kf.P[0, 0])
        lon_variance = float(self.kf.P[1, 1])
        
        # Convert variance to standard deviation in meters
        lat_std_m = np.sqrt(lat_variance) * 111000  # Rough conversion
        lon_std_m = np.sqrt(lon_variance) * 111000 * np.cos(np.radians(lat_estimate))
        uncertainty_m = np.sqrt(lat_std_m**2 + lon_std_m**2)
        
        return lat_estimate, lon_estimate, uncertainty_m
    
    def _gps_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between GPS points in meters"""
        R = 6378137.0
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c


class KalmanGPSDeduplicator:
    """
    GPS deduplication using Kalman filters for optimal location estimation
    """
    
    def __init__(self, merge_distance_m: float = 3.0):
        """
        Initialize Kalman GPS deduplicator
        
        Args:
            merge_distance_m: Distance threshold for clustering LEDs
        """
        self.merge_distance_m = merge_distance_m
        self.logger = logging.getLogger(f"{__name__}.KalmanGPSDeduplicator")
    
    def deduplicate_locations(self, features: List[dict]) -> List[dict]:
        """
        Deduplicate GPS locations using Kalman filtering
        
        Args:
            features: List of GeoJSON features
            
        Returns:
            Deduplicated features with Kalman-estimated locations
        """
        if len(features) <= 1:
            return features
        
        self.logger.info(f"🔄 Starting Kalman GPS deduplication with {len(features)} locations")
        self.logger.info(f"   Merge threshold: {self.merge_distance_m} meters")
        
        # Step 1: Cluster nearby features
        clusters = self._cluster_features(features)
        
        # Step 2: Apply Kalman filtering to each cluster
        deduplicated_features = []
        for cluster in clusters:
            if len(cluster) == 1:
                # Single feature, no filtering needed
                deduplicated_features.append(cluster[0])
            else:
                # Multiple features, apply Kalman filtering
                filtered_feature = self._apply_kalman_filtering(cluster)
                deduplicated_features.append(filtered_feature)
        
        removed_count = len(features) - len(deduplicated_features)
        self.logger.info(f"✅ Kalman GPS deduplication complete:")
        self.logger.info(f"   Original locations: {len(features)}")
        self.logger.info(f"   Clusters found: {len(clusters)}")
        self.logger.info(f"   Final locations: {len(deduplicated_features)}")
        self.logger.info(f"   Duplicates removed: {removed_count}")
        
        return deduplicated_features
    
    def _cluster_features(self, features: List[dict]) -> List[List[dict]]:
        """
        Cluster features by proximity using single-linkage clustering
        """
        clusters = []
        used_indices = set()
        
        for i, feature in enumerate(features):
            if i in used_indices:
                continue
            
            # Start new cluster
            cluster = [feature]
            cluster_indices = {i}
            
            lat1 = feature['geometry']['coordinates'][1]
            lon1 = feature['geometry']['coordinates'][0]
            
            # Find all features within merge distance
            for j, other_feature in enumerate(features):
                if j == i or j in used_indices:
                    continue
                
                lat2 = other_feature['geometry']['coordinates'][1]
                lon2 = other_feature['geometry']['coordinates'][0]
                
                distance = self._gps_distance(lat1, lon1, lat2, lon2)
                
                if distance <= self.merge_distance_m:
                    cluster.append(other_feature)
                    cluster_indices.add(j)
                    
                    self.logger.debug(f"   Clustering tracks {feature['properties']['track_id']} "
                                    f"and {other_feature['properties']['track_id']} "
                                    f"(distance: {distance:.1f}m)")
            
            used_indices.update(cluster_indices)
            clusters.append(cluster)
        
        return clusters
    
    def _apply_kalman_filtering(self, cluster: List[dict]) -> dict:
        """
        Apply Kalman filtering to a cluster of features
        """
        # Calculate initial estimate (centroid)
        initial_lat = np.mean([f['geometry']['coordinates'][1] for f in cluster])
        initial_lon = np.mean([f['geometry']['coordinates'][0] for f in cluster])
        
        # Create Kalman filter
        kalman_filter = LEDLocationKalmanFilter(initial_lat, initial_lon)
        
        # Add all measurements to filter
        for feature in cluster:
            measurement = GPSMeasurement(
                latitude=feature['geometry']['coordinates'][1],
                longitude=feature['geometry']['coordinates'][0],
                detection_count=feature['properties']['detection_count'],
                confidence=feature['properties']['confidence'],
                distance_m=feature['properties']['estimated_distance_m'],
                track_id=feature['properties']['track_id']
            )
            kalman_filter.add_measurement(measurement)
        
        # Get final filtered estimate
        final_lat, final_lon, uncertainty_m = kalman_filter.get_estimated_location()
        
        # Create merged feature
        total_detections = sum(f['properties']['detection_count'] for f in cluster)
        avg_confidence = np.mean([f['properties']['confidence'] for f in cluster])
        avg_distance = np.mean([f['properties']['estimated_distance_m'] for f in cluster])
        
        # Use track with most detections as primary
        primary_track = max(cluster, key=lambda f: f['properties']['detection_count'])
        merged_track_ids = [f['properties']['track_id'] for f in cluster]
        
        merged_feature = {
            "type": "Feature",
            "geometry": {
                "type": "Point",
                "coordinates": [float(final_lon), float(final_lat)]
            },
            "properties": {
                "track_id": primary_track['properties']['track_id'],
                "merged_tracks": merged_track_ids,
                "confidence": round(float(avg_confidence), 3),
                "estimated_distance_m": round(float(avg_distance), 1),
                "detection_count": int(total_detections),
                "class_id": primary_track['properties']['class_id'],
                "processing_method": "kalman_filtered_gps_deduplication",
                "merge_cluster_size": len(cluster)
            }
        }
        
        self.logger.debug(f"   Kalman filtered cluster of {len(cluster)} tracks → "
                         f"Final location: ({final_lat:.6f}, {final_lon:.6f})")
        
        return merged_feature
    
    def _gps_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between GPS points in meters"""
        R = 6378137.0
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c


# Convenience function
def create_kalman_gps_deduplicator(merge_distance_m: float = 3.0) -> KalmanGPSDeduplicator:
    """
    Create a Kalman GPS deduplicator
    
    Args:
        merge_distance_m: Distance threshold for merging duplicates
        
    Returns:
        Configured Kalman GPS deduplicator
    """
    return KalmanGPSDeduplicator(merge_distance_m)

================
File: argus_track/utils/motion_compensation.py
================
# argus_track/utils/motion_compensation.py

import cv2
import numpy as np
from typing import List, Tuple, Optional
import logging
from ..core import Detection, Track, GPSData

class MotionCompensationTracker:
    """
    Handles camera motion compensation to improve track continuity
    """
    
    def __init__(self, 
                 feature_detector: str = 'ORB',
                 max_features: int = 500,
                 match_threshold: float = 0.7):
        """
        Initialize motion compensation tracker
        
        Args:
            feature_detector: Type of feature detector ('ORB', 'SIFT', 'AKAZE')
            max_features: Maximum number of features to track
            match_threshold: Threshold for feature matching
        """
        self.logger = logging.getLogger(f"{__name__}.MotionCompensationTracker")
        
        # Initialize feature detector
        if feature_detector == 'ORB':
            self.detector = cv2.ORB_create(nfeatures=max_features)
        elif feature_detector == 'SIFT':
            self.detector = cv2.SIFT_create(nfeatures=max_features)
        elif feature_detector == 'AKAZE':
            self.detector = cv2.AKAZE_create()
        else:
            raise ValueError(f"Unsupported detector: {feature_detector}")
        
        self.matcher = cv2.BFMatcher()
        self.match_threshold = match_threshold
        
        # Store previous frame data
        self.prev_frame = None
        self.prev_keypoints = None
        self.prev_descriptors = None
        self.camera_motion_history = []
        
        self.logger.info(f"Initialized motion compensation with {feature_detector}")
    
    def estimate_camera_motion(self, current_frame: np.ndarray) -> np.ndarray:
        """
        Estimate camera motion between frames using feature matching
        
        Args:
            current_frame: Current frame
            
        Returns:
            Homography matrix representing camera motion
        """
        if self.prev_frame is None:
            self._update_reference_frame(current_frame)
            return np.eye(3)  # Identity matrix for first frame
        
        # Detect features in current frame
        keypoints, descriptors = self.detector.detectAndCompute(current_frame, None)
        
        if descriptors is None or self.prev_descriptors is None:
            self.logger.warning("No features detected for motion estimation")
            return np.eye(3)
        
        # Match features between frames
        matches = self.matcher.knnMatch(self.prev_descriptors, descriptors, k=2)
        
        # Apply Lowe's ratio test to filter good matches
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < self.match_threshold * n.distance:
                    good_matches.append(m)
        
        if len(good_matches) < 10:
            self.logger.warning(f"Insufficient matches for motion estimation: {len(good_matches)}")
            return np.eye(3)
        
        # Extract matched points
        prev_pts = np.float32([self.prev_keypoints[m.queryIdx].pt for m in good_matches])
        curr_pts = np.float32([keypoints[m.trainIdx].pt for m in good_matches])
        
        # Estimate homography
        try:
            homography, mask = cv2.findHomography(
                prev_pts, curr_pts,
                cv2.RANSAC,
                ransacReprojThreshold=5.0,
                confidence=0.99
            )
            
            if homography is not None:
                # Store motion for smoothing
                self.camera_motion_history.append(homography)
                if len(self.camera_motion_history) > 5:
                    self.camera_motion_history = self.camera_motion_history[-5:]
                
                # Update reference frame
                self._update_reference_frame(current_frame, keypoints, descriptors)
                
                return homography
            else:
                self.logger.warning("Failed to compute homography")
                return np.eye(3)
                
        except Exception as e:
            self.logger.error(f"Error in motion estimation: {e}")
            return np.eye(3)
    
    def compensate_track_positions(self, 
                                  tracks: List[Track], 
                                  homography: np.ndarray) -> List[Track]:
        """
        Compensate track positions for camera motion
        
        Args:
            tracks: List of tracks to compensate
            homography: Camera motion homography
            
        Returns:
            List of tracks with compensated positions
        """
        if np.allclose(homography, np.eye(3)):
            return tracks  # No motion compensation needed
        
        compensated_tracks = []
        
        for track in tracks:
            # Get current predicted position
            current_bbox = track.to_tlbr()
            
            # Convert bbox to center point
            center_x = (current_bbox[0] + current_bbox[2]) / 2
            center_y = (current_bbox[1] + current_bbox[3]) / 2
            
            # Apply inverse homography to compensate for camera motion
            try:
                homography_inv = np.linalg.inv(homography)
                
                # Transform center point
                point = np.array([[center_x, center_y]], dtype=np.float32)
                point = point.reshape(-1, 1, 2)
                
                compensated_point = cv2.perspectiveTransform(point, homography_inv)
                compensated_center = compensated_point.reshape(-1, 2)[0]
                
                # Update track's Kalman filter with compensated position
                if track.kalman_filter:
                    # Calculate offset
                    offset_x = compensated_center[0] - center_x
                    offset_y = compensated_center[1] - center_y
                    
                    # Apply offset to Kalman state
                    track.kalman_filter.kf.x[0] += offset_x  # x position
                    track.kalman_filter.kf.x[1] += offset_y  # y position
                
                compensated_tracks.append(track)
                
            except Exception as e:
                self.logger.error(f"Error compensating track {track.track_id}: {e}")
                compensated_tracks.append(track)  # Use original track
        
        return compensated_tracks
    
    def predict_detection_positions(self, 
                                   detections: List[Detection],
                                   homography: np.ndarray) -> List[Detection]:
        """
        Predict where detections should be based on camera motion
        
        Args:
            detections: Current detections
            homography: Camera motion homography
            
        Returns:
            Detections with motion-compensated positions
        """
        if np.allclose(homography, np.eye(3)):
            return detections
        
        compensated_detections = []
        
        for detection in detections:
            try:
                # Get detection center
                center = detection.center
                
                # Transform center using homography
                point = np.array([[center[0], center[1]]], dtype=np.float32)
                point = point.reshape(-1, 1, 2)
                
                transformed_point = cv2.perspectiveTransform(point, homography)
                new_center = transformed_point.reshape(-1, 2)[0]
                
                # Calculate offset and apply to bbox
                offset_x = new_center[0] - center[0]
                offset_y = new_center[1] - center[1]
                
                new_bbox = detection.bbox.copy()
                new_bbox[0] += offset_x  # x1
                new_bbox[1] += offset_y  # y1
                new_bbox[2] += offset_x  # x2
                new_bbox[3] += offset_y  # y2
                
                # Create new detection with adjusted position
                compensated_detection = Detection(
                    bbox=new_bbox,
                    score=detection.score,
                    class_id=detection.class_id,
                    frame_id=detection.frame_id
                )
                
                compensated_detections.append(compensated_detection)
                
            except Exception as e:
                self.logger.error(f"Error compensating detection: {e}")
                compensated_detections.append(detection)  # Use original
        
        return compensated_detections
    
    def estimate_motion_from_gps(self, 
                                current_gps: GPSData,
                                previous_gps: GPSData,
                                camera_focal_length: float = 1400) -> np.ndarray:
        """
        Estimate camera motion from GPS data
        
        Args:
            current_gps: Current GPS position
            previous_gps: Previous GPS position
            camera_focal_length: Camera focal length in pixels
            
        Returns:
            Estimated motion transformation matrix
        """
        try:
            # Calculate GPS displacement
            lat_diff = current_gps.latitude - previous_gps.latitude
            lon_diff = current_gps.longitude - previous_gps.longitude
            heading_diff = current_gps.heading - previous_gps.heading
            
            # Convert to meters (approximate)
            R = 6378137.0  # Earth radius
            lat_offset = lat_diff * R * np.pi / 180
            lon_offset = lon_diff * R * np.pi / 180 * np.cos(np.radians(current_gps.latitude))
            
            # Convert to pixels (rough approximation)
            # This needs calibration for your specific camera setup
            pixels_per_meter = camera_focal_length / 10.0  # Adjust based on your setup
            
            pixel_offset_x = lon_offset * pixels_per_meter
            pixel_offset_y = -lat_offset * pixels_per_meter  # Y is inverted in image coordinates
            
            # Create translation matrix
            translation_matrix = np.array([
                [1, 0, pixel_offset_x],
                [0, 1, pixel_offset_y],
                [0, 0, 1]
            ], dtype=np.float32)
            
            # Add rotation for heading change if significant
            if abs(heading_diff) > 1.0:  # degrees
                heading_rad = np.radians(heading_diff)
                cos_h = np.cos(heading_rad)
                sin_h = np.sin(heading_rad)
                
                rotation_matrix = np.array([
                    [cos_h, -sin_h, 0],
                    [sin_h, cos_h, 0],
                    [0, 0, 1]
                ], dtype=np.float32)
                
                # Combine rotation and translation
                motion_matrix = rotation_matrix @ translation_matrix
            else:
                motion_matrix = translation_matrix
            
            return motion_matrix
            
        except Exception as e:
            self.logger.error(f"Error estimating motion from GPS: {e}")
            return np.eye(3)
    
    def _update_reference_frame(self, 
                               frame: np.ndarray,
                               keypoints: Optional[List] = None,
                               descriptors: Optional[np.ndarray] = None):
        """Update reference frame for motion tracking"""
        self.prev_frame = frame.copy()
        
        if keypoints is not None and descriptors is not None:
            self.prev_keypoints = keypoints
            self.prev_descriptors = descriptors
        else:
            # Detect features if not provided
            self.prev_keypoints, self.prev_descriptors = self.detector.detectAndCompute(frame, None)
    
    def get_motion_statistics(self) -> dict:
        """Get motion tracking statistics"""
        if not self.camera_motion_history:
            return {"motion_detected": False}
        
        # Analyze recent motion
        recent_motions = self.camera_motion_history[-3:]
        translations = []
        rotations = []
        
        for H in recent_motions:
            # Extract translation
            tx, ty = H[0, 2], H[1, 2]
            translation_magnitude = np.sqrt(tx**2 + ty**2)
            translations.append(translation_magnitude)
            
            # Estimate rotation (simplified)
            rotation_angle = np.arctan2(H[1, 0], H[0, 0])
            rotations.append(abs(rotation_angle))
        
        return {
            "motion_detected": True,
            "avg_translation": np.mean(translations),
            "max_translation": np.max(translations),
            "avg_rotation": np.mean(rotations),
            "motion_frames": len(self.camera_motion_history)
        }

================
File: argus_track/utils/overlap_fixer.py
================
# Create new file: argus_track/utils/overlap_fixer.py

"""
Simple Overlap Fixer for Ultralytics Tracking Issues
"""

import numpy as np
import logging
from typing import List, Dict, Optional, Tuple

class OverlapFixer:
    """
    Fixes Ultralytics tracking issues:
    1. Removes overlapping bounding boxes in same frame
    2. Consolidates track IDs based on GPS proximity
    """
    
    def __init__(self, overlap_threshold: float = 0.5, distance_threshold: float = 3.0):
        """
        Initialize overlap fixer
        
        Args:
            overlap_threshold: IoU threshold for detecting overlaps (0.5 = 50% overlap)
            distance_threshold: GPS distance threshold for same object (meters)
        """
        self.overlap_threshold = overlap_threshold
        self.distance_threshold = distance_threshold
        self.logger = logging.getLogger(f"{__name__}.OverlapFixer")
        
        # Track ID consolidation
        self.id_mapping = {}  # original_id -> consolidated_id
        self.next_consolidated_id = 1
        self.track_positions = {}  # track_id -> recent GPS positions
        self.fixed_count = 0
        self.overlap_count = 0
        
    def fix_ultralytics_results(self, ultralytics_result, current_gps: Optional['GPSData'], 
                               frame_id: int) -> List[Dict]:
        """
        Fix Ultralytics tracking results
        
        Args:
            ultralytics_result: Single result from model.track()[0]
            current_gps: Current GPS data
            frame_id: Current frame number
            
        Returns:
            List of fixed detection dictionaries
        """
        # Extract raw detections
        raw_detections = self._extract_detections(ultralytics_result, frame_id)
        
        if not raw_detections:
            return []
        
        # Step 1: Remove overlapping bounding boxes
        non_overlapping = self._remove_overlapping_boxes(raw_detections, frame_id)
        
        # Step 2: Consolidate track IDs
        consolidated = self._consolidate_track_ids(non_overlapping, current_gps)
        
        return consolidated
    
    def _extract_detections(self, result, frame_id: int) -> List[Dict]:
        """Extract detections from Ultralytics result"""
        detections = []
        
        if not result.boxes or not hasattr(result.boxes, 'id') or result.boxes.id is None:
            return detections
        
        boxes = result.boxes.xyxy.cpu().numpy()
        scores = result.boxes.conf.cpu().numpy()
        classes = result.boxes.cls.cpu().numpy().astype(int)
        track_ids = result.boxes.id.cpu().numpy().astype(int)
        
        for box, score, cls_id, track_id in zip(boxes, scores, classes, track_ids):
            detections.append({
                'bbox': box,
                'score': float(score),
                'class_id': int(cls_id),
                'track_id': int(track_id),
                'frame': frame_id
            })
        
        return detections
    
    def _remove_overlapping_boxes(self, detections: List[Dict], frame_id: int) -> List[Dict]:
        """Remove overlapping bounding boxes in same frame"""
        if len(detections) <= 1:
            return detections
        
        # Calculate IoU for all pairs
        keep_indices = set(range(len(detections)))
        overlaps_removed = 0
        
        for i in range(len(detections)):
            if i not in keep_indices:
                continue
                
            for j in range(i + 1, len(detections)):
                if j not in keep_indices:
                    continue
                
                # Calculate IoU
                iou = self._calculate_iou(detections[i]['bbox'], detections[j]['bbox'])
                
                if iou > self.overlap_threshold:
                    # Keep the detection with higher confidence
                    if detections[i]['score'] >= detections[j]['score']:
                        keep_indices.discard(j)
                        overlaps_removed += 1
                        self.logger.debug(f"Frame {frame_id}: Removed overlapping track {detections[j]['track_id']} "
                                        f"(IoU {iou:.2f} with track {detections[i]['track_id']})")
                    else:
                        keep_indices.discard(i)
                        overlaps_removed += 1
                        self.logger.debug(f"Frame {frame_id}: Removed overlapping track {detections[i]['track_id']} "
                                        f"(IoU {iou:.2f} with track {detections[j]['track_id']})")
                        break
        
        if overlaps_removed > 0:
            self.overlap_count += overlaps_removed
            self.logger.info(f"Frame {frame_id}: Removed {overlaps_removed} overlapping boxes")
        
        return [detections[i] for i in sorted(keep_indices)]
    
    def _consolidate_track_ids(self, detections: List[Dict], current_gps: Optional['GPSData']) -> List[Dict]:
        """Consolidate track IDs to prevent multiple IDs for same object"""
        
        for detection in detections:
            original_id = detection['track_id']
            
            # Get consolidated ID
            consolidated_id = self._get_consolidated_id(detection, current_gps)
            
            # Update detection
            detection['original_track_id'] = original_id
            detection['track_id'] = consolidated_id
            
            if original_id != consolidated_id:
                self.fixed_count += 1
                self.logger.info(f"Frame {detection['frame']}: Consolidated track {original_id} → {consolidated_id}")
        
        return detections
    
    def _get_consolidated_id(self, detection: Dict, current_gps: Optional['GPSData']) -> int:
        """Get consolidated track ID for detection"""
        original_id = detection['track_id']
        
        # If we've seen this original ID before, return its mapping
        if original_id in self.id_mapping:
            return self.id_mapping[original_id]
        
        # Check if this detection is close to any existing tracks (if we have GPS)
        if current_gps:
            detection_gps = self._estimate_detection_gps(detection, current_gps)
            
            if detection_gps:
                # Find existing tracks within distance threshold
                for existing_id, positions in self.track_positions.items():
                    if not positions:
                        continue
                    
                    # Check distance to most recent position
                    recent_pos = positions[-1]
                    distance = self._gps_distance(
                        detection_gps[0], detection_gps[1],
                        recent_pos['lat'], recent_pos['lon']
                    )
                    
                    if distance <= self.distance_threshold:
                        # Merge with existing track
                        self.id_mapping[original_id] = existing_id
                        
                        # Add position to existing track
                        self.track_positions[existing_id].append({
                            'lat': detection_gps[0],
                            'lon': detection_gps[1],
                            'frame': detection['frame']
                        })
                        
                        self.logger.info(f"Merged track {original_id} into {existing_id} (distance: {distance:.1f}m)")
                        return existing_id
        
        # This is a genuinely new track
        new_consolidated_id = self.next_consolidated_id
        self.id_mapping[original_id] = new_consolidated_id
        self.next_consolidated_id += 1
        
        # Initialize position tracking
        if current_gps:
            detection_gps = self._estimate_detection_gps(detection, current_gps)
            if detection_gps:
                self.track_positions[new_consolidated_id] = [{
                    'lat': detection_gps[0],
                    'lon': detection_gps[1],
                    'frame': detection['frame']
                }]
        
        return new_consolidated_id
    
    def _estimate_detection_gps(self, detection: Dict, gps: 'GPSData') -> Optional[Tuple[float, float]]:
        """Estimate GPS coordinates for detection (simplified version)"""
        try:
            bbox = detection['bbox']
            bbox_height = bbox[3] - bbox[1]
            
            if bbox_height <= 0:
                return None
            
            # Simple depth estimation
            focal_length = 1400
            lightpost_height = 4.0
            estimated_depth = (lightpost_height * focal_length) / bbox_height
            
            # GPS calculation (simplified)
            bbox_center_x = (bbox[0] + bbox[2]) / 2
            image_width = 1920  # Assume standard width
            
            pixels_from_center = bbox_center_x - (image_width / 2)
            degrees_per_pixel = 60.0 / image_width
            bearing_offset = pixels_from_center * degrees_per_pixel
            object_bearing = gps.heading + bearing_offset
            
            import math
            lat_offset = (estimated_depth * math.cos(math.radians(object_bearing))) / 111000
            lon_offset = (estimated_depth * math.sin(math.radians(object_bearing))) / (111000 * math.cos(math.radians(gps.latitude)))
            
            object_lat = gps.latitude + lat_offset
            object_lon = gps.longitude + lon_offset
            
            return (object_lat, object_lon)
            
        except Exception:
            return None
    
    def _calculate_iou(self, box1: np.ndarray, box2: np.ndarray) -> float:
        """Calculate IoU between two bounding boxes"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def _gps_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between GPS points in meters"""
        R = 6378137.0
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c
    
    def get_statistics(self) -> Dict:
        """Get overlap fixing statistics"""
        return {
            'overlaps_removed': self.overlap_count,
            'ids_consolidated': self.fixed_count,
            'unique_tracks': len(set(self.id_mapping.values())),
            'original_tracks': len(self.id_mapping)
        }

================
File: argus_track/utils/static_car_detector.py
================
# argus_track/utils/static_car_detector.py - NEW FILE

"""
Static Car Detector - Skip frames when GPS position doesn't change
"""

import numpy as np
import logging
from typing import List, Optional
from dataclasses import dataclass

from ..core import GPSData

@dataclass
class StaticCarConfig:
    """Configuration for static car detection"""
    movement_threshold_meters: float = 2.0      # Minimum movement to consider as "moving"
    stationary_time_threshold: float = 10.0     # Seconds before considering stationary
    gps_frame_interval: int = 6                 # Normal GPS frame processing interval


class StaticCarDetector:
    """
    Detects when car is stationary and skips frames for processing efficiency
    
    Logic:
    1. Car stops moving (< 2m movement for 10+ seconds) → Skip all frames
    2. Car starts moving again → Resume normal frame processing
    3. Purpose: Speed up processing, we already captured objects when we first stopped
    """
    
    def __init__(self, config: StaticCarConfig):
        """
        Initialize static car detector
        
        Args:
            config: Static car detection configuration
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.StaticCarDetector")
        
        # State tracking
        self.gps_history: List[GPSData] = []
        self.last_movement_time: float = 0.0
        self.is_currently_stationary: bool = False
        self.total_frames_processed: int = 0
        self.total_frames_skipped: int = 0
        
        # Statistics
        self.stationary_periods: List[float] = []  # Duration of each stationary period
        self.current_stationary_start: Optional[float] = None
        
        self.logger.info(f"Static Car Detector initialized:")
        self.logger.info(f"  Movement threshold: {config.movement_threshold_meters}m")
        self.logger.info(f"  Stationary threshold: {config.stationary_time_threshold}s")
    
    def should_process_frame(self, gps_data: GPSData, frame_id: int) -> bool:
        """
        Determine if frame should be processed based on car movement
        
        Args:
            gps_data: Current GPS data
            frame_id: Current frame number
            
        Returns:
            True if frame should be processed, False if it should be skipped
        """
        # Always process first frame
        if len(self.gps_history) == 0:
            self._add_gps_data(gps_data)
            self.total_frames_processed += 1
            self.logger.debug(f"Frame {frame_id}: First frame - processing")
            return True
        
        # Check if car has moved significantly
        has_moved = self._has_moved_enough(gps_data)
        current_time = gps_data.timestamp
        
        if has_moved:
            # Car is moving
            self._handle_movement_detected(current_time, frame_id)
            self._add_gps_data(gps_data)
            self.total_frames_processed += 1
            return True
        else:
            # Car hasn't moved much - check if we should skip
            if self._should_skip_stationary_frame(current_time, frame_id):
                self.total_frames_skipped += 1
                return False
            else:
                # Still in grace period - keep processing
                self._add_gps_data(gps_data)
                self.total_frames_processed += 1
                return True
    
    def _has_moved_enough(self, current_gps: GPSData) -> bool:
        """Check if car has moved beyond the threshold"""
        if not self.gps_history:
            return True
        
        # Calculate distance from most recent position
        last_gps = self.gps_history[-1]
        distance = self._calculate_distance(last_gps, current_gps)
        
        moved_enough = distance >= self.config.movement_threshold_meters
        
        if moved_enough:
            self.logger.debug(f"Movement detected: {distance:.1f}m")
        
        return moved_enough
    
    def _should_skip_stationary_frame(self, current_time: float, frame_id: int) -> bool:
        """Determine if we should skip this frame due to stationary car"""
        if self.last_movement_time == 0:
            self.last_movement_time = current_time
            return False
        
        time_stationary = current_time - self.last_movement_time
        
        if time_stationary >= self.config.stationary_time_threshold:
            # Car has been stationary long enough - start skipping frames
            if not self.is_currently_stationary:
                self.logger.info(f"Frame {frame_id}: Car stationary for {time_stationary:.1f}s - "
                               f"starting to skip frames for efficiency")
                self.is_currently_stationary = True
                self.current_stationary_start = current_time
            
            return True  # Skip this frame
        
        return False  # Still in grace period
    
    def _handle_movement_detected(self, current_time: float, frame_id: int):
        """Handle when movement is detected after being stationary"""
        if self.is_currently_stationary:
            # End of stationary period
            if self.current_stationary_start:
                stationary_duration = current_time - self.current_stationary_start
                self.stationary_periods.append(stationary_duration)
                
                self.logger.info(f"Frame {frame_id}: Movement resumed after "
                               f"{stationary_duration:.1f}s stationary period")
            
            self.is_currently_stationary = False
            self.current_stationary_start = None
        
        # Update last movement time
        self.last_movement_time = current_time
    
    def _calculate_distance(self, gps1: GPSData, gps2: GPSData) -> float:
        """
        Calculate distance between two GPS points using Haversine formula
        
        Returns:
            Distance in meters
        """
        # Earth's radius in meters
        R = 6378137.0
        
        # Convert to radians
        lat1_rad = np.radians(gps1.latitude)
        lat2_rad = np.radians(gps2.latitude)
        dlat = np.radians(gps2.latitude - gps1.latitude)
        dlon = np.radians(gps2.longitude - gps1.longitude)
        
        # Haversine formula
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c
    
    def _add_gps_data(self, gps_data: GPSData):
        """Add GPS data to history"""
        self.gps_history.append(gps_data)
        
        # Keep history manageable (last 50 points)
        if len(self.gps_history) > 50:
            self.gps_history = self.gps_history[-50:]
    
    def get_statistics(self) -> dict:
        """Get static car detection statistics"""
        total_frames = self.total_frames_processed + self.total_frames_skipped
        
        return {
            'total_frames': total_frames,
            'processed_frames': self.total_frames_processed,
            'skipped_frames': self.total_frames_skipped,
            'skip_ratio': self.total_frames_skipped / total_frames if total_frames > 0 else 0,
            'stationary_periods_count': len(self.stationary_periods),
            'total_stationary_time': sum(self.stationary_periods),
            'avg_stationary_duration': np.mean(self.stationary_periods) if self.stationary_periods else 0,
            'max_stationary_duration': max(self.stationary_periods) if self.stationary_periods else 0,
            'currently_stationary': self.is_currently_stationary,
            'efficiency_gain': f"{self.total_frames_skipped / total_frames * 100:.1f}%" if total_frames > 0 else "0%"
        }
    
    def reset(self):
        """Reset detector state"""
        self.gps_history.clear()
        self.last_movement_time = 0.0
        self.is_currently_stationary = False
        self.total_frames_processed = 0
        self.total_frames_skipped = 0
        self.stationary_periods.clear()
        self.current_stationary_start = None


def create_static_car_detector(movement_threshold_m: float = 2.0,
                              stationary_time_s: float = 10.0,
                              gps_frame_interval: int = 6) -> StaticCarDetector:
    """
    Create a static car detector with specified parameters
    
    Args:
        movement_threshold_m: Minimum movement in meters to consider as moving
        stationary_time_s: Time in seconds before starting to skip frames
        gps_frame_interval: Normal GPS frame processing interval
        
    Returns:
        Configured static car detector
    """
    config = StaticCarConfig(
        movement_threshold_meters=movement_threshold_m,
        stationary_time_threshold=stationary_time_s,
        gps_frame_interval=gps_frame_interval
    )
    
    return StaticCarDetector(config)

================
File: argus_track/__version__.py
================
"""Version information for ByteTrack Light Post Tracking System"""

__version__ = "1.0.0"
__author__ = "Light Post Tracking Team"
__email__ = "joaquin.olivera@gmial.com"
__description__ = "ByteTrack implementation optimized for light post tracking with GPS integration"

================
File: argus_track/bytetrack_custom.yaml
================
# bytetrack_custom.yaml - Custom ByteTrack configuration for LED tracking

# Tracker type
tracker_type: bytetrack

# CORE PARAMETERS (SOLVES YOUR ISSUES)
track_high_thresh: 0.6      # High confidence detections
track_low_thresh: 0.1       # Low confidence detections (for continuity)
new_track_thresh: 0.7       # Threshold for creating new tracks (PREVENTS MULTIPLE IDs)
track_buffer: 60           # Keep lost tracks for 120 frames (PREVENTS ID REUSE)
match_thresh: 0.8           # High IoU threshold for matching (STRICTER MATCHING)

# Additional parameters
frame_rate: 60              # Video frame rate

================
File: argus_track/requirements.txt
================
# argus_track/requirements.txt (UPDATED WITH GPS EXTRACTION)

# Core dependencies
numpy>=1.19.0
scipy>=1.5.0
opencv-python>=4.5.0
filterpy>=1.4.5
numba>=0.53.0  # For JIT compilation

# PyTorch for YOLOv11 support
torch>=1.9.0
torchvision>=0.10.0
torchaudio>=0.9.0

# YOLOv11 specific
ultralytics>=8.0.0  # For YOLOv11 support

# Optional optimizations
lap>=0.4.0  # Faster Hungarian algorithm

# Visualization
matplotlib>=3.3.0
seaborn>=0.11.0

# Development dependencies
pytest>=6.0.0
pytest-benchmark>=3.4.0
black>=21.0
flake8>=3.9.0
mypy>=0.910

# Documentation
sphinx>=4.0.0
sphinx-rtd-theme>=0.5.0

# GPS support
pyproj>=3.0.0  # For GPS coordinate transformations
scikit-learn>=0.24.0  # For clustering in static analysis
pynvml>=11.0.0  # Optional: For GPU monitoring

# GPS visualization
folium>=0.12.0  # For interactive maps
geojson>=2.5.0  # For GeoJSON export

# Stereo vision and 3D processing
transforms3d>=0.3.1  # For 3D transformations
open3d>=0.13.0  # Optional: For 3D visualization

# Configuration and data handling
pyyaml>=5.4.0  # For YAML configuration files
pandas>=1.3.0  # For data analysis and CSV handling

# Image processing enhancements
Pillow>=8.0.0  # Enhanced image processing
scikit-image>=0.18.0  # Additional image processing algorithms

# GPS EXTRACTION DEPENDENCIES
# ============================

# HTML/XML parsing for GPS metadata
beautifulsoup4>=4.9.0  # For parsing GPS metadata from ExifTool output
lxml>=4.6.0  # XML parser backend for BeautifulSoup

# GPS metadata handling
GPSPhoto>=2.2.0  # For reading/writing GPS metadata in images (optional)

# GoPro telemetry extraction (optional but recommended)
gopro-overlay>=0.10.0  # For GoPro telemetry data extraction

# ExifTool integration (ExifTool must be installed separately)
# Note: ExifTool (https://exiftool.org/) must be installed on the system
# Windows: Download from https://exiftool.org/
# macOS: brew install exiftool
# Linux: sudo apt-get install libimage-exiftool-perl

# Video processing enhancements
ffmpeg-python>=0.2.0  # For video metadata extraction (alternative method)

# Time and date handling
python-dateutil>=2.8.0  # Enhanced date parsing

# Process management
psutil>=5.8.0  # For system monitoring during processing

================
File: images/bytetrack-workflow-diagram.svg
================
<svg viewBox="0 0 800 600" xmlns="http://www.w3.org/2000/svg">
  <!-- Title -->
  <text x="400" y="30" font-size="20" font-weight="bold" text-anchor="middle">ByteTrack Workflow for Light Post Tracking</text>
  
  <!-- Input Frame -->
  <rect x="20" y="60" width="120" height="80" fill="#e3f2fd" stroke="#1976d2" stroke-width="2"/>
  <text x="80" y="100" text-anchor="middle" font-size="14">Input Frame</text>
  <text x="80" y="120" text-anchor="middle" font-size="12">(with detections)</text>
  
  <!-- Detection Preprocessing -->
  <rect x="180" y="60" width="140" height="80" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2"/>
  <text x="250" y="90" text-anchor="middle" font-size="14">Split Detections</text>
  <text x="250" y="110" text-anchor="middle" font-size="12">High Score (>0.5)</text>
  <text x="250" y="130" text-anchor="middle" font-size="12">Low Score (≤0.5)</text>
  
  <!-- First Association -->
  <rect x="360" y="40" width="140" height="60" fill="#e8f5e9" stroke="#388e3c" stroke-width="2"/>
  <text x="430" y="70" text-anchor="middle" font-size="14">First Association</text>
  <text x="430" y="90" text-anchor="middle" font-size="12">(High Score + Tracks)</text>
  
  <!-- Second Association -->
  <rect x="360" y="120" width="140" height="60" fill="#fff3e0" stroke="#f57c00" stroke-width="2"/>
  <text x="430" y="150" text-anchor="middle" font-size="14">Second Association</text>
  <text x="430" y="170" text-anchor="middle" font-size="12">(Low Score + Unmatched)</text>
  
  <!-- Track Management -->
  <rect x="540" y="60" width="140" height="80" fill="#fce4ec" stroke="#c2185b" stroke-width="2"/>
  <text x="610" y="90" text-anchor="middle" font-size="14">Track Management</text>
  <text x="610" y="110" text-anchor="middle" font-size="12">• Update matched</text>
  <text x="610" y="130" text-anchor="middle" font-size="12">• Create new tracks</text>
  
  <!-- Arrows -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" 
     refX="0" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
    </marker>
  </defs>
  
  <line x1="140" y1="100" x2="180" y2="100" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="320" y1="100" x2="360" y2="80" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="320" y1="100" x2="360" y2="140" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="500" y1="70" x2="540" y2="90" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="500" y1="150" x2="540" y2="110" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Kalman Filter Box -->
  <rect x="180" y="200" width="440" height="100" fill="#f5f5f5" stroke="#666" stroke-width="2" stroke-dasharray="5,5"/>
  <text x="400" y="230" text-anchor="middle" font-size="16" font-weight="bold">Kalman Filter (per track)</text>
  
  <!-- Kalman States -->
  <rect x="200" y="250" width="100" height="40" fill="#e1f5fe" stroke="#0288d1" stroke-width="1"/>
  <text x="250" y="270" text-anchor="middle" font-size="12">State</text>
  <text x="250" y="285" text-anchor="middle" font-size="10">[x,y,w,h,vx,vy,vw,vh]</text>
  
  <rect x="320" y="250" width="100" height="40" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="1"/>
  <text x="370" y="270" text-anchor="middle" font-size="12">Predict</text>
  <text x="370" y="285" text-anchor="middle" font-size="10">Next position</text>
  
  <rect x="440" y="250" width="100" height="40" fill="#e8f5e9" stroke="#388e3c" stroke-width="1"/>
  <text x="490" y="270" text-anchor="middle" font-size="12">Update</text>
  <text x="490" y="285" text-anchor="middle" font-size="10">With detection</text>
  
  <!-- Track States -->
  <text x="400" y="340" text-anchor="middle" font-size="16" font-weight="bold">Track States</text>
  
  <rect x="100" y="360" width="120" height="50" fill="#e8f5e9" stroke="#388e3c" stroke-width="2"/>
  <text x="160" y="385" text-anchor="middle" font-size="14">Tentative</text>
  <text x="160" y="400" text-anchor="middle" font-size="12">(< 3 frames)</text>
  
  <rect x="260" y="360" width="120" height="50" fill="#fff9c4" stroke="#f9a825" stroke-width="2"/>
  <text x="320" y="385" text-anchor="middle" font-size="14">Confirmed</text>
  <text x="320" y="400" text-anchor="middle" font-size="12">(≥ 3 frames)</text>
  
  <rect x="420" y="360" width="120" height="50" fill="#ffebee" stroke="#c62828" stroke-width="2"/>
  <text x="480" y="385" text-anchor="middle" font-size="14">Lost</text>
  <text x="480" y="400" text-anchor="middle" font-size="12">(no match)</text>
  
  <rect x="580" y="360" width="120" height="50" fill="#e0e0e0" stroke="#616161" stroke-width="2"/>
  <text x="640" y="385" text-anchor="middle" font-size="14">Removed</text>
  <text x="640" y="400" text-anchor="middle" font-size="12">(> 30 frames lost)</text>
  
  <!-- State Transitions -->
  <line x1="220" y1="385" x2="260" y2="385" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="380" y1="385" x2="420" y2="385" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="540" y1="385" x2="580" y2="385" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Light Post Specific Features -->
  <rect x="50" y="450" width="700" height="120" fill="#f8f9fa" stroke="#333" stroke-width="2"/>
  <text x="400" y="480" text-anchor="middle" font-size="16" font-weight="bold">Light Post Tracking Optimizations</text>
  
  <rect x="70" y="500" width="150" height="50" fill="#e3f2fd" stroke="#1565c0" stroke-width="1"/>
  <text x="145" y="520" text-anchor="middle" font-size="12" font-weight="bold">Static Assumption</text>
  <text x="145" y="535" text-anchor="middle" font-size="11">Low velocity noise</text>
  <text x="145" y="548" text-anchor="middle" font-size="11">in Kalman filter</text>
  
  <rect x="250" y="500" width="150" height="50" fill="#f3e5f5" stroke="#6a1b9a" stroke-width="1"/>
  <text x="325" y="520" text-anchor="middle" font-size="12" font-weight="bold">GPS Integration</text>
  <text x="325" y="535" text-anchor="middle" font-size="11">Track 3D positions</text>
  <text x="325" y="548" text-anchor="middle" font-size="11">for triangulation</text>
  
  <rect x="430" y="500" width="150" height="50" fill="#e8f5e9" stroke="#2e7d32" stroke-width="1"/>
  <text x="505" y="520" text-anchor="middle" font-size="12" font-weight="bold">Appearance Buffer</text>
  <text x="505" y="535" text-anchor="middle" font-size="11">Store visual features</text>
  <text x="505" y="548" text-anchor="middle" font-size="11">for re-identification</text>
  
  <rect x="610" y="500" width="120" height="50" fill="#fff3e0" stroke="#e65100" stroke-width="1"/>
  <text x="670" y="520" text-anchor="middle" font-size="12" font-weight="bold">ID Persistence</text>
  <text x="670" y="535" text-anchor="middle" font-size="11">Long buffer for</text>
  <text x="670" y="548" text-anchor="middle" font-size="11">occluded objects</text>
</svg>

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
repomix-output.txt
repomix-output.xml

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Bell South

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: repomix.config.json
================
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.txt",
    "style": "plain",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}

================
File: run_argus.sh
================
#!/bin/bash
# Run with LOWER detection threshold to actually see detections

VIDEO_FILE="${1:-../fellowship_of_the_frame/data/Videos/Camino_8/FI/GX018691.MP4}"

echo "🔧 Running Argus Track with FIXED DETECTION THRESHOLD"
echo "===================================================="
echo "📄 Video file: $VIDEO_FILE"
echo "🔧 Using LOWER threshold to see actual detections"
echo "⏰ Started at: $(date)"
echo ""

echo "🐛 ISSUE IDENTIFIED:"
echo "   ❌ Detection threshold (0.3) was too high"
echo "   ❌ All detections were filtered out (max confidence: 0.14)"
echo "   ❌ Visualization bug with None frame"
echo ""

echo "✅ FIXES APPLIED:"
echo "   🔧 Lower detection threshold: 0.1 (was 0.3)"
echo "   🔧 Fixed visualization None frame bug"
echo "   🔧 Better error handling in real-time display"
echo ""

echo "🎯 Expected Results:"
echo "   ✅ Should now see LED detections (confidence 0.14, 0.06, etc.)"
echo "   ✅ Real-time window should work properly"
echo "   ✅ Track formation should begin"
echo ""

# Run with LOWER threshold to actually detect objects
argus_track "$VIDEO_FILE" \
    --calibration argus_calibration.pkl \
    --detector yolov11 \
    --model ../best_2.pt \
    --track-thresh 0.1 \
    --match-thresh 0.3 \
    --track-buffer 150 \
    --gps-interval 30 \
# --show-realtime \
# --display-size 1280 720 \
    --output tracked_leds_test.mp4 \
    --verbose

EXIT_CODE=$?

echo ""
echo "📊 PROCESSING RESULT"
echo "==================="
echo "Exit Code: $EXIT_CODE"

if [ $EXIT_CODE -eq 0 ]; then
    echo "✅ SUCCESS!"
    
    # Check results
    geojson_file="${VIDEO_FILE%.*}.geojson"
    if [ -f "$geojson_file" ]; then
        geolocated_count=$(jq '.features | length' "$geojson_file" 2>/dev/null || echo "0")
        echo "📍 Geolocated objects: $geolocated_count"
        
        if [ "$geolocated_count" != "0" ] && [ "$geolocated_count" != "null" ]; then
            echo "🎉 SUCCESS: Objects detected and geolocated!"
            
            # Show sample detection
            echo ""
            echo "📋 Sample Detection:"
            jq -r '.features[0] | "   Track \(.properties.track_id) (\(.properties.class_name))\n   Location: \(.geometry.coordinates[1]), \(.geometry.coordinates[0])\n   Confidence: \(.properties.confidence), Reliability: \(.properties.reliability)"' "$geojson_file" 2>/dev/null || echo "   (Unable to parse)"
        else
            echo "⚠️  No geolocated objects - but detections should now be visible in real-time"
        fi
    fi
    
elif [ $EXIT_CODE -eq 1 ]; then
    echo "❌ Still failed - checking for specific issues..."
    
    echo ""
    echo "🔍 DEBUGGING STEPS:"
    echo "1. Try without real-time display:"
    echo "   argus_track $VIDEO_FILE --detector yolov11 --model ../best.pt --track-thresh 0.1 --no-realtime --verbose"
    echo ""
    echo "2. Test just detection without tracking:"
    echo "   python3 -c \"
from ultralytics import YOLO
import cv2
model = YOLO('../best.pt')
cap = cv2.VideoCapture('$VIDEO_FILE')
ret, frame = cap.read()
if ret:
    results = model.predict(frame, conf=0.05, verbose=True)
    print(f'Detections: {len(results[0].boxes) if results[0].boxes else 0}')
cap.release()
\""
    echo ""
    echo "3. Check display availability:"
    echo "   echo \$DISPLAY"
    
else
    echo "❌ Failed with exit code $EXIT_CODE"
fi

echo ""
echo "💡 THRESHOLD EXPLANATION:"
echo "   Your model detects objects with confidence 0.06-0.14"
echo "   Previous threshold 0.3 filtered out ALL detections"
echo "   New threshold 0.1 should capture the strongest detections"
echo "   You can adjust further: --track-thresh 0.05 for even more detections"

================
File: tracker_config.yaml
================
#Custom ByteTrack configuration for LED tracking

# Tracker type
tracker_type: bytetrack

# Track lifecycle management
track_thresh: 0.25          # High confidence threshold for track creation
track_buffer: 120           # Keep lost tracks for 120 frames (longer memory)
match_thresh: 0.8           # High IoU threshold for matching (stricter)
frame_rate: 60              # Video frame rate

# ID management (CRITICAL FOR YOUR ISSUES)
track_high_thresh: 0.6      # High confidence detections
track_low_thresh: 0.1       # Low confidence detections (for continuity)
new_track_thresh: 0.7       # Threshold for creating new tracks (higher = less new IDs)

# Track confirmation
min_box_area: 10            # Minimum bounding box area
track_activation: 3         # Frames needed to confirm a track (reduces premature IDs)

# Advanced ByteTrack parameters
# These control the two-stage association that prevents ID reuse
alpha: 0.9                  # Smoothing parameter for track prediction
beta: 0.1                   # Association threshold for second stage matching

# Track removal (PREVENTS ID REUSE)
remove_thresh: 200          # Frames before permanently removing a track ID
max_time_lost: 120          # Maximum frames a track can be lost before removal

# Detection filtering
min_confidence: 0.25        # Minimum detection confidence
nms_iou: 0.3               # Non-maximum suppression IoU (ADDRESSES MULTIPLE IDs)
max_detections: 100         # Maximum detections per frame

================
File: argus_track/core/track.py
================
"""Track data structure"""

from dataclasses import dataclass, field
from typing import List, Optional
import numpy as np

from .detection import Detection

@dataclass
class Track:
    """Represents a tracked object through multiple frames"""
    track_id: int
    detections: List[Detection] = field(default_factory=list)
    kalman_filter: Optional['KalmanBoxTracker'] = None
    state: str = 'tentative'           # tentative, confirmed, lost, removed
    hits: int = 0                      # Number of successful updates
    age: int = 0                       # Total frames since creation
    time_since_update: int = 0         # Frames since last update
    start_frame: int = 0
    
    @property
    def is_confirmed(self) -> bool:
        """Check if track is confirmed (has enough hits)"""
        return self.state == 'confirmed'
    
    @property
    def is_active(self) -> bool:
        """Check if track is currently active"""
        return self.state in ['tentative', 'confirmed']
    
    def to_tlbr(self) -> np.ndarray:
        """Get current position in tlbr format"""
        if self.kalman_filter is None:
            return self.detections[-1].tlbr if self.detections else np.zeros(4)
        return self.kalman_filter.get_state()
    
    @property
    def last_detection(self) -> Optional[Detection]:
        """Get the most recent detection"""
        return self.detections[-1] if self.detections else None
    
    @property
    def trajectory(self) -> List[np.ndarray]:
        """Get trajectory as list of center points"""
        return [det.center for det in self.detections]
    
    def to_dict(self) -> dict:
        """Convert to dictionary representation"""
        return {
            'track_id': self.track_id,
            'state': self.state,
            'hits': self.hits,
            'age': self.age,
            'time_since_update': self.time_since_update,
            'start_frame': self.start_frame,
            'detections': [det.to_dict() for det in self.detections[-10:]]  # Last 10 detections
        }

================
File: argus_track/utils/__init__.py
================
"""Utility functions for ByteTrack system"""

from .static_car_detector import StaticCarDetector, create_static_car_detector
from .iou import calculate_iou, calculate_iou_matrix
from .visualization import draw_tracks, create_track_overlay
from .io import save_tracking_results, load_gps_data, setup_logging
from .gps_utils import GPSInterpolator, CoordinateTransformer
from .overlap_fixer import OverlapFixer
from .output_manager import OutputManager, FrameData
from .smart_track_manager import SmartTrackManager, TrackMemory
from .visual_feature_extractor import VisualFeatures, VisualFeatureExtractor
from .motion_prediction import MotionPredictor, PredictedPosition, CameraMotion, EnhancedTrackMatcher

__all__ = [
    "calculate_iou",
    "calculate_iou_matrix",
    "draw_tracks",
    "create_track_overlay",
    "save_tracking_results",
    "load_gps_data",
    "setup_logging",
    "GPSInterpolator",
    "StaticCarDetector",
    "OutputManager",
    "FrameData",
    "SmartTrackManager",
    "TrackMemory",
    "create_static_car_detector",
    "OverlapFixer",
    "CoordinateTransformer",
    "VisualFeatures", 
    "VisualFeatureExtractor",
    "MotionPredictor", 
    "PredictedPosition", 
    "CameraMotion",
    "EnhancedTrackMatcher"
]

================
File: argus_track/utils/gps_extraction.py
================
# argus_track/utils/gps_extraction.py (NEW FILE)

"""
GPS Data Extraction from GoPro Videos
=====================================
Integrated GPS extraction functionality for Argus Track stereo processing.
Supports both ExifTool and GoPro API methods for extracting GPS metadata.
"""

import os
import sys
import time
import logging
import subprocess
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Union
from datetime import datetime, timedelta
import numpy as np
from bs4 import BeautifulSoup
import tempfile
import shutil
from dataclasses import dataclass

from ..core import GPSData

# Configure logging
logger = logging.getLogger(__name__)

# Try to import GoPro API if available
try:
    from gopro_overlay.goprotelemetry import telemetry
    GOPRO_API_AVAILABLE = True
except ImportError:
    GOPRO_API_AVAILABLE = False
    logger.debug("GoPro telemetry API not available")

# Check for ExifTool availability
def check_exiftool_available() -> bool:
    """Check if ExifTool is available in the system"""
    try:
        result = subprocess.run(['exiftool', '-ver'], 
                               capture_output=True, text=True, timeout=10)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False

EXIFTOOL_AVAILABLE = check_exiftool_available()


@dataclass
class GPSExtractionResult:
    """Result of GPS extraction operation"""
    success: bool
    gps_data: List[GPSData]
    method_used: str
    total_points: int
    time_range: Optional[Tuple[float, float]] = None
    error_message: Optional[str] = None


class GoProGPSExtractor:
    """Extract GPS data from GoPro videos using multiple methods"""
    
    def __init__(self, fps_video: float = 60.0, fps_gps: float = 10.0):
        """
        Initialize GPS extractor
        
        Args:
            fps_video: Video frame rate (default: 60 fps)
            fps_gps: GPS data rate (default: 10 Hz)
        """
        self.fps_video = fps_video
        self.fps_gps = fps_gps
        self.frame_time_ms = 1000.0 / fps_video
        
        # Check available extraction methods
        self.methods_available = []
        if EXIFTOOL_AVAILABLE:
            self.methods_available.append('exiftool')
            logger.debug("ExifTool method available")
        if GOPRO_API_AVAILABLE:
            self.methods_available.append('gopro_api')
            logger.debug("GoPro API method available")
        
        if not self.methods_available:
            logger.warning("No GPS extraction methods available!")
    
    def extract_gps_data(self, video_path: str, 
                        method: str = 'auto') -> GPSExtractionResult:
        """
        Extract GPS data from GoPro video
        
        Args:
            video_path: Path to GoPro video file
            method: Extraction method ('auto', 'exiftool', 'gopro_api')
            
        Returns:
            GPSExtractionResult: Extraction results
        """
        if not os.path.exists(video_path):
            return GPSExtractionResult(
                success=False,
                gps_data=[],
                method_used='none',
                total_points=0,
                error_message=f"Video file not found: {video_path}"
            )
        
        # Determine extraction method
        if method == 'auto':
            # Prefer GoPro API for better accuracy, fallback to ExifTool
            if 'gopro_api' in self.methods_available:
                method = 'gopro_api'
            elif 'exiftool' in self.methods_available:
                method = 'exiftool'
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='none',
                    total_points=0,
                    error_message="No GPS extraction methods available"
                )
        
        logger.info(f"Extracting GPS data from {video_path} using {method} method")
        
        try:
            if method == 'exiftool':
                return self._extract_with_exiftool(video_path)
            elif method == 'gopro_api':
                return self._extract_with_gopro_api(video_path)
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used=method,
                    total_points=0,
                    error_message=f"Unknown extraction method: {method}"
                )
        except Exception as e:
            logger.error(f"Error extracting GPS data: {e}")
            return GPSExtractionResult(
                success=False,
                gps_data=[],
                method_used=method,
                total_points=0,
                error_message=str(e)
            )
    
    def _extract_with_exiftool(self, video_path: str) -> GPSExtractionResult:
        """Extract GPS data using ExifTool method"""
        temp_dir = tempfile.mkdtemp()
        
        try:
            metadata_file = os.path.join(temp_dir, 'metadata.xml')
            gps_file = os.path.join(temp_dir, 'gps_data.txt')
            
            # Extract metadata using ExifTool
            cmd = [
                'exiftool',
                '-api', 'largefilesupport=1',
                '-ee',  # Extract embedded data
                '-G3',  # Show group names
                '-X',   # XML format
                video_path
            ]
            
            logger.debug(f"Running ExifTool command: {' '.join(cmd)}")
            
            with open(metadata_file, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, 
                                      text=True, timeout=300)
            
            if result.returncode != 0:
                raise RuntimeError(f"ExifTool failed: {result.stderr}")
            
            # Extract Track4 GPS data
            self._extract_track4_data(metadata_file, gps_file)
            
            # Parse GPS data
            gps_data = self._parse_gps_file(gps_file)
            
            if gps_data:
                time_range = (gps_data[0].timestamp, gps_data[-1].timestamp)
                return GPSExtractionResult(
                    success=True,
                    gps_data=gps_data,
                    method_used='exiftool',
                    total_points=len(gps_data),
                    time_range=time_range
                )
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='exiftool',
                    total_points=0,
                    error_message="No GPS data found in metadata"
                )
                
        finally:
            # Cleanup temporary files
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _extract_with_gopro_api(self, video_path: str) -> GPSExtractionResult:
        """Extract GPS data using GoPro API method"""
        try:
            # Extract telemetry data
            telem = telemetry.Telemetry(video_path)
            
            if not telem.has_gps():
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='gopro_api',
                    total_points=0,
                    error_message="No GPS data found in video"
                )
            
            # Get GPS track
            gps_track = telem.gps_track()
            gps_data = []
            
            for point in gps_track:
                if point.lat != 0.0 and point.lon != 0.0:
                    # Convert timestamp to seconds
                    timestamp = point.timestamp.total_seconds() if hasattr(point.timestamp, 'total_seconds') else point.timestamp
                    
                    gps_point = GPSData(
                        timestamp=float(timestamp),
                        latitude=float(point.lat),
                        longitude=float(point.lon),
                        altitude=float(getattr(point, 'alt', 0.0)),
                        heading=float(getattr(point, 'heading', 0.0)),
                        accuracy=float(getattr(point, 'dop', 1.0))
                    )
                    gps_data.append(gps_point)
            
            if gps_data:
                time_range = (gps_data[0].timestamp, gps_data[-1].timestamp)
                return GPSExtractionResult(
                    success=True,
                    gps_data=gps_data,
                    method_used='gopro_api',
                    total_points=len(gps_data),
                    time_range=time_range
                )
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='gopro_api',
                    total_points=0,
                    error_message="No valid GPS points found"
                )
                
        except Exception as e:
            raise RuntimeError(f"GoPro API extraction failed: {e}")
    
    def _extract_track4_data(self, metadata_file: str, output_file: str) -> None:
        """Extract Track4 GPS data from metadata XML file"""
        try:
            with open(metadata_file, 'r', encoding='utf-8') as in_file, \
                 open(output_file, 'w', encoding='utf-8') as out_file:
                
                for line in in_file:
                    # Look for Track4 GPS data
                    if 'Track4' in line or 'GPS' in line:
                        out_file.write(line)
                        
            logger.debug(f"Extracted Track4 data to {output_file}")
            
        except Exception as e:
            logger.error(f"Error extracting Track4 data: {e}")
            raise
    
    def _parse_gps_file(self, gps_file: str) -> List[GPSData]:
        """Parse GPS data from extracted Track4 file"""
        gps_data = []
        
        try:
            with open(gps_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Skip first two lines if they exist
            lines = content.split('\n')[2:] if len(content.split('\n')) > 2 else content.split('\n')
            
            current_timestamp = None
            current_lat = None
            current_lon = None
            
            for line in lines:
                if not line.strip():
                    continue
                    
                # Parse XML-like content
                soup = BeautifulSoup(line, "html.parser")
                text_content = soup.get_text()
                
                # Look for GPS tags
                if ':GPSLatitude>' in line:
                    current_lat = self._convert_gps_coordinate(text_content)
                elif ':GPSLongitude>' in line and current_lat is not None:
                    current_lon = self._convert_gps_coordinate(text_content)
                elif ':GPSDateTime>' in line:
                    current_timestamp = self._convert_timestamp(text_content)
                    
                    # If we have complete GPS data, save it
                    if (current_timestamp is not None and 
                        current_lat is not None and 
                        current_lon is not None and
                        current_lat != 0.0 and current_lon != 0.0):
                        
                        gps_point = GPSData(
                            timestamp=current_timestamp,
                            latitude=current_lat,
                            longitude=current_lon,
                            altitude=0.0,
                            heading=0.0,
                            accuracy=1.0
                        )
                        gps_data.append(gps_point)
                        
                        # Reset for next point
                        current_lat = None
                        current_lon = None
            
            logger.info(f"Parsed {len(gps_data)} GPS points from file")
            return gps_data
            
        except Exception as e:
            logger.error(f"Error parsing GPS file: {e}")
            return []
    
    def _convert_gps_coordinate(self, coord_str: str) -> float:
        """Convert GPS coordinate from DMS format to decimal degrees"""
        if not coord_str or not isinstance(coord_str, str):
            return 0.0
            
        try:
            # Clean the string
            coord_str = coord_str.strip()
            
            # Handle the format: "34 deg 39' 45.72" S"
            import re
            # Pattern for: "34 deg 39' 45.72" S"
            pattern = r"(\d+)\s+deg\s+(\d+)'\s+([\d.]+)\"\s*([NSEW])"
            match = re.search(pattern, coord_str)
            
            if match:
                degrees = float(match.group(1))
                minutes = float(match.group(2))
                seconds = float(match.group(3))
                direction = match.group(4)
                
                # Convert to decimal degrees
                decimal = degrees + minutes/60.0 + seconds/3600.0
                
                # Apply sign based on direction
                if direction in ['S', 'W']:
                    decimal = -decimal
                    
                return decimal
            
            if coord_str.startswith('<'):
                coord_str = coord_str[1:]
            if coord_str.endswith('>'):
                coord_str = coord_str[:-1]
                
            # Parse DMS format: "deg min' sec" N/S/E/W"
            parts = coord_str.split(' ')
            if len(parts) < 6:
                logger.warning(f"Invalid GPS coordinate format: {coord_str}")
                return 0.0
            
            degrees = float(parts[1])
            minutes = float(parts[3].replace("'", ""))
            seconds = float(parts[4].replace('"', ""))
            direction = parts[5][0] if len(parts[5]) > 0 else 'N'
            
            # Convert to decimal degrees
            decimal = degrees + minutes/60.0 + seconds/3600.0
            
            # Apply sign based on direction
            if direction in ['S', 'W']:
                decimal = -decimal
                
            return decimal
            
        except (ValueError, IndexError) as e:
            logger.warning(f"Error converting GPS coordinate '{coord_str}': {e}")
            return 0.0
    
    def _convert_timestamp(self, timestamp_str: str) -> float:
        """Convert timestamp string to Unix timestamp"""
        if not timestamp_str:
            return 0.0
            
        try:
            # Clean timestamp string
            timestamp_str = timestamp_str.strip()
            if timestamp_str.startswith('<'):
                timestamp_str = timestamp_str[1:]
            if timestamp_str.endswith('>'):
                timestamp_str = timestamp_str[:-1]
            
            # Parse timestamp formats
            try:
                # Try with microseconds
                dt = datetime.strptime(timestamp_str, '%Y:%m:%d %H:%M:%S.%f')
            except ValueError:
                # Try without microseconds
                dt = datetime.strptime(timestamp_str, '%Y:%m:%d %H:%M:%S')
            
            return dt.timestamp()
            
        except ValueError as e:
            logger.warning(f"Error converting timestamp '{timestamp_str}': {e}")
            return 0.0
    
    def synchronize_with_video(self, gps_data: List[GPSData], 
                              video_duration: float,
                              target_fps: float = 10.0) -> List[GPSData]:
        """
        Synchronize GPS data with video timeline
        
        Args:
            gps_data: Raw GPS data
            video_duration: Video duration in seconds
            target_fps: Target GPS sampling rate
            
        Returns:
            List[GPSData]: Synchronized GPS data
        """
        if not gps_data:
            return []
        
        # Sort GPS data by timestamp
        sorted_gps = sorted(gps_data, key=lambda x: x.timestamp)
        
        # Normalize timestamps to start from 0
        start_time = sorted_gps[0].timestamp
        for gps_point in sorted_gps:
            gps_point.timestamp -= start_time
        
        # Create synchronized timeline
        sync_interval = 1.0 / target_fps
        sync_timeline = np.arange(0, video_duration, sync_interval)
        
        # Interpolate GPS data to match timeline
        timestamps = np.array([gps.timestamp for gps in sorted_gps])
        latitudes = np.array([gps.latitude for gps in sorted_gps])
        longitudes = np.array([gps.longitude for gps in sorted_gps])
        
        # Interpolate
        sync_gps = []
        for sync_time in sync_timeline:
            if sync_time <= timestamps[-1]:
                # Find closest GPS points for interpolation
                idx = np.searchsorted(timestamps, sync_time)
                
                if idx == 0:
                    # Use first point
                    lat = latitudes[0]
                    lon = longitudes[0]
                elif idx >= len(timestamps):
                    # Use last point
                    lat = latitudes[-1]
                    lon = longitudes[-1]
                else:
                    # Linear interpolation
                    t1, t2 = timestamps[idx-1], timestamps[idx]
                    lat1, lat2 = latitudes[idx-1], latitudes[idx]
                    lon1, lon2 = longitudes[idx-1], longitudes[idx]
                    
                    alpha = (sync_time - t1) / (t2 - t1)
                    lat = lat1 + alpha * (lat2 - lat1)
                    lon = lon1 + alpha * (lon2 - lon1)
                
                sync_point = GPSData(
                    timestamp=sync_time,
                    latitude=lat,
                    longitude=lon,
                    altitude=0.0,
                    heading=0.0,
                    accuracy=1.0
                )
                sync_gps.append(sync_point)
        
        logger.info(f"Synchronized {len(sync_gps)} GPS points for {video_duration:.1f}s video")
        return sync_gps


def extract_gps_from_stereo_videos(left_video: str, 
                                  right_video: str,
                                  method: str = 'auto') -> Tuple[List[GPSData], str]:
    """
    Extract GPS data from stereo video pair
    
    Args:
        left_video: Path to left camera video
        right_video: Path to right camera video  
        method: Extraction method ('auto', 'exiftool', 'gopro_api')
        
    Returns:
        Tuple[List[GPSData], str]: GPS data and method used
    """
    extractor = GoProGPSExtractor()
    
    # Try extracting from left video first
    logger.info("Attempting GPS extraction from left video")
    result_left = extractor.extract_gps_data(left_video, method)
    
    if result_left.success and result_left.total_points > 0:
        logger.info(f"Successfully extracted {result_left.total_points} GPS points from left video")
        return result_left.gps_data, result_left.method_used
    
    # Fallback to right video
    logger.info("Left video GPS extraction failed, trying right video")
    result_right = extractor.extract_gps_data(right_video, method)
    
    if result_right.success and result_right.total_points > 0:
        logger.info(f"Successfully extracted {result_right.total_points} GPS points from right video")
        return result_right.gps_data, result_right.method_used
    
    # No GPS data found
    logger.warning("No GPS data found in either video")
    return [], 'none'


def save_gps_to_csv(gps_data: List[GPSData], output_path: str) -> None:
    """
    Save GPS data to CSV file for Argus Track
    
    Args:
        gps_data: GPS data to save
        output_path: Path to output CSV file
    """
    import csv
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['timestamp', 'latitude', 'longitude', 'altitude', 'heading', 'accuracy'])
        
        for gps in gps_data:
            writer.writerow([
                gps.timestamp,
                gps.latitude,
                gps.longitude,
                gps.altitude,
                gps.heading,
                gps.accuracy
            ])
    
    logger.info(f"Saved {len(gps_data)} GPS points to {output_path}")

================
File: argus_track/utils/performance.py
================
"""
Performance monitoring utilities for ArgusTrack
"""

import time
from typing import Dict, List, Optional
from dataclasses import dataclass, field

@dataclass
class PerformanceMetrics:
    """Container for performance metrics"""
    fps: float = 0.0
    frame_time: float = 0.0
    detection_time: float = 0.0
    tracking_time: float = 0.0
    total_time: float = 0.0
    frame_count: int = 0
    
    def reset(self):
        """Reset all metrics to zero"""
        self.fps = 0.0
        self.frame_time = 0.0
        self.detection_time = 0.0
        self.tracking_time = 0.0
        self.total_time = 0.0
        self.frame_count = 0

class PerformanceMonitor:
    """Monitor and track performance metrics"""
    
    def __init__(self):
        self.metrics = PerformanceMetrics()
        self.start_time: Optional[float] = None
        self.frame_times: List[float] = []
        self.detection_times: List[float] = []
        self.tracking_times: List[float] = []
        
    def start_frame(self):
        """Start timing a frame"""
        self.start_time = time.time()
        
    def end_frame(self):
        """End timing a frame and update metrics"""
        if self.start_time is not None:
            frame_time = time.time() - self.start_time
            self.frame_times.append(frame_time)
            self.metrics.frame_count += 1
            self.start_time = None
            
    def record_detection_time(self, detection_time: float):
        """Record detection time for current frame"""
        self.detection_times.append(detection_time)
        
    def record_tracking_time(self, tracking_time: float):
        """Record tracking time for current frame"""
        self.tracking_times.append(tracking_time)
        
    def update_metrics(self):
        """Update average metrics"""
        if self.frame_times:
            self.metrics.frame_time = sum(self.frame_times) / len(self.frame_times)
            self.metrics.fps = 1.0 / self.metrics.frame_time if self.metrics.frame_time > 0 else 0.0
            
        if self.detection_times:
            self.metrics.detection_time = sum(self.detection_times) / len(self.detection_times)
            
        if self.tracking_times:
            self.metrics.tracking_time = sum(self.tracking_times) / len(self.tracking_times)
            
        self.metrics.total_time = self.metrics.detection_time + self.metrics.tracking_time
        
    def get_metrics(self) -> PerformanceMetrics:
        """Get current performance metrics"""
        self.update_metrics()
        return self.metrics
        
    def reset(self):
        """Reset all timing data"""
        self.metrics.reset()
        self.frame_times.clear()
        self.detection_times.clear()
        self.tracking_times.clear()
        self.start_time = None
        
    def print_stats(self):
        """Print performance statistics"""
        self.update_metrics()
        print(f"Performance Stats:")
        print(f"  FPS: {self.metrics.fps:.2f}")
        print(f"  Frame Time: {self.metrics.frame_time*1000:.2f}ms")
        print(f"  Detection Time: {self.metrics.detection_time*1000:.2f}ms")
        print(f"  Tracking Time: {self.metrics.tracking_time*1000:.2f}ms")
        print(f"  Total Frames: {self.metrics.frame_count}")

================
File: README.md
================
# Argus Track: Enhanced Stereo Tracking with Automatic GPS Extraction

A specialized implementation of ByteTrack optimized for tracking light posts and static infrastructure in **stereo video sequences** with **automatic GPS extraction from GoPro videos**. Features **3D triangulation**, **integrated GPS processing**, and **1-2 meter geolocation accuracy** for mapping and asset management.

## 🎯 Key Features

- **🔍 Stereo Vision Processing**: 3D triangulation from stereo camera pairs for accurate depth estimation
- **🛰️ Automatic GPS Extraction**: Extract GPS data directly from GoPro video metadata (no separate GPS file needed!)
- **📍 Precise Geolocation**: 1-2 meter accuracy GPS coordinate estimation for tracked objects  
- **🚦 Infrastructure Focus**: Optimized for light posts, traffic signals, and static infrastructure
- **🧠 YOLOv11 Support**: Advanced object detection with latest YOLO architecture
- **📡 GPS Synchronization**: Smart GPS frame processing (60fps video → 10fps GPS alignment)
- **🎥 GoPro Optimized**: Designed for GoPro Hero 11 stereo camera setups with embedded GPS
- **📊 Multiple Export Formats**: JSON, GeoJSON, and CSV outputs for GIS integration

## 🚀 Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/Bell-South/ArgusTrack.git
cd ArgusTrack

# Install dependencies (including GPS extraction tools)
pip install -r argus_track/requirements.txt

# Install ExifTool (required for GPS extraction)
# Windows: Download from https://exiftool.org/
# macOS: brew install exiftool  
# Linux: sudo apt-get install libimage-exiftool-perl

# Install package
pip install -e .
```

### 🎬 Complete Example (With Your Files)

```bash
# Enhanced stereo tracking with automatic GPS extraction
argus_track --stereo left_camera.mp4 right_camera.mp4 \
    --calibration stereo_calibration.pkl \
    --detector yolov11 \
    --model your_finetuned_model.pt \
    --auto-gps \
    --output tracked_result.mp4
```

**That's it!** No need to extract GPS separately - it's automatic! 🎉

### 📁 Required Files

```
your_project/
├── left_camera.mp4              # Left camera video (with GPS metadata)
├── right_camera.mp4             # Right camera video  
├── stereo_calibration.pkl       # Your calibration file
└── your_finetuned_model.pt     # Your fine-tuned YOLOv11 model
```

## 🛰️ GPS Extraction Methods

The system automatically tries multiple methods to extract GPS data from your videos:

### Method 1: ExifTool (Recommended)
- ✅ Works with most GoPro videos
- ✅ High accuracy GPS extraction
- ✅ Extracts full GPS tracks from metadata

### Method 2: GoPro API
- ✅ Official GoPro telemetry extraction
- ✅ Best accuracy when available
- ⚠️ Requires `gopro-overlay` package

### Method 3: Auto Detection
- 🔄 Tries ExifTool first, falls back to GoPro API
- 🔄 Automatically handles different video formats

## 📐 Usage Examples

### 1. Complete Automatic Processing

```bash
# Everything automatic - GPS extraction, tracking, geolocation
argus_track --stereo left.mp4 right.mp4 \
    --calibration calibration.pkl \
    --detector yolov11 \
    --model model.pt \
    --auto-gps
```

### 2. Extract GPS Only (No Tracking)

```bash
# Just extract GPS data to CSV
argus_track --extract-gps-only left.mp4 right.mp4 \
    --output gps_data.csv \
    --gps-method exiftool
```

### 3. Use Existing GPS File

```bash
# Use pre-extracted GPS file
argus_track --stereo left.mp4 right.mp4 \
    --calibration calibration.pkl \
    --gps existing_gps.csv \
    --detector yolov11 \
    --model model.pt
```

### 4. Python API Usage

```python
from argus_track import (
    TrackerConfig, StereoCalibrationConfig, 
    YOLOv11Detector
)
from argus_track.trackers.stereo_lightpost_tracker import EnhancedStereoLightPostTracker

# Load calibration
stereo_calibration = StereoCalibrationConfig.from_pickle('calibration.pkl')

# Initialize detector with your fine-tuned model
detector = YOLOv11Detector(
    model_path='your_model.pt',
    target_classes=['light_post', 'traffic_signal', 'pole'],
    device='auto'
)

# Configure tracker
config = TrackerConfig(
    track_thresh=0.4,
    stereo_mode=True,
    gps_frame_interval=6
)

# Initialize enhanced tracker
tracker = EnhancedStereoLightPostTracker(
    config=config,
    detector=detector,
    stereo_calibration=stereo_calibration
)

# Process with automatic GPS extraction
tracks = tracker.process_stereo_video_with_auto_gps(
    left_video_path='left.mp4',
    right_video_path='right.mp4',
    save_results=True
)

# Get results
stats = tracker.get_enhanced_tracking_statistics()
print(f"GPS extraction method: {stats['gps_extraction_method']}")
print(f"Average accuracy: {stats['accuracy_achieved']:.1f}m")
print(f"Locations found: {stats['estimated_locations']}")
```

## 📊 Output Files

After processing, you get:

### 1. **GPS Data (Automatic)**
- `left_camera.csv` - Extracted GPS data in CSV format
- 📡 Contains: timestamp, latitude, longitude, altitude, heading, accuracy

### 2. **Tracking Results**  
- `left_camera.json` - Complete tracking data with 3D trajectories
- 📹 Contains: tracks, stereo detections, depth info, processing stats

### 3. **Geolocation Map**
- `left_camera.geojson` - GPS locations ready for GIS software
- 🗺️ Contains: precise coordinates, accuracy, reliability scores

### 4. **Visualization Video**
- `tracked_result.mp4` - Side-by-side stereo tracking visualization
- 🎬 Shows: bounding boxes, track IDs, trajectories

## 🎯 Accuracy Results

The system provides detailed accuracy metrics:

```bash
=== TRACKING RESULTS ===
📹 Total stereo tracks: 12
🏗️  Static tracks: 8
📍 Estimated locations: 8
🛰️  GPS extraction method: exiftool
📡 GPS points used: 450
📏 Average depth: 25.4m
🎯 Average accuracy: 1.2m
✅ Average reliability: 0.94

🏆 TARGET ACHIEVED: Average accuracy ≤ 2 meters!
```

### Accuracy Interpretation:
- **🎯 < 2m**: Excellent accuracy (target achieved)
- **✅ 2-5m**: Good accuracy for most applications  
- **⚠️ > 5m**: Consider recalibration or GPS quality check

## 🔧 Configuration

### Stereo Configuration (`stereo_config.yaml`)

```yaml
# Tracking parameters
track_thresh: 0.4              # Lower for fine-tuned models
match_thresh: 0.8
stereo_mode: true
gps_frame_interval: 6          # 60fps -> 10fps GPS sync

# Your fine-tuned detector
detector:
  model_type: "yolov11"
  model_path: "your_model.pt"
  target_classes:              # YOUR CLASSES
    - "light_post"
    - "traffic_signal" 
    - "utility_pole"
    - "street_light"

# GPS extraction
gps_extraction:
  method: "auto"               # auto, exiftool, gopro_api
  accuracy_threshold: 5.0      # Ignore GPS > 5m accuracy
```

## 🛠️ Comparison with Your Original Code

Your original GPS extraction code has been **fully integrated** into Argus Track:

| Your Original Code | Argus Track Integration |
|-------------------|------------------------|
| ✅ ExifTool GPS extraction | ✅ **Enhanced** ExifTool method |
| ✅ Track4 GPS parsing | ✅ **Improved** metadata parsing |
| ✅ DMS coordinate conversion | ✅ **Robust** coordinate handling |
| ✅ Frame synchronization | ✅ **Advanced** stereo-GPS sync |
| ❌ No 3D tracking | ✅ **Added** stereo tracking |
| ❌ No geolocation | ✅ **Added** 1-2m accuracy |
| ❌ Manual process | ✅ **Automatic** end-to-end |

## 📋 Processing Pipeline

```
GoPro Videos (with GPS) → GPS Extraction → Stereo Processing → 3D Tracking → Geolocation
     ↓                         ↓                ↓               ↓            ↓
Left/Right MP4          GPS Metadata      Object Detection  ByteTrack     GPS Coords
60fps + 10Hz GPS    →   CSV Export    →   YOLOv11        →  3D Tracks  →  1-2m Accuracy
```

## 🚨 Troubleshooting

### GPS Extraction Issues

```bash
# Check if ExifTool is installed
exiftool -ver

# Test GPS extraction on single video
argus_track --extract-gps-only left.mp4 right.mp4 --verbose

# Check video metadata
exiftool -G -a -s left.mp4 | grep GPS
```

### Accuracy Issues

```python
# Check calibration quality
from argus_track.stereo import StereoCalibrationManager
calib = StereoCalibrationManager.from_pickle_file('calibration.pkl')
print("Calibration valid:", calib.validate_calibration()[0])
```

### Detection Issues

```python
# Test your model
from argus_track import YOLOv11Detector
detector = YOLOv11Detector('your_model.pt')
print("Model classes:", detector.get_class_names())
```

## 🌟 Advanced Features

### Real-time Processing

```python
# Process live stereo stream (conceptual)
def process_live_stereo():
    while True:
        left_frame, right_frame = get_stereo_frames()
        current_gps = get_current_gps()
        
        tracks = tracker.process_frame_pair(
            left_frame, right_frame, current_gps
        )
```

### Batch Processing

```bash
# Process multiple video pairs
for video_pair in /data/videos/*/; do
    argus_track --stereo "$video_pair"/{left,right}.mp4 \
        --calibration calibration.pkl \
        --model model.pt \
        --auto-gps
done
```

### GIS Integration

```python
# Load results in QGIS/ArcGIS
import geopandas as gpd
gdf = gpd.read_file('results.geojson')
print(f"Found {len(gdf)} light posts")
```

## 📞 Support

- **Documentation**: [Complete usage guide](docs/USAGE_GUIDE.md)
- **Issues**: [GitHub Issues](https://github.com/Bell-South/ArgusTrack/issues)
- **Examples**: [examples/](examples/) directory

## 🎯 Summary

Argus Track now provides a **complete solution** for your light post mapping needs:

1. **🎬 Input**: Your stereo GoPro videos (with embedded GPS)
2. **🔄 Process**: Automatic GPS extraction + stereo tracking + 3D triangulation  
3. **📍 Output**: 1-2 meter accurate GPS coordinates of light posts
4. **📊 Export**: Ready for GIS software and mapping applications

**No manual GPS extraction needed - everything is automatic!** 🚀

---

*Argus Track: From GoPro videos to precise infrastructure maps* 🎯📍

================
File: setup.py
================
"""
Setup script for ByteTrack Light Post Tracking System
"""

from setuptools import setup, find_packages
import os

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

# Try to read requirements from multiple locations
requirements = []
possible_req_files = [
    "requirements.txt",
    "argus_track/requirements.txt"
]

for req_file in possible_req_files:
    if os.path.exists(req_file):
        with open(req_file, "r", encoding="utf-8") as fh:
            requirements = [
                line.strip() 
                for line in fh 
                if line.strip() and not line.startswith("#")
            ]
        break

# If no requirements file found, use minimal requirements
if not requirements:
    requirements = [
        "numpy>=1.19.0",
        "scipy>=1.5.0",
        "opencv-python>=4.5.0",
        "matplotlib>=3.3.0",
        "pyyaml>=5.4.0",
        "pandas>=1.3.0",
        "Pillow>=8.0.0",
        "beautifulsoup4>=4.9.0",
        "lxml>=4.6.0",
        "python-dateutil>=2.8.0",
        "psutil>=5.8.0"
    ]

setup(
    name="argus-track",
    version="1.0.0",
    author="Light Post Tracking Team",
    author_email="joaquin.olivera@gmail.com",
    description="ByteTrack implementation optimized for light post tracking with GPS integration",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/Bell-South/ArgusTrack.git",
    packages=find_packages(exclude=["tests", "docs", "examples"]),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
    python_requires=">=3.8",
    install_requires=requirements,
    extras_require={
        "dev": [
            "pytest>=6.0.0",
            "black>=21.0",
            "mypy>=0.910",
        ],
        "docs": [
            "sphinx>=4.0.0",
            "sphinx-rtd-theme>=0.5.0",
        ],
        "gpu": [
            "torch>=1.9.0",
            "torchvision>=0.10.0",
        ],
    },
    entry_points={
        "console_scripts": [
            "argus_track=argus_track.main:main",
        ],
    },
    include_package_data=True,
    package_data={
        "argus_track": ["config/*.yaml", "config/*.json"],
    },
)

================
File: argus_track/detectors/__init__.py
================
"""Object detectors for ByteTrack system"""

from .base import ObjectDetector
from .yolov11 import YOLOv11Detector

__all__ = ["ObjectDetector", "YOLOv11Detector"]

================
File: argus_track/detectors/yolov11.py
================
# argus_track/detectors/yolov11.py (FIXED)
"""YOLOv11 detector implementation with proper class handling"""

import cv2
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path
import torch
import torchvision.transforms as transforms

from .base import ObjectDetector


class YOLOv11Detector(ObjectDetector):
    """YOLOv11-based object detector implementation with PyTorch backend"""
    
    def __init__(self, 
                 model_path: str,
                 target_classes: Optional[List[str]] = None,
                 confidence_threshold: float = 0.5,
                 nms_threshold: float = 0.4,
                 device: str = 'auto',
                 input_size: int = 640):
        """
        Initialize YOLOv11 detector
        
        Args:
            model_path: Path to YOLOv11 model file (.pt)
            target_classes: List of class names to detect (None for all)
            confidence_threshold: Minimum confidence for detections
            nms_threshold: Non-maximum suppression threshold
            device: Device to use ('cpu', 'cuda', or 'auto')
            input_size: Model input size (typically 640)
        """
        self.logger = logging.getLogger(f"{__name__}.YOLOv11Detector")
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.input_size = input_size
        
        # Set device
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        self.logger.info(f"Using device: {self.device}")
        
        # Load model
        self.model = self._load_model()
        
        # Get class names directly from the loaded model
        self.class_names = list(self.model.names.values())
        self.logger.info(f"Model classes: {dict(self.model.names)}")

        # Set target classes - FIXED: Now handles all classes properly
        if target_classes is None:
            self.target_classes = self.class_names.copy()
            self.logger.info(f"Using all model classes: {self.target_classes}")
        else:
            # Filter to only valid classes that exist in the model
            valid_classes = [cls for cls in target_classes if cls in self.class_names]
            if not valid_classes:
                self.logger.warning(f"None of the target classes {target_classes} found in model. Using all model classes.")
                self.target_classes = self.class_names.copy()
            else:
                self.target_classes = valid_classes
                self.logger.info(f"Using filtered target classes: {self.target_classes}")
        
        # Target class indices - FIXED: Now maps to actual class names
        self.target_class_indices = [
            i for i, name in enumerate(self.class_names) 
            if name in self.target_classes
        ]
        
        self.logger.info(f"Initialized YOLOv11 detector with {len(self.target_classes)} target classes")
        self.logger.info(f"Target class indices: {self.target_class_indices}")
    
    def _load_model(self):
        """Load YOLOv11 model"""
        try:
            # Try to load with ultralytics (if available)
            try:
                from ultralytics import YOLO
                model = YOLO(self.model_path)
                model.to(self.device)
                self.logger.info("Loaded YOLOv11 model using ultralytics")
                return model
            except ImportError:
                self.logger.warning("ultralytics not available, falling back to torch.hub")
                
            # Fallback to torch.hub or direct torch loading
            if self.model_path.endswith('.pt'):
                model = torch.jit.load(self.model_path, map_location=self.device)
                model.eval()
                self.logger.info("Loaded YOLOv11 model using torch.jit")
                return model
            else:
                raise ValueError(f"Unsupported model format: {self.model_path}")
                
        except Exception as e:
            self.logger.error(f"Failed to load YOLOv11 model: {e}")
            raise
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect objects in frame using YOLOv11
        
        Args:
            frame: Input image
            
        Returns:
            List of detections
        """
        try:
            # Check if using ultralytics YOLO
            if hasattr(self.model, 'predict'):
                return self._detect_ultralytics(frame)
            else:
                return self._detect_torch(frame)
        except Exception as e:
            self.logger.error(f"Detection failed: {e}")
            return []
    
    def _detect_ultralytics(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """Detection using ultralytics YOLO - FIXED to handle all classes"""
        
        # Run inference with low confidence to catch all possible detections
        results = self.model.predict(
            frame, 
            conf=0.001,  # Very low confidence to catch everything
            iou=self.nms_threshold,
            verbose=False
        )
        
        detections = []
        
        if results and len(results) > 0:
            result = results[0]
            
            if result.boxes is not None:
                boxes = result.boxes.xyxy.cpu().numpy()
                scores = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy().astype(int)
                
                self.logger.debug(f"Raw detections: {len(boxes)} boxes, classes: {set(classes)}")
                
                for i, (box, score, cls_id) in enumerate(zip(boxes, scores, classes)):
                    # FIXED: Check if class is in our target classes instead of hardcoded check
                    if cls_id < len(self.class_names):
                        class_name = self.class_names[cls_id]
                        
                        # Only keep detections of target classes with sufficient confidence
                        if (class_name in self.target_classes and 
                            score >= self.confidence_threshold):
                            
                            detections.append({
                                'bbox': box.tolist(),
                                'score': float(score),
                                'class_name': class_name,
                                'class_id': cls_id
                            })
                            
                            self.logger.debug(f"Kept detection: {class_name} (ID:{cls_id}), Conf: {score:.4f}")
                        else:
                            self.logger.debug(f"Filtered out: {class_name} (ID:{cls_id}), Conf: {score:.4f}")
                    else:
                        self.logger.warning(f"Invalid class ID: {cls_id}")
        
        self.logger.debug(f"Final detections: {len(detections)}")
        return detections

    def _detect_torch(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """Detection using pure PyTorch model"""
        # Preprocess image
        input_tensor = self._preprocess_image(frame)
        
        # Run inference
        with torch.no_grad():
            predictions = self.model(input_tensor)
        
        # Post-process results
        detections = self._postprocess_predictions(predictions, frame.shape)
        
        return detections
    
    def _preprocess_image(self, frame: np.ndarray) -> torch.Tensor:
        """Preprocess image for YOLOv11"""
        # Resize to model input size
        height, width = frame.shape[:2]
        
        # Calculate scale factor
        scale = min(self.input_size / width, self.input_size / height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        # Resize image
        resized = cv2.resize(frame, (new_width, new_height))
        
        # Pad to square
        top = (self.input_size - new_height) // 2
        bottom = self.input_size - new_height - top
        left = (self.input_size - new_width) // 2
        right = self.input_size - new_width - left
        
        padded = cv2.copyMakeBorder(
            resized, top, bottom, left, right, 
            cv2.BORDER_CONSTANT, value=(114, 114, 114)
        )
        
        # Convert to tensor
        image_tensor = torch.from_numpy(padded).permute(2, 0, 1).float()
        image_tensor /= 255.0  # Normalize to [0, 1]
        
        # Add batch dimension
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        return image_tensor
    
    def _postprocess_predictions(self, 
                                predictions: torch.Tensor, 
                                original_shape: tuple) -> List[Dict[str, Any]]:
        """Post-process YOLOv11 predictions"""
        detections = []
        
        # Assuming predictions shape: [batch, num_boxes, 85] (x, y, w, h, conf, classes...)
        pred = predictions[0]  # Remove batch dimension
        
        # Filter by confidence
        conf_mask = pred[:, 4] >= self.confidence_threshold
        pred = pred[conf_mask]
        
        if len(pred) == 0:
            return detections
        
        # Convert boxes from center format to corner format
        boxes = pred[:, :4].clone()
        boxes[:, 0] = pred[:, 0] - pred[:, 2] / 2  # x1 = cx - w/2
        boxes[:, 1] = pred[:, 1] - pred[:, 3] / 2  # y1 = cy - h/2
        boxes[:, 2] = pred[:, 0] + pred[:, 2] / 2  # x2 = cx + w/2
        boxes[:, 3] = pred[:, 1] + pred[:, 3] / 2  # y2 = cy + h/2
        
        # Scale boxes back to original image size
        scale_x = original_shape[1] / self.input_size
        scale_y = original_shape[0] / self.input_size
        
        boxes[:, [0, 2]] *= scale_x
        boxes[:, [1, 3]] *= scale_y
        
        # Get class predictions
        class_probs = pred[:, 5:]
        class_ids = torch.argmax(class_probs, dim=1)
        max_class_probs = torch.max(class_probs, dim=1)[0]
        
        # Apply NMS
        keep_indices = torchvision.ops.nms(
            boxes, 
            pred[:, 4] * max_class_probs,  # Combined confidence
            self.nms_threshold
        )
        
        # Filter results
        final_boxes = boxes[keep_indices]
        final_scores = pred[keep_indices, 4]
        final_classes = class_ids[keep_indices]
        
        # Convert to detection format
        for box, score, cls_id in zip(final_boxes, final_scores, final_classes):
            cls_id = int(cls_id.item())
            
            # Filter by target classes - FIXED: Now uses proper class checking
            if cls_id < len(self.class_names):
                class_name = self.class_names[cls_id]
                
                if class_name in self.target_classes:
                    detections.append({
                        'bbox': box.cpu().numpy().tolist(),
                        'score': float(score.item()),
                        'class_name': class_name,
                        'class_id': cls_id
                    })
        
        return detections
    
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        return self.class_names.copy()
    
    def set_confidence_threshold(self, threshold: float) -> None:
        """Set detection confidence threshold"""
        self.confidence_threshold = threshold
        self.logger.info(f"Updated confidence threshold to {threshold}")
    
    def set_nms_threshold(self, threshold: float) -> None:
        """Set NMS threshold"""
        self.nms_threshold = threshold
        self.logger.info(f"Updated NMS threshold to {threshold}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            'model_path': self.model_path,
            'device': str(self.device),
            'input_size': self.input_size,
            'confidence_threshold': self.confidence_threshold,
            'nms_threshold': self.nms_threshold,
            'target_classes': self.target_classes,
            'num_classes': len(self.class_names),
            'all_classes': dict(enumerate(self.class_names))
        }

================
File: argus_track/filters/__init__.py
================
"""Motion filters for tracking"""

from .kalman import StaticOptimizedKalmanBoxTracker,KalmanBoxTracker, batch_predict_kalman, batch_predict_kalman_enhanced

__all__ = ["StaticOptimizedKalmanBoxTracker", "batch_predict_kalman", "KalmanBoxTracker", "batch_predict_kalman_enhanced"]

================
File: argus_track/filters/kalman.py
================
"""Kalman filter implementation for object tracking"""

import numpy as np
from filterpy.kalman import KalmanFilter
from typing import List, Optional

from ..core import Detection


class KalmanBoxTracker:
    """
    Kalman filter implementation optimized for static/slow-moving objects
    
    State vector: [x, y, w, h, vx, vy, vw, vh]
    where (x, y) is center position, (w, h) is width/height,
    and v* are the corresponding velocities
    """
    
    def __init__(self, initial_detection: Detection):
        """
        Initialize Kalman filter with detection
        
        Args:
            initial_detection: First detection to initialize the filter
        """
        # 8-dimensional state, 4-dimensional measurement
        self.kf = KalmanFilter(dim_x=8, dim_z=4)
        
        # State transition matrix (constant velocity model)
        self.kf.F = np.array([
            [1, 0, 0, 0, 1, 0, 0, 0],  # x = x + vx
            [0, 1, 0, 0, 0, 1, 0, 0],  # y = y + vy
            [0, 0, 1, 0, 0, 0, 1, 0],  # w = w + vw
            [0, 0, 0, 1, 0, 0, 0, 1],  # h = h + vh
            [0, 0, 0, 0, 1, 0, 0, 0],  # vx = vx
            [0, 0, 0, 0, 0, 1, 0, 0],  # vy = vy
            [0, 0, 0, 0, 0, 0, 1, 0],  # vw = vw
            [0, 0, 0, 0, 0, 0, 0, 1]   # vh = vh
        ])
        
        # Measurement matrix (we only measure position and size)
        self.kf.H = np.array([
            [1, 0, 0, 0, 0, 0, 0, 0],  # x
            [0, 1, 0, 0, 0, 0, 0, 0],  # y
            [0, 0, 1, 0, 0, 0, 0, 0],  # w
            [0, 0, 0, 1, 0, 0, 0, 0]   # h
        ])
        
        xywh = initial_detection.xywh
        self.kf.x[0] = xywh[0]  # center x
        self.kf.x[1] = xywh[1]  # center y
        self.kf.x[2] = xywh[2]  # width
        self.kf.x[3] = xywh[3]  # height
        self.kf.x[4:] = 0       # Zero initial velocity (static assumption)
        
        # Initial uncertainty (higher for velocities)
        self.kf.P[4:, 4:] *= 1000  # High uncertainty for velocities
        self.kf.P[:4, :4] *= 10    # Lower uncertainty for position
        
        # Process noise (very low for static objects)
        self.kf.Q[4:, 4:] *= 0.01  # Minimal velocity changes expected
        self.kf.Q[:4, :4] *= 0.1   # Small position changes expected
        
        # Measurement noise
        self.kf.R *= 10.0
        
        self.time_since_update = 0
        self.history: List[Detection] = []
        self.hits = 1
        self.age = 1
        
        # Gap handling
        self.consecutive_gaps = 0
        self.prediction_confidence = 1.0
        
    def predict(self) -> np.ndarray:
        """
        Predict next state with gap tolerance
        
        Returns:
            Predicted bounding box in tlbr format
        """
        # Adapt process noise based on gap duration
        if self.time_since_update > 0:
            self.consecutive_gaps += 1
            
            # Increase uncertainty during longer gaps
            gap_factor = min(3.0, 1.0 + self.consecutive_gaps * 0.1)
            
            # For static objects, increase only slightly
            self.kf.Q[:4, :4] *= gap_factor * 0.5
            self.kf.Q[4:, 4:] *= gap_factor * 0.2
        else:
            self.consecutive_gaps = 0
        
        # Handle numerical stability
        if np.trace(self.kf.P[4:, 4:]) > 1e4:
            self.kf.P[4:, 4:] *= 0.01
            
        self.kf.predict()
        self.age += 1
        self.time_since_update += 1
        
        # Reduce prediction confidence during gaps
        if self.consecutive_gaps > 0:
            confidence_decay = 0.95 ** self.consecutive_gaps
            self.prediction_confidence = max(0.1, confidence_decay)
        else:
            self.prediction_confidence = 1.0
        
        return self.get_state()
    
    def update(self, detection: Detection) -> None:
        """
        Update filter with new detection
        
        Args:
            detection: New detection measurement
        """
        self.time_since_update = 0
        self.consecutive_gaps = 0
        self.history.append(detection)
        self.hits += 1
        self.prediction_confidence = 1.0
        
        # Perform Kalman update
        self.kf.update(detection.xywh)

    def get_state(self) -> np.ndarray:
        """
        Get current state estimate with confidence weighting
        
        Returns:
            Bounding box in tlbr format
        """
        x, y, w, h = self.kf.x[:4].flatten()
        
        # Apply confidence-based adjustment for predictions during gaps
        if self.prediction_confidence < 1.0 and len(self.history) > 0:
            # Blend prediction with last known good position
            last_detection = self.history[-1]
            last_center = last_detection.center
            last_size = np.array([last_detection.bbox[2] - last_detection.bbox[0],
                                 last_detection.bbox[3] - last_detection.bbox[1]])
            
            # Weighted blend based on confidence
            blend_factor = 1.0 - self.prediction_confidence
            x = x * self.prediction_confidence + last_center[0] * blend_factor
            y = y * self.prediction_confidence + last_center[1] * blend_factor
            w = w * self.prediction_confidence + last_size[0] * blend_factor
            h = h * self.prediction_confidence + last_size[1] * blend_factor
        
        return np.array([
            x - w/2,  # x1
            y - h/2,  # y1
            x + w/2,  # x2
            y + h/2   # y2
        ])
    
    def get_velocity(self) -> np.ndarray:
        """
        Get current velocity estimate
        
        Returns:
            Velocity vector [vx, vy]
        """
        return self.kf.x[4:6]
    
    def get_prediction_confidence(self) -> float:
        """Get current prediction confidence"""
        return self.prediction_confidence
    
    def is_stable(self, threshold: float = 5.0) -> bool:
        """Check if track represents a stable/static object"""
        if len(self.history) < 3:
            return False
        
        recent_centers = [det.center for det in self.history[-5:]]
        positions = np.array(recent_centers)
        position_std = np.std(positions, axis=0)
        max_std = np.max(position_std)
        
        return max_std < threshold


def batch_predict_kalman(kalman_trackers: List[KalmanBoxTracker]) -> np.ndarray:
    """
    Batch prediction for multiple Kalman filters
    
    Args:
        kalman_trackers: List of KalmanBoxTracker instances
        
    Returns:
        Array of predicted bounding boxes in tlbr format
    """
    if not kalman_trackers:
        return np.array([])
    
    # Collect predicted states
    predictions = np.zeros((len(kalman_trackers), 4))
    
    for i, tracker in enumerate(kalman_trackers):
        # Predict and get state
        tracker.predict()
        predictions[i] = tracker.get_state()
    
    return predictions


class StaticOptimizedKalmanBoxTracker(KalmanBoxTracker):
    """
    Enhanced Kalman filter specifically optimized for static objects
    """
    
    def __init__(self, initial_detection: Detection, static_mode: bool = True):
        """
        Initialize enhanced Kalman filter
        
        Args:
            initial_detection: First detection
            static_mode: Whether to optimize for static objects
        """
        super().__init__(initial_detection)
        
        self.static_mode = static_mode
        
        if static_mode:
            # Apply static object optimizations
            self.kf.P[4:, 4:] *= 0.1      # Much lower velocity uncertainty
            self.kf.P[:4, :4] *= 0.5      # Lower position uncertainty
            
            # Very low process noise for static objects
            self.kf.Q[4:, 4:] *= 0.001    # Almost no velocity changes
            self.kf.Q[:4, :4] *= 0.01     # Minimal position changes
            
            # Lower measurement noise for consistent detections
            self.kf.R *= 0.5
    
    def predict(self) -> np.ndarray:
        """Enhanced prediction for static objects"""
        # For static objects, be more conservative with uncertainty growth
        if self.static_mode and self.time_since_update > 0:
            # Slower uncertainty growth for static objects
            gap_factor = min(2.0, 1.0 + self.consecutive_gaps * 0.05)
            self.kf.Q[:4, :4] *= gap_factor * 0.3
            self.kf.Q[4:, 4:] *= gap_factor * 0.1
        
        return super().predict()


# Enhanced batch prediction for static objects
def batch_predict_kalman_enhanced(kalman_trackers: List[KalmanBoxTracker]) -> np.ndarray:
    """
    Enhanced batch prediction with confidence information
    
    Args:
        kalman_trackers: List of Kalman trackers
        
    Returns:
        Array of predicted bounding boxes with confidence
    """
    if not kalman_trackers:
        return np.array([])
    
    predictions = np.zeros((len(kalman_trackers), 5))  # Include confidence
    
    for i, tracker in enumerate(kalman_trackers):
        bbox = tracker.predict()
        confidence = tracker.get_prediction_confidence() if hasattr(tracker, 'get_prediction_confidence') else 1.0
        
        predictions[i, :4] = bbox
        predictions[i, 4] = confidence
    
    return predictions


# Utility functions for gap tolerance analysis
def analyze_tracking_gaps(kalman_trackers: List[KalmanBoxTracker]) -> dict:
    """
    Analyze tracking gaps across all trackers
    
    Args:
        kalman_trackers: List of Kalman trackers
        
    Returns:
        Dictionary with gap analysis
    """
    total_gaps = 0
    max_gap = 0
    trackers_with_gaps = 0
    total_predictions = 0
    
    for tracker in kalman_trackers:
        if hasattr(tracker, 'consecutive_gaps'):
            if tracker.consecutive_gaps > 0:
                total_gaps += tracker.consecutive_gaps
                max_gap = max(max_gap, tracker.consecutive_gaps)
                trackers_with_gaps += 1
        
        total_predictions += tracker.age
    
    return {
        'total_gaps': total_gaps,
        'max_gap': max_gap,
        'trackers_with_gaps': trackers_with_gaps,
        'total_trackers': len(kalman_trackers),
        'avg_gap_per_tracker': total_gaps / len(kalman_trackers) if kalman_trackers else 0,
        'gap_ratio': total_gaps / total_predictions if total_predictions > 0 else 0
    }

================
File: argus_track/stereo/matching.py
================
"""Stereo matching for object detections"""

import cv2
import numpy as np
from typing import List, Tuple, Optional, Dict
from scipy.optimize import linear_sum_assignment
import logging

from ..core import Detection
from ..core.stereo import StereoDetection
from ..config import StereoCalibrationConfig
from ..utils.iou import calculate_iou


class StereoMatcher:
    """
    Matches detections between left and right camera views using epipolar constraints
    and appearance similarity for robust stereo correspondence.
    """
    
    def __init__(self, 
                 calibration: StereoCalibrationConfig,
                 max_disparity: float = 1000.0,
                 min_disparity: float = -50.0,
                 epipolar_threshold: float = 16.0,
                 iou_threshold: float = 0.0,
                 cost_threshold: float = 0.8):  # Added cost threshold parameter
        """
        Initialize stereo matcher
        
        Args:
            calibration: Stereo camera calibration
            max_disparity: Maximum disparity in pixels
            min_disparity: Minimum disparity in pixels
            epipolar_threshold: Maximum distance from epipolar line in pixels
            iou_threshold: Minimum IoU for detection matching
            cost_threshold: Maximum matching cost to accept (lower is better)
        """
        self.calibration = calibration
        self.max_disparity = max_disparity
        self.min_disparity = min_disparity
        self.epipolar_threshold = epipolar_threshold
        self.iou_threshold = iou_threshold
        self.cost_threshold = cost_threshold  # Added this line
        self.logger = logging.getLogger(f"{__name__}.StereoMatcher")
        
        # Precompute rectification maps if available
        self.has_rectification = (calibration.P1 is not None and 
                                calibration.P2 is not None and 
                                calibration.Q is not None)
        
        if self.has_rectification:
            self.logger.info("Using rectified stereo matching")
        else:
            self.logger.info("Using unrectified stereo matching with epipolar constraints")
    
    def match_detections(self, 
                        left_detections: List[Detection], 
                        right_detections: List[Detection]) -> List[StereoDetection]:
        """
        Match detections between left and right cameras
        
        Args:
            left_detections: Detections from left camera
            right_detections: Detections from right camera
            
        Returns:
            List of stereo detection pairs
        """
        if not left_detections or not right_detections:
            return []
        
        # Calculate matching costs
        cost_matrix = self._calculate_matching_costs(left_detections, right_detections)
        
        # Apply Hungarian algorithm for optimal matching
        row_indices, col_indices = linear_sum_assignment(cost_matrix)
        
        stereo_detections = []
        
        for left_idx, right_idx in zip(row_indices, col_indices):
            cost = cost_matrix[left_idx, right_idx]
            
            # Filter matches by cost threshold
            if cost < 1.0:  # Cost of 1.0 means no valid match
                left_det = left_detections[left_idx]
                right_det = right_detections[right_idx]
                
                # Calculate disparity and depth
                disparity = self._calculate_disparity(left_det, right_det)
                depth = self._estimate_depth(disparity)
                
                # Calculate 3D world coordinates
                world_coords = self._triangulate_point(left_det, right_det)
                
                # Calculate stereo confidence
                confidence = self._calculate_stereo_confidence(left_det, right_det, cost)
                
                stereo_detection = StereoDetection(
                    left_detection=left_det,
                    right_detection=right_det,
                    disparity=disparity,
                    depth=depth,
                    world_coordinates=world_coords,
                    stereo_confidence=confidence
                )
                
                stereo_detections.append(stereo_detection)
        
        self.logger.debug(f"Matched {len(stereo_detections)} stereo pairs from "
                         f"{len(left_detections)} left and {len(right_detections)} right detections")
        
        return stereo_detections
    
    def _calculate_matching_costs(self, 
                                 left_detections: List[Detection], 
                                 right_detections: List[Detection]) -> np.ndarray:
        """Calculate cost matrix for detection matching"""
        n_left = len(left_detections)
        n_right = len(right_detections)
        cost_matrix = np.ones((n_left, n_right))  # Initialize with high cost
        
        for i, left_det in enumerate(left_detections):
            for j, right_det in enumerate(right_detections):
                # Check epipolar constraint
                if not self._check_epipolar_constraint(left_det, right_det):
                    continue
                
                # Check disparity range
                disparity = self._calculate_disparity(left_det, right_det)
                if not (self.min_disparity <= disparity <= self.max_disparity):
                    continue
                
                # Calculate geometric cost
                geometric_cost = self._calculate_geometric_cost(left_det, right_det)
                
                # Calculate appearance cost (simplified - could use features)
                appearance_cost = self._calculate_appearance_cost(left_det, right_det)
                
                # Combine costs
                total_cost = 0.7 * geometric_cost + 0.3 * appearance_cost
                cost_matrix[i, j] = total_cost
        
        return cost_matrix
    
    def _check_epipolar_constraint(self, left_det: Detection, right_det: Detection) -> bool:
        """Check if detections satisfy epipolar constraint"""
        if self.has_rectification:
            # For rectified images, epipolar lines are horizontal
            left_center = left_det.center
            right_center = right_det.center
            
            y_diff = abs(left_center[1] - right_center[1])
            return y_diff <= self.epipolar_threshold
        else:
            # Use fundamental matrix for unrectified images
            if self.calibration.F is None:
                # Fallback: assume roughly horizontal epipolar lines
                left_center = left_det.center
                right_center = right_det.center
                y_diff = abs(left_center[1] - right_center[1])
                return y_diff <= self.epipolar_threshold * 2
            
            # Proper epipolar constraint using fundamental matrix
            left_point = np.array([left_det.center[0], left_det.center[1], 1])
            right_point = np.array([right_det.center[0], right_det.center[1], 1])
            
            # Calculate epipolar line
            epipolar_line = self.calibration.F @ left_point
            
            # Distance from point to line
            distance = abs(np.dot(epipolar_line, right_point)) / np.sqrt(epipolar_line[0]**2 + epipolar_line[1]**2)
            
            return distance <= self.epipolar_threshold
    
    def _calculate_disparity(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate disparity between matched detections"""
        left_center = left_det.center
        right_center = right_det.center
        
        # Disparity is the horizontal difference
        disparity = left_center[0] - right_center[0]
        return max(0, disparity)  # Ensure positive disparity
    
    def _estimate_depth(self, disparity: float) -> float:
        """Estimate depth from disparity"""
        if disparity <= 0:
            return float('inf')
        
        # Use calibration baseline and focal length
        baseline = self.calibration.baseline
        focal_length = self.calibration.camera_matrix_left[0, 0]  # fx
        
        if baseline == 0 or focal_length == 0:
            self.logger.warning("Invalid calibration parameters for depth estimation")
            return float('inf')
        
        depth = (baseline * focal_length) / disparity
        return depth
    
    def _triangulate_point(self, left_det: Detection, right_det: Detection) -> np.ndarray:
        """Triangulate 3D point from stereo detections"""
        left_center = left_det.center
        right_center = right_det.center
        
        # Prepare points for triangulation
        left_point = np.array([left_center[0], left_center[1]], dtype=np.float32)
        right_point = np.array([right_center[0], right_center[1]], dtype=np.float32)
        
        if self.has_rectification and self.calibration.Q is not None:
            # Use Q matrix for rectified images
            disparity = self._calculate_disparity(left_det, right_det)
            
            # Create homogeneous point
            point_2d = np.array([left_center[0], left_center[1], disparity, 1])
            
            # Transform to 3D
            point_3d = self.calibration.Q @ point_2d
            
            if point_3d[3] != 0:
                point_3d = point_3d / point_3d[3]
            
            return point_3d[:3]
        else:
            # Use projection matrices if available
            if self.calibration.P1 is not None and self.calibration.P2 is not None:
                # Triangulate using OpenCV
                points_4d = cv2.triangulatePoints(
                    self.calibration.P1,
                    self.calibration.P2,
                    left_point.reshape(2, 1),
                    right_point.reshape(2, 1)
                )
                
                # Convert from homogeneous coordinates
                if points_4d[3, 0] != 0:
                    point_3d = points_4d[:3, 0] / points_4d[3, 0]
                else:
                    point_3d = points_4d[:3, 0]
                
                return point_3d
            else:
                # Fallback: simple depth estimation
                depth = self._estimate_depth(self._calculate_disparity(left_det, right_det))
                
                # Convert to 3D using camera intrinsics
                fx = self.calibration.camera_matrix_left[0, 0]
                fy = self.calibration.camera_matrix_left[1, 1]
                cx = self.calibration.camera_matrix_left[0, 2]
                cy = self.calibration.camera_matrix_left[1, 2]
                
                x = (left_center[0] - cx) * depth / fx
                y = (left_center[1] - cy) * depth / fy
                z = depth
                
                return np.array([x, y, z])
    
    def _calculate_geometric_cost(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate geometric matching cost"""
        # Size similarity
        left_area = left_det.area
        right_area = right_det.area
        
        if left_area == 0 or right_area == 0:
            size_cost = 1.0
        else:
            size_ratio = min(left_area, right_area) / max(left_area, right_area)
            size_cost = 1.0 - size_ratio
        
        # Y-coordinate difference (should be small for good stereo)
        y_diff = abs(left_det.center[1] - right_det.center[1])
        y_cost = min(1.0, y_diff / 50.0)  # Normalize by expected max difference
        
        # Disparity reasonableness
        disparity = self._calculate_disparity(left_det, right_det)
        if disparity < self.min_disparity or disparity > self.max_disparity:
            disparity_cost = 1.0
        else:
            # Prefer moderate disparities
            normalized_disparity = disparity / self.max_disparity
            disparity_cost = abs(normalized_disparity - 0.3)  # Prefer ~30% of max disparity
        
        return (size_cost + y_cost + disparity_cost) / 3.0
    
    def _calculate_appearance_cost(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate appearance-based matching cost (simplified)"""
        # For now, use detection confidence similarity
        conf_diff = abs(left_det.score - right_det.score)
        
        # Class consistency
        class_cost = 0.0 if left_det.class_id == right_det.class_id else 1.0
        
        return (conf_diff + class_cost) / 2.0
    
    def _calculate_stereo_confidence(self, 
                                   left_det: Detection, 
                                   right_det: Detection, 
                                   matching_cost: float) -> float:
        """Calculate confidence for stereo match"""
        # Base confidence from detection scores
        base_confidence = (left_det.score + right_det.score) / 2.0
        
        # Reduce confidence based on matching cost
        matching_confidence = 1.0 - matching_cost
        
        # Combine confidences
        stereo_confidence = base_confidence * matching_confidence
        
        return min(1.0, max(0.0, stereo_confidence))

================
File: argus_track/trackers/__init__.py
================
"""Tracking algorithms"""

from .stereo_lightpost_tracker import EnhancedStereoLightPostTracker
from .lightpost_tracker import EnhancedLightPostTracker
from .simplified_lightpost_tracker import SimplifiedLightPostTracker

__all__ = ["EnhancedLightPostTracker",  "EnhancedStereoLightPostTracker", "SimplifiedLightPostTracker"]

================
File: argus_track/trackers/stereo_lightpost_tracker.py
================
# argus_track/trackers/stereo_lightpost_tracker.py (UPDATED)

"""Enhanced Stereo Light Post Tracker with Integrated GPS Extraction"""

import cv2
import numpy as np
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
import json

from ultralytics import YOLO
from ..config import TrackerConfig, StereoCalibrationConfig
from ..core import Detection, GPSData
from ..core.stereo import StereoDetection, StereoFrame, StereoTrack
from ..detectors import ObjectDetector
from ..stereo import StereoMatcher, StereoTriangulator, StereoCalibrationManager
from ..utils.visualization import draw_tracks
from ..utils.io import save_tracking_results
from ..utils.gps_utils import sync_gps_with_frames, GeoLocation
from ..utils.gps_extraction import extract_gps_from_stereo_videos, save_gps_to_csv


class EnhancedStereoLightPostTracker:
    """
    Enhanced stereo light post tracking system with integrated GPS extraction
    
    This class includes:
    1. Automatic GPS extraction from GoPro videos
    2. Stereo object detection and matching
    3. 3D triangulation for depth estimation
    4. Multi-object tracking with ByteTrack
    5. GPS data synchronization (every 6th frame)
    6. 3D to GPS coordinate transformation
    7. Static object analysis and geolocation
    """
    
    def __init__(self, 
                 config: TrackerConfig,
                 model_path: str,
                 stereo_calibration: StereoCalibrationConfig):
        """
        Initialize enhanced stereo light post tracker
        
        Args:
            config: Tracker configuration
            detector: Object detection module
            stereo_calibration: Stereo camera calibration
        """
        self.config = config
        # self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.EnhancedStereoLightPostTracker")
        
        # Stereo processing components
        self.calibration_manager = StereoCalibrationManager(stereo_calibration)
        self.stereo_matcher = StereoMatcher(
            calibration=stereo_calibration,
            epipolar_threshold=16.0,
            iou_threshold=config.stereo_match_threshold
        )
        self.triangulator = StereoTriangulator(stereo_calibration)
        
        # Tracking components
        self.model = YOLO(model_path)
        # self.left_tracker = ByteTrack(config)
        # self.right_tracker = ByteTrack(config)
        
        # Stereo tracking data
        self.stereo_tracks: Dict[int, StereoTrack] = {}
        self.stereo_frames: List[StereoFrame] = []
        self.track_id_counter = 0
        
        # GPS and geolocation
        self.gps_data_history: List[GPSData] = []
        self.estimated_locations: Dict[int, GeoLocation] = {}
        self.gps_extraction_method: str = 'none'
        
        # Performance monitoring
        self.processing_times = []
        self.frame_count = 0
        
        # Validate calibration
        is_valid, errors = self.calibration_manager.validate_calibration()
        if not is_valid:
            self.logger.warning(f"Calibration validation failed: {errors}")
        
        self.logger.info("Initialized enhanced stereo light post tracker")
        self.logger.info(f"Calibration: {self.calibration_manager.get_calibration_summary()}")
    
    def process_stereo_video_with_auto_gps(self, 
                                          left_video_path: str,
                                          right_video_path: str,
                                          output_path: Optional[str] = None,
                                          save_results: bool = True,
                                          gps_extraction_method: str = 'auto',
                                          save_extracted_gps: bool = True) -> Dict[int, StereoTrack]:
        """
        Process stereo video pair with automatic GPS extraction
        
        Args:
            left_video_path: Path to left camera video
            right_video_path: Path to right camera video
            output_path: Optional path for output video
            save_results: Whether to save tracking results
            gps_extraction_method: GPS extraction method ('auto', 'exiftool', 'gopro_api')
            save_extracted_gps: Whether to save extracted GPS data to CSV
            
        Returns:
            Dictionary of stereo tracks
        """
        self.logger.info("=== Enhanced Stereo Processing with GPS Extraction ===")
        self.logger.info(f"Left video: {left_video_path}")
        self.logger.info(f"Right video: {right_video_path}")
        
        # Step 1: Extract GPS data from videos
        self.logger.info("Step 1: Extracting GPS data from videos...")
        gps_data, method_used = extract_gps_from_stereo_videos(
            left_video_path, right_video_path, gps_extraction_method
        )
        
        self.gps_extraction_method = method_used
        
        if gps_data:
            self.logger.info(f"✅ Successfully extracted {len(gps_data)} GPS points using {method_used}")
            
            # Save extracted GPS data if requested
            if save_extracted_gps:
                gps_csv_path = Path(left_video_path).with_suffix('.csv')
                save_gps_to_csv(gps_data, str(gps_csv_path))
                self.logger.info(f"Saved GPS data to: {gps_csv_path}")
        else:
            self.logger.warning("⚠️  No GPS data extracted - proceeding without geolocation")
            gps_data = None
        
        # Step 2: Process stereo video with extracted GPS data
        self.logger.info("Step 2: Processing stereo video with tracking...")
        return self.process_stereo_video(
            left_video_path=left_video_path,
            right_video_path=right_video_path,
            gps_data=gps_data,
            output_path=output_path,
            save_results=save_results
        )
    
    def process_stereo_video(self, 
                            left_video_path: str,
                            right_video_path: str,
                            gps_data: Optional[List[GPSData]] = None,
                            output_path: Optional[str] = None,
                            save_results: bool = True) -> Dict[int, StereoTrack]:
        """
        Process stereo video pair with tracking and geolocation
        
        Args:
            left_video_path: Path to left camera video
            right_video_path: Path to right camera video
            gps_data: Optional GPS data synchronized with frames
            output_path: Optional path for output video
            save_results: Whether to save tracking results
            
        Returns:
            Dictionary of stereo tracks
        """
        self.logger.info(f"Processing stereo videos: {left_video_path}, {right_video_path}")
        
        # Open video captures
        left_cap = cv2.VideoCapture(left_video_path)
        right_cap = cv2.VideoCapture(right_video_path)
        
        if not left_cap.isOpened() or not right_cap.isOpened():
            error_msg = "Could not open one or both video files"
            self.logger.error(error_msg)
            raise IOError(error_msg)
        
        # Get video properties
        fps = left_cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(left_cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(left_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(left_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        video_duration = total_frames / fps
        
        self.logger.info(f"Video properties: {total_frames} frames, {fps} FPS, {width}x{height}")
        self.logger.info(f"Video duration: {video_duration:.1f} seconds")
        
        # Setup video writer if output requested
        out_writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            # Create side-by-side output
            out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))
        
        # Synchronize GPS data with frame rate if available
        if gps_data:
            from ..utils.gps_extraction import GoProGPSExtractor
            extractor = GoProGPSExtractor(fps_video=fps, fps_gps=10.0)
            
            # Synchronize GPS data to match video timeline
            gps_frame_data = extractor.synchronize_with_video(
                gps_data, video_duration, target_fps=10.0
            )
            self.logger.info(f"Synchronized {len(gps_frame_data)} GPS points with video timeline")
        else:
            gps_frame_data = []
        
        # Process frames
        processed_frames = 0
        
        try:
            while True:
                # Read frame pair
                left_ret, left_frame = left_cap.read()
                right_ret, right_frame = right_cap.read()
                
                if not left_ret or not right_ret:
                    break
                
                start_time = time.time()
                
                # Process every 6th frame (GPS synchronization strategy)
                if processed_frames % self.config.gps_frame_interval == 0:
                    gps_index = processed_frames // self.config.gps_frame_interval
                    current_gps = gps_frame_data[gps_index] if gps_index < len(gps_frame_data) else None
                    
                    # Process stereo frame
                    stereo_frame = self._process_stereo_frame_pair(
                        left_frame, right_frame, processed_frames, current_gps
                    )
                    
                    if stereo_frame:
                        self.stereo_frames.append(stereo_frame)
                        
                        # Update tracking
                        self._update_stereo_tracking(stereo_frame)
                
                # Visualize if output requested
                if out_writer:
                    vis_frame = self._create_stereo_visualization(left_frame, right_frame)
                    out_writer.write(vis_frame)
                
                # Performance monitoring
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                
                # Progress logging
                if processed_frames % 300 == 0:  # Every 10 seconds at 30fps
                    avg_time = np.mean(self.processing_times[-100:]) if self.processing_times else 0
                    progress = processed_frames / total_frames * 100
                    self.logger.info(
                        f"Processed {processed_frames}/{total_frames} frames "
                        f"({progress:.1f}%) Avg time: {avg_time*1000:.1f}ms"
                    )
                
                processed_frames += 1
                
        except Exception as e:
            self.logger.error(f"Error processing stereo video: {e}")
            raise
        finally:
            # Cleanup
            left_cap.release()
            right_cap.release()
            if out_writer:
                out_writer.release()
            cv2.destroyAllWindows()
        
        # Post-processing: estimate locations for static tracks
        self._estimate_stereo_track_locations()
        
        # Save results if requested
        if save_results:
            self._save_enhanced_stereo_results(left_video_path, fps, width, height)
        
        self.logger.info(f"Processing complete. Tracked {len(self.stereo_tracks)} stereo objects")
        return self.stereo_tracks
    
    def _process_stereo_frame_pair(self, 
                                  left_frame: np.ndarray, 
                                  right_frame: np.ndarray,
                                  frame_id: int,
                                  gps_data: Optional[GPSData]) -> Optional[StereoFrame]:
        """Process a single stereo frame pair"""

        # Rectify images if calibration supports it
        if self.config.stereo_mode:
            left_rect, right_rect = self.calibration_manager.rectify_image_pair(
                left_frame, right_frame
            )
        else:
            left_rect, right_rect = left_frame, right_frame
        
        # Detect objects in both frames
        left_detections = self._detect_objects(left_rect, frame_id, 'left')
        
        right_detections = self._detect_objects(right_rect, frame_id, 'right')
        
        if not left_detections and not right_detections:
            return None
        
        # Match detections between left and right views
        stereo_detections = []
        if left_detections and right_detections:
            stereo_detections = self.stereo_matcher.match_detections(
                left_detections, right_detections
            )
            
            # Validate triangulation results
            valid_stereo_detections = []
            for stereo_det in stereo_detections:
                if self.triangulator.validate_triangulation(stereo_det):
                    valid_stereo_detections.append(stereo_det)
                else:
                    self.logger.debug(f"Invalid triangulation for detection at frame {frame_id}")
            
            stereo_detections = valid_stereo_detections
        
        # Create stereo frame
        stereo_frame = StereoFrame(
            frame_id=frame_id,
            timestamp=gps_data.timestamp if gps_data else frame_id / 30.0,  # Assume 30fps fallback
            left_frame=left_rect,
            right_frame=right_rect,
            left_detections=left_detections,
            right_detections=right_detections,
            stereo_detections=stereo_detections,
            gps_data=gps_data
        )
        
        return stereo_frame
    
    def _detect_objects(self, frame: np.ndarray, frame_id: int, camera: str) -> List[Detection]:
        """Detect objects in a single frame"""
        raw_detections = self.detector.detect(frame)
        
        detections = []
        for det in raw_detections:
            detection = Detection(
                bbox=np.array(det['bbox']),
                score=det['score'],
                class_id=det['class_id'],
                frame_id=frame_id
            )
            detections.append(detection)
        
        return detections
    
    def _update_stereo_tracking(self, stereo_frame: StereoFrame) -> None:
        """Update stereo tracking with new frame"""
        
        # Update individual camera trackers (for robustness)
        left_tracks = self.left_tracker.update(stereo_frame.left_detections)
        right_tracks = self.right_tracker.update(stereo_frame.right_detections)
        
        # Process stereo detections for 3D tracking
        for stereo_det in stereo_frame.stereo_detections:
            # Find corresponding tracks in left/right trackers
            left_track_id = self._find_matching_track(stereo_det.left_detection, left_tracks)
            right_track_id = self._find_matching_track(stereo_det.right_detection, right_tracks)
            
            if left_track_id is not None and right_track_id is not None:
                # Find existing stereo track or create new one
                stereo_track_id = self._get_or_create_stereo_track(left_track_id, right_track_id)
                
                if stereo_track_id in self.stereo_tracks:
                    # Update existing stereo track
                    stereo_track = self.stereo_tracks[stereo_track_id]
                    stereo_track.stereo_detections.append(stereo_det)
                    stereo_track.world_trajectory.append(stereo_det.world_coordinates)
                    
                    # Add GPS coordinate if available
                    if stereo_frame.gps_data:
                        # Transform to GPS coordinates
                        gps_locations = self.triangulator.world_to_gps_coordinates(
                            [stereo_det.world_coordinates], stereo_frame.gps_data
                        )
                        if gps_locations:
                            stereo_track.gps_trajectory.append(
                                np.array([gps_locations[0].latitude, gps_locations[0].longitude])
                            )
                    
                    # Update depth consistency
                    self._update_depth_consistency(stereo_track)
    
    def _find_matching_track(self, detection: Detection, tracks: List) -> Optional[int]:
        """Find track that matches the given detection"""
        best_track_id = None
        best_iou = 0.0
        
        for track in tracks:
            if track.last_detection:
                from ..utils.iou import calculate_iou
                iou = calculate_iou(detection.bbox, track.last_detection.bbox)
                if iou > best_iou and iou > 0.5:  # Minimum IoU threshold
                    best_iou = iou
                    best_track_id = track.track_id
        
        return best_track_id
    
    def _get_or_create_stereo_track(self, left_track_id: int, right_track_id: int) -> int:
        """Get existing stereo track or create new one"""
        # Look for existing stereo track that matches either left or right track
        for stereo_id, stereo_track in self.stereo_tracks.items():
            # For simplicity, use left track ID as primary identifier
            if stereo_id == left_track_id:
                return stereo_id
        
        # Create new stereo track
        stereo_track = StereoTrack(
            track_id=left_track_id,  # Use left track ID
            stereo_detections=[],
            world_trajectory=[],
            gps_trajectory=[]
        )
        
        self.stereo_tracks[left_track_id] = stereo_track
        return left_track_id
    
    def _update_depth_consistency(self, stereo_track: StereoTrack) -> None:
        """Update depth consistency metric for a stereo track"""
        if len(stereo_track.stereo_detections) < 3:
            return
        
        # Calculate depth variance over recent detections
        recent_depths = [det.depth for det in stereo_track.stereo_detections[-10:]]
        depth_std = np.std(recent_depths)
        
        # Consistency is inversely related to standard deviation
        stereo_track.depth_consistency = 1.0 / (1.0 + depth_std)
    
    def _estimate_stereo_track_locations(self) -> None:
        """Estimate final GPS locations for static stereo tracks"""
        for track_id, stereo_track in self.stereo_tracks.items():
            if stereo_track.is_static_3d and len(stereo_track.gps_trajectory) >= 3:
                # Get GPS history for this track
                gps_points = []
                for gps_coord in stereo_track.gps_trajectory:
                    # Convert back to GPSData format
                    gps_point = GPSData(
                        timestamp=0.0,  # Timestamp not needed for location estimation
                        latitude=gps_coord[0],
                        longitude=gps_coord[1],
                        altitude=0.0,
                        heading=0.0
                    )
                    gps_points.append(gps_point)
                
                # Estimate location using triangulator
                estimated_location = self.triangulator.estimate_object_location(
                    stereo_track, gps_points
                )
                
                if estimated_location:
                    stereo_track.estimated_location = estimated_location
                    self.estimated_locations[track_id] = estimated_location
                    
                    self.logger.debug(
                        f"Track {track_id} located at ({estimated_location.latitude:.6f}, {estimated_location.longitude:.6f}) "
                        f"reliability: {estimated_location.reliability:.2f}"
                    )
    
    def _create_stereo_visualization(self, 
                                   left_frame: np.ndarray, 
                                   right_frame: np.ndarray) -> np.ndarray:
        """Create side-by-side visualization of stereo tracking"""
        # Draw tracks on both frames
        left_vis = draw_tracks(left_frame, self.left_tracker.active_tracks)
        right_vis = draw_tracks(right_frame, self.right_tracker.active_tracks)
        
        # Create side-by-side visualization
        stereo_vis = np.hstack([left_vis, right_vis])
        
        # Add stereo information overlay
        self._add_stereo_info_overlay(stereo_vis)
        
        return stereo_vis
    
    def _add_stereo_info_overlay(self, stereo_frame: np.ndarray) -> None:
        """Add information overlay to stereo visualization"""
        # Add text information
        info_text = [
            f"Stereo Tracks: {len(self.stereo_tracks)}",
            f"GPS Method: {self.gps_extraction_method}",
            f"GPS Points: {len(self.gps_data_history)}",
            f"Locations: {len(self.estimated_locations)}",
            f"Frame: {self.frame_count}"
        ]
        
        y_offset = 30
        for text in info_text:
            cv2.putText(stereo_frame, text, (10, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            y_offset += 25
    
    def _save_enhanced_stereo_results(self, video_path: str, fps: float, width: int, height: int) -> None:
        """Save enhanced stereo tracking results with GPS extraction info"""
        results_path = Path(video_path).with_suffix('.json')
        
        # Prepare results data
        results = {
            'metadata': {
                'total_frames': len(self.stereo_frames),
                'fps': fps,
                'width': width,
                'height': height,
                'stereo_mode': self.config.stereo_mode,
                'gps_frame_interval': self.config.gps_frame_interval,
                'gps_extraction_method': self.gps_extraction_method,
                'gps_points_extracted': len(self.gps_data_history),
                'processing_times': {
                    'mean': np.mean(self.processing_times) if self.processing_times else 0,
                    'std': np.std(self.processing_times) if self.processing_times else 0,
                    'min': np.min(self.processing_times) if self.processing_times else 0,
                    'max': np.max(self.processing_times) if self.processing_times else 0
                }
            },
            'stereo_tracks': {
                str(track_id): track.to_dict() 
                for track_id, track in self.stereo_tracks.items()
            },
            'estimated_locations': {
                str(track_id): location.__dict__ 
                for track_id, location in self.estimated_locations.items()
            },
            'calibration_summary': self.calibration_manager.get_calibration_summary()
        }
        
        # Save to JSON
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"Saved enhanced stereo tracking results to {results_path}")
        
        # Also save GeoJSON for mapping
        geojson_path = Path(video_path).with_suffix('.geojson')
        self._export_locations_to_geojson(geojson_path)
    
    def _export_locations_to_geojson(self, output_path: Path) -> None:
        """Export estimated locations to GeoJSON format"""
        features = []
        
        for track_id, location in self.estimated_locations.items():
            if location.reliability > 0.5:  # Only export reliable locations
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [location.longitude, location.latitude]
                    },
                    "properties": {
                        "track_id": track_id,
                        "reliability": location.reliability,
                        "accuracy": location.accuracy,
                        "method": "stereo_triangulation_with_auto_gps",
                        "gps_extraction_method": self.gps_extraction_method
                    }
                }
                features.append(feature)
        
        geojson = {
            "type": "FeatureCollection",
            "features": features,
            "metadata": {
                "generator": "Argus Track Enhanced Stereo Tracker",
                "gps_extraction_method": self.gps_extraction_method,
                "total_locations": len(features)
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(geojson, f, indent=2)
        
        self.logger.info(f"Exported {len(features)} locations to GeoJSON: {output_path}")
    
    def get_enhanced_tracking_statistics(self) -> Dict[str, Any]:
        """Get comprehensive enhanced stereo tracking statistics"""
        static_count = sum(1 for track in self.stereo_tracks.values() if track.is_static_3d)
        
        return {
            'total_stereo_tracks': len(self.stereo_tracks),
            'static_tracks': static_count,
            'estimated_locations': len(self.estimated_locations),
            'processed_frames': len(self.stereo_frames),
            'gps_extraction_method': self.gps_extraction_method,
            'gps_points_used': len(self.gps_data_history),
            'avg_depth': np.mean([track.average_depth for track in self.stereo_tracks.values()]) if self.stereo_tracks else 0,
            'avg_depth_consistency': np.mean([track.depth_consistency for track in self.stereo_tracks.values()]) if self.stereo_tracks else 0,
            'calibration_baseline': self.calibration_manager.calibration.baseline if self.calibration_manager.calibration else 0,
            'accuracy_achieved': np.mean([loc.accuracy for loc in self.estimated_locations.values()]) if self.estimated_locations else 0,
            'avg_reliability': np.mean([loc.reliability for loc in self.estimated_locations.values()]) if self.estimated_locations else 0
        }

================
File: argus_track/utils/visualization.py
================
"""Enhanced visualization utilities with real-time display"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
import time
import logging

from ..core import Track, Detection


# Color palette for different track states
TRACK_COLORS = {
    'tentative': (255, 255, 0),    # Yellow
    'confirmed': (0, 255, 0),      # Green  
    'lost': (0, 0, 255),          # Red
    'removed': (128, 128, 128)     # Gray
}

# Class-specific colors
CLASS_COLORS = {
    'Led-150': (255, 0, 0),       # Red
    'Led-240': (0, 0, 255),       # Blue
    'light_post': (0, 255, 0),    # Green
    'street_light': (255, 165, 0), # Orange
    'pole': (128, 0, 128)          # Purple
}


class RealTimeVisualizer:
    """Real-time visualization during tracking"""
    
    def __init__(self, window_name: str = "Argus Track - Real-time Detection", 
                 display_size: Tuple[int, int] = (1280, 720),
                 show_info_panel: bool = True):
        """
        Initialize real-time visualizer
        
        Args:
            window_name: Name of the display window
            display_size: Size of the display window (width, height)
            show_info_panel: Whether to show information panel
        """
        self.window_name = window_name
        self.display_size = display_size
        self.show_info_panel = show_info_panel
        
        # Initialize logger
        self.logger = logging.getLogger(f"{__name__}.RealTimeVisualizer")
        
        # Statistics tracking
        self.frame_count = 0
        self.detection_history = []
        self.fps_history = []
        self.last_time = time.time()
        
        # Create window
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, display_size[0], display_size[1])
        
        # Create default blank frame for error cases
        self.blank_frame = np.zeros((display_size[1], display_size[0], 3), dtype=np.uint8)
        cv2.putText(self.blank_frame, "No frame data available", 
                   (display_size[0]//4, display_size[1]//2), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        self.logger.info(f"🖥️  Real-time visualization window opened: {window_name}")
        self.logger.info("   Press 'q' to quit, 'p' to pause, 's' to save screenshot")

    def _add_info_panel(self, frame: np.ndarray,
                    detections: List[Detection],
                    tracks: List[Track],
                    gps_data: Optional[Dict] = None,
                    frame_info: Optional[Dict] = None) -> np.ndarray:
        """Enhanced information panel with motion prediction and visual feature info"""
        
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        try:
            # Create larger panel for enhanced info
            panel_height = 200  # Increased height
            panel_width = 400   # Increased width
            
            # Create semi-transparent overlay
            overlay = frame.copy()
            cv2.rectangle(overlay, 
                        (frame.shape[1] - panel_width - 10, 10),
                        (frame.shape[1] - 10, panel_height + 10), 
                        (0, 0, 0), -1)
            
            # Blend with original frame
            result = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
            
            # Safety checks
            if detections is None:
                detections = []
            if tracks is None:
                tracks = []
            if frame_info is None:
                frame_info = {}
            
            # Enhanced info lines
            y_offset = 35
            text_color = (255, 255, 255)
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.5
            small_font_scale = 0.4
            
            # === BASIC INFO ===
            info_lines = [
                f"Frame: {frame_info.get('frame_idx', self.frame_count)}",
                f"Detections: {len(detections)}",
                f"Active Tracks: {len([t for t in tracks if getattr(t, 'state', None) in ['tentative', 'confirmed']])}",
            ]
            
            # === MOTION PREDICTION INFO ===
            if frame_info.get('motion_prediction_enabled', False):
                motion_detected = "YES" if frame_info.get('camera_motion_detected', False) else "NO"
                pred_accuracy = frame_info.get('avg_prediction_accuracy', 0)
                info_lines.extend([
                    f"Motion Pred: {motion_detected} ({pred_accuracy:.2f})",
                ])
            
            # === VISUAL FEATURES INFO ===
            if frame_info.get('visual_features_enabled', False):
                tracks_with_features = frame_info.get('tracks_with_features', 0)
                appearance_stability = frame_info.get('avg_appearance_stability', 0)
                info_lines.extend([
                    f"Visual Features: {tracks_with_features} tracks",
                    f"Appearance Stability: {appearance_stability:.2f}",
                ])
            
            # === DETECTION TYPE BREAKDOWN ===
            motion_compensated = len([d for d in detections if getattr(d, 'motion_compensated', False)])
            prediction_matches = len([d for d in detections if getattr(d, 'prediction_match', False)])
            reappearances = len([d for d in detections if getattr(d, 'reappearance_match', False)])
            
            if motion_compensated > 0 or prediction_matches > 0 or reappearances > 0:
                info_lines.append(f"Enhanced: MC:{motion_compensated} P:{prediction_matches} R:{reappearances}")
            
            # === GPS INFO ===
            if gps_data:
                speed_ms = gps_data.get('vehicle_speed_ms', 0)
                speed_kmh = gps_data.get('vehicle_speed_kmh', 0)
                info_lines.extend([
                    f"GPS: {gps_data.get('latitude', 0):.5f}",
                    f"     {gps_data.get('longitude', 0):.5f}",
                    f"Speed: {speed_kmh:.1f} km/h",
                    f"Heading: {gps_data.get('heading', 0):.1f}°"
                ])
            
            # === TRACK CONSOLIDATION INFO ===
            consolidations = frame_info.get('track_consolidations', 0)
            reappearances_total = frame_info.get('track_reappearances', 0)
            if consolidations > 0 or reappearances_total > 0:
                info_lines.append(f"Consolidations: {consolidations}")
                info_lines.append(f"Reappearances: {reappearances_total}")
            
            # === PERFORMANCE INFO ===
            processed_frames = frame_info.get('processed_frames', 0)
            total_frames = frame_info.get('total_frames', 1)
            efficiency = (processed_frames / total_frames * 100) if total_frames > 0 else 0
            info_lines.append(f"Efficiency: {efficiency:.1f}%")
            
            # Render text lines
            for i, line in enumerate(info_lines):
                y_pos = y_offset + i * 18
                
                # Use smaller font for detailed info
                current_font_scale = small_font_scale if i > 6 else font_scale
                
                # Color coding for different types of info
                if "Motion Pred:" in line:
                    color = (0, 255, 255)  # Cyan for motion
                elif "Visual Features:" in line:
                    color = (255, 0, 255)  # Magenta for visual
                elif "Enhanced:" in line:
                    color = (0, 255, 0)    # Green for enhanced detections
                elif "GPS:" in line or "Speed:" in line:
                    color = (255, 255, 0)  # Yellow for GPS
                elif "Consolidations:" in line or "Reappearances:" in line:
                    color = (255, 165, 0)  # Orange for consolidation
                else:
                    color = text_color      # White for basic info
                
                cv2.putText(result, line, 
                        (frame.shape[1] - panel_width + 5, y_pos),
                        font, current_font_scale, color, 1)
            
            # === DETECTION QUALITY INDICATORS ===
            # Show quality indicators at the bottom of panel
            if detections:
                best_detection = max(detections, key=lambda d: getattr(d, 'score', 0))
                best_score = getattr(best_detection, 'score', 0)
                
                # Show best detection score with color coding
                score_color = (0, 255, 0) if best_score > 0.7 else (0, 255, 255) if best_score > 0.5 else (0, 0, 255)
                cv2.putText(result, f"Best: {best_score:.3f}", 
                        (frame.shape[1] - panel_width + 5, panel_height - 10),
                        font, font_scale, score_color, 2)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error adding enhanced info panel: {e}")
            return frame

    def _draw_tracks(self, frame: np.ndarray, tracks: List[Track],
                            scale_x: float, scale_y: float) -> np.ndarray:
        """Enhanced track drawing with motion and visual feature indicators"""
        
        if frame is None or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        if tracks is None:
            tracks = []
            
        try:
            result = frame.copy()
            for track in tracks:
                # Determine track color based on enhanced attributes
                if getattr(track, 'motion_compensated', False):
                    color = (0, 255, 255)  # Cyan for motion compensated
                elif getattr(track, 'prediction_match', False):
                    color = (255, 0, 255)  # Magenta for prediction match
                elif getattr(track, 'reappearance_match', False):
                    color = (0, 165, 255)  # Orange for reappearance
                else:
                    # Use standard colors based on state
                    color = TRACK_COLORS.get(getattr(track, 'state', 'confirmed'), (255, 255, 255))
                
                # Get bounding box
                bbox = track.to_tlbr()
                x1, y1, x2, y2 = bbox
                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                
                # Draw bounding box with enhanced thickness for special tracks
                thickness = 4 if getattr(track, 'motion_compensated', False) else 3 if track.state == 'confirmed' else 2
                cv2.rectangle(result, (x1, y1), (x2, y2), color, thickness)
                
                # Enhanced track info
                track_info_parts = [f"ID:{track.track_id}"]
                
                if hasattr(track, 'hits'):
                    track_info_parts.append(f"H:{track.hits}")
                
                # Add enhancement indicators
                if getattr(track, 'motion_compensated', False):
                    track_info_parts.append("MC")
                if getattr(track, 'prediction_match', False):
                    match_score = getattr(track, 'match_score', 0)
                    track_info_parts.append(f"P:{match_score:.2f}")
                if getattr(track, 'reappearance_match', False):
                    track_info_parts.append("R")
                
                if track.state == 'confirmed':
                    track_info_parts.append("✓")
                
                track_info = " ".join(track_info_parts)
                
                # Create enhanced text background
                text_size = cv2.getTextSize(track_info, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                bg_color = color
                cv2.rectangle(result, (x1, y1 - text_size[1] - 8),
                            (x1 + text_size[0] + 4, y1), bg_color, -1)
                
                # Draw text with contrasting color
                text_color = (0, 0, 0) if sum(color) > 400 else (255, 255, 255)
                cv2.putText(result, track_info, (x1 + 2, y1 - 4),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2)
                
                # Draw enhanced trajectory for confirmed tracks
                if (track.state == 'confirmed' and 
                    len(getattr(track, 'detections', [])) > 1 and
                    hasattr(track, 'detections')):
                    self._draw_trajectory_enhanced(result, track, scale_x, scale_y, color)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error drawing enhanced tracks: {e}")
            return frame

    def _draw_trajectory(self, frame: np.ndarray, track: Track,
                                scale_x: float, scale_y: float, color: Tuple[int, int, int]):
        """Draw enhanced trajectory with motion prediction indicators"""
        try:
            recent_detections = track.detections[-min(10, len(track.detections)):]
            
            if len(recent_detections) < 2:
                return
            
            points = []
            for detection in recent_detections:
                center = detection.center
                scaled_center = (int(center[0] * scale_x), int(center[1] * scale_y))
                points.append(scaled_center)
            
            # Draw trajectory lines with varying thickness
            for i in range(1, len(points)):
                # Thicker line for more recent trajectory
                thickness = max(1, 3 - i//3)
                cv2.line(frame, points[i-1], points[i], color, thickness)
            
            # Draw trajectory points with size indicating recency
            for i, point in enumerate(points):
                radius = 4 if i == len(points) - 1 else max(2, 3 - i//3)
                cv2.circle(frame, point, radius, color, -1)
                
                # Add prediction indicator for the latest point
                if (i == len(points) - 1 and 
                    getattr(track, 'prediction_match', False)):
                    # Draw prediction indicator
                    cv2.circle(frame, point, radius + 3, (255, 255, 255), 1)
                    
        except Exception as e:
            self.logger.error(f"Error drawing enhanced trajectory: {e}")

    def visualize_frame(self, frame: np.ndarray, 
                    detections: List[Detection],
                    tracks: List[Track],
                    gps_data: Optional[Dict] = None,
                    frame_info: Optional[Dict] = None) -> bool:
        """
        Visualize a single frame with detections and tracks
        
        Args:
            frame: Input frame
            detections: Raw detections for this frame
            tracks: Active tracks
            gps_data: Optional GPS data
            frame_info: Optional frame information
            
        Returns:
            False if user wants to quit, True otherwise
        """
        self.frame_count += 1
        
        # Input validation - use blank frame if input is invalid
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            self.logger.warning(f"Invalid frame received (type: {type(frame)}, frame_count: {self.frame_count})")
            vis_frame = self.blank_frame.copy()
        else:
            # Create visualization with error handling
            try:
                vis_frame = self._create_visualization(frame, detections, tracks, gps_data, frame_info)
                if vis_frame is None:
                    self.logger.warning("Visualization failed, using blank frame")
                    vis_frame = self.blank_frame.copy()
            except Exception as e:
                self.logger.error(f"Visualization error: {e}")
                vis_frame = self.blank_frame.copy()
                cv2.putText(vis_frame, f"Visualization error: {str(e)[:50]}", 
                           (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Calculate FPS
        current_time = time.time()
        fps = 1.0 / max(0.001, current_time - self.last_time)
        self.fps_history.append(fps)
        self.last_time = current_time
        
        # Keep only recent FPS values
        if len(self.fps_history) > 30:
            self.fps_history = self.fps_history[-30:]
        
        # Add FPS overlay
        avg_fps = np.mean(self.fps_history)
        cv2.putText(vis_frame, f"FPS: {avg_fps:.1f}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Display frame
        cv2.imshow(self.window_name, vis_frame)
        
        # Handle keyboard input
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            return False  # Quit
        elif key == ord('p'):
            # Pause - wait for another key press
            cv2.putText(vis_frame, "PAUSED - Press any key to continue", 
                       (vis_frame.shape[1]//4, vis_frame.shape[0]//2), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            cv2.imshow(self.window_name, vis_frame)
            self.logger.info("⏸️  Paused - Press any key to continue...")
            cv2.waitKey(0)
        elif key == ord('s'):
            # Save screenshot
            screenshot_name = f"argus_track_screenshot_{self.frame_count:06d}.jpg"
            cv2.imwrite(screenshot_name, vis_frame)
            self.logger.info(f"📸 Screenshot saved: {screenshot_name}")
        
        return True  # Continue
    
    def _create_visualization(self, frame: np.ndarray,
                             detections: List[Detection],
                             tracks: List[Track],
                             gps_data: Optional[Dict] = None,
                             frame_info: Optional[Dict] = None) -> np.ndarray:
        """Create comprehensive visualization frame"""
        # Safety check for frame
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        # Resize frame to display size if needed
        try:
            vis_frame = self._resize_frame(frame)
        except Exception as e:
            self.logger.error(f"Error resizing frame: {e}")
            return self.blank_frame.copy()
        
        # Calculate scale factors for coordinate adjustment
        scale_x = vis_frame.shape[1] / max(1, frame.shape[1])
        scale_y = vis_frame.shape[0] / max(1, frame.shape[0])
        
        # Draw raw detections first (lighter overlay)
        vis_frame = self._draw_detections(vis_frame, detections, scale_x, scale_y)
        
        # Draw tracks (more prominent)
        vis_frame = self._draw_tracks(vis_frame, tracks, scale_x, scale_y)
        
        # Add information panels
        if self.show_info_panel:
            vis_frame = self._add_info_panel(vis_frame, detections, tracks, gps_data, frame_info)
        
        return vis_frame
    
    def _resize_frame(self, frame: np.ndarray) -> np.ndarray:
        """Resize frame to display size - DEFENSIVE"""
        # Handle None or invalid frame
        if frame is None:
            return self.blank_frame.copy()
        
        if len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        if frame.shape[:2] == (self.display_size[1], self.display_size[0]):
            return frame.copy()
        
        try:
            return cv2.resize(frame, self.display_size)
        except Exception as e:
            self.logger.error(f"Error resizing frame: {e}")
            return self.blank_frame.copy()

    def _draw_detections(self, frame: np.ndarray, detections: List[Detection],
                        scale_x: float, scale_y: float) -> np.ndarray:
        """Draw raw detections with semi-transparent overlay"""
        if frame is None or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        # Safety check - if frame is valid but detections is None
        if detections is None:
            detections = []
            
        # Create overlay for semi-transparency
        try:
            overlay = frame.copy()
            for detection in detections:
                bbox = detection.bbox
                x1, y1, x2, y2 = bbox
                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 255, 255), 1)
                conf_text = f"{detection.score:.2f}"
                cv2.putText(overlay, conf_text, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # Blend overlay with original frame
            result = cv2.addWeighted(frame, 0.8, overlay, 0.2, 0)
            return result
        except Exception as e:
            self.logger.error(f"Error drawing detections: {e}")
            return frame  # Return original frame if drawing fails

    def close(self):
        """Close the visualization window"""
        try:
            cv2.destroyWindow(self.window_name)
            self.logger.info(f"🖥️  Closed visualization window")
            
            # Print final statistics
            if self.fps_history:
                avg_fps = np.mean(self.fps_history)
                self.logger.info(f"📊 Average FPS: {avg_fps:.1f}")
                self.logger.info(f"📊 Total frames processed: {self.frame_count}")
        except Exception as e:
            self.logger.error(f"Error closing visualization window: {e}")


def draw_tracks(frame: np.ndarray, tracks: List[Track], 
                show_trajectory: bool = True,
                show_id: bool = True,
                show_state: bool = True) -> np.ndarray:
    """
    Draw tracks on frame (existing function - now enhanced with error handling)
    
    Args:
        frame: Input frame
        tracks: List of tracks to draw
        show_trajectory: Whether to show track trajectories
        show_id: Whether to show track IDs
        show_state: Whether to show track states
        
    Returns:
        Frame with track visualizations
    """
    # Handle None inputs
    if frame is None:
        # Create blank frame
        blank_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
        cv2.putText(blank_frame, "No frame data available", (400, 360), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        return blank_frame
    
    if tracks is None:
        tracks = []

    try:
        vis_frame = frame.copy()
        
        for track in tracks:
            # Get color based on state
            color = TRACK_COLORS.get(track.state, (255, 255, 255))
            
            # Draw bounding box
            x1, y1, x2, y2 = track.to_tlbr().astype(int)
            thickness = 3 if track.state == 'confirmed' else 2
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)
            
            # Draw track information
            if show_id or show_state:
                label_parts = []
                if show_id:
                    label_parts.append(f"ID: {track.track_id}")
                if show_state:
                    label_parts.append(f"[{track.state}]")
                
                label = " ".join(label_parts)
                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                
                # Draw label background
                cv2.rectangle(vis_frame, 
                             (x1, y1 - label_size[1] - 10),
                             (x1 + label_size[0], y1),
                             color, -1)
                
                # Draw text
                cv2.putText(vis_frame, label, (x1, y1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # Draw trajectory for confirmed tracks
            if show_trajectory and track.state == 'confirmed' and len(track.detections) > 1:
                points = []
                for det in track.detections[-10:]:  # Last 10 detections
                    center = det.center
                    points.append(center.astype(int))
                
                points = np.array(points)
                cv2.polylines(vis_frame, [points], False, color, 2)
                
                # Draw points
                for point in points:
                    cv2.circle(vis_frame, tuple(point), 3, color, -1)
        
        return vis_frame
    except Exception as e:
        # In case of error, log and return original frame
        logging.error(f"Error in draw_tracks: {e}")
        return frame

def create_track_overlay(frame: np.ndarray, tracks: List[Track],
                        alpha: float = 0.3) -> np.ndarray:
    """
    Create semi-transparent overlay with track information
    
    Args:
        frame: Input frame
        tracks: List of tracks
        alpha: Transparency level (0-1)
        
    Returns:
        Frame with overlay
    """
    # Handle None inputs
    if frame is None:
        blank_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
        return blank_frame
    
    if tracks is None or len(tracks) == 0:
        return frame.copy()
    
    try:
        overlay = np.zeros_like(frame)
        
        for track in tracks:
            if track.state != 'confirmed':
                continue
                
            # Create mask for track region
            mask = np.zeros(frame.shape[:2], dtype=np.uint8)
            x1, y1, x2, y2 = track.to_tlbr().astype(int)
            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)
            
            # Apply color overlay
            color = TRACK_COLORS[track.state]
            overlay[mask > 0] = color
        
        # Blend with original frame
        result = cv2.addWeighted(frame, 1 - alpha, overlay, alpha, 0)
        
        return result
    except Exception as e:
        logging.error(f"Error in create_track_overlay: {e}")
        return frame.copy()  # Return original frame if error

================
File: argus_track/trackers/lightpost_tracker.py
================
# argus_track/trackers/lightpost_tracker_v2.py

"""Enhanced Light Post Tracker with Motion Compensation"""

import json
import time
import logging
import numpy as np
import cv2
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

from ..config import TrackerConfig
from ..core import Detection, Track, GPSData
from ultralytics import YOLO
from ..utils.static_car_detector import StaticCarDetector, StaticCarConfig
from ..utils.visualization import draw_tracks, RealTimeVisualizer
from ..utils.io import save_tracking_results, load_gps_data
from ..utils.gps_utils import compute_average_location, GeoLocation
from ..utils.gps_sync_tracker import GPSSynchronizer
from ..utils.overlap_fixer import OverlapFixer

class EnhancedLightPostTracker:
    """
    Enhanced Light Post Tracker with Motion Compensation for Moving Cameras
    """
    
    def __init__(self, 
                 config: TrackerConfig,
                 model_path: str,
                 show_realtime: bool = False,
                 display_size: Tuple[int, int] = (1280, 720),
                 auto_adjust_motion: bool = True):
        """
        Initialize motion-aware light post tracker
        
        Args:
            config: Motion-aware tracker configuration
            detector: Object detection module
            show_realtime: Whether to show real-time visualization
            display_size: Size of real-time display
            auto_adjust_motion: Automatically adjust parameters based on motion
        """
        self.config = config
        # self.detector = detector
        self.show_realtime = show_realtime
        self.display_size = display_size
        self.auto_adjust_motion = auto_adjust_motion

        self.overlap_fixer = OverlapFixer(overlap_threshold=0.9, distance_threshold=1.0)

        # Initialize YOLO model with tracking
        self.model = YOLO(model_path)
        
        # Logging
        self.logger = logging.getLogger(f"{__name__}.MotionAwareLightPostTracker")
        
        # Real-time visualization
        self.visualizer = None
        if show_realtime:
            self.visualizer = RealTimeVisualizer(
                window_name="Motion-Aware Light Post Tracking",
                display_size=display_size,
                show_info_panel=True
            )
        
        # GPS and motion tracking
        self.gps_synchronizer: Optional[GPSSynchronizer] = None
        self.gps_tracks: Dict[int, List[GPSData]] = {}
        self.track_locations: Dict[int, GeoLocation] = {}
        
        # Motion analysis
        self.motion_history = []
        self.adaptive_config = config
        self.motion_adjustment_frequency = 60  # Adjust every 60 frames
        
        # Performance monitoring
        self.processing_times = []
        self.motion_compensation_times = []
        self.frame_count = 0
        
        self.logger.info("Initialized Motion-Aware Light Post Tracker")
        self.logger.info(f"Auto motion adjustment: {auto_adjust_motion}")

        # Import Kalman deduplicator
        from ..utils.kalman_gps_filter import create_kalman_gps_deduplicator
        self.kalman_deduplicator = create_kalman_gps_deduplicator(merge_distance_m=3.0)
        self.logger.info("Kalman GPS deduplication enabled (3m threshold)")

        self.static_car_detector = None
        if hasattr(config, 'enable_static_car_detection') and config.enable_static_car_detection:
            from ..utils.static_car_detector import StaticCarDetector, StaticCarConfig
            static_config = StaticCarConfig(
                movement_threshold_meters=getattr(config, 'static_movement_threshold_m', 2.0),
                stationary_time_threshold=getattr(config, 'static_time_threshold_s', 10.0),
                gps_frame_interval=6
            )
            self.static_car_detector = StaticCarDetector(static_config)
            self.logger.info("Static car detection enabled")
        else:
            self.logger.info("Static car detection disabled")

        self._analyze_tracking_issues()

    def process_video(self, 
                     video_path: str,
                     gps_data: Optional[List[GPSData]] = None,
                     output_path: Optional[str] = None,
                     save_results: bool = True,
                     resolution_scale: float = 1.0) -> Dict[int, List[Dict]]:
        """
        Process video with motion-aware tracking
        
        Args:
            video_path: Path to input video
            gps_data: Optional GPS data
            output_path: Optional output video path
            save_results: Whether to save results
            resolution_scale: Resolution scaling factor
            
        Returns:
            Dictionary of tracked objects
        """
        self.logger.info(f"Processing video with motion-aware tracking: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            error_msg = f"Could not open video: {video_path}"
            self.logger.error(error_msg)
            raise IOError(error_msg)
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        self.logger.info(f"Video: {frame_count} frames, {fps} FPS, {width}x{height}")
        
        # Initialize GPS synchronizer if available
        if gps_data:
            self.gps_synchronizer = GPSSynchronizer(gps_data, fps, gps_fps=10.0)
            sync_stats = self.gps_synchronizer.get_processing_statistics()
            self.logger.info(f"GPS sync: {sync_stats['sync_frames']} frames to process")
        
        # Setup video writer
        out_writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # Process frames
        all_tracks = {}
        current_frame_idx = 0
        processed_frames = 0
        motion_estimates = []
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Check if frame should be processed (GPS sync or all frames)
                should_process = True
                current_gps = None
                
                if self.gps_synchronizer:
                    should_process = self.gps_synchronizer.should_process_frame(current_frame_idx)
                    if should_process:
                        current_gps = self.gps_synchronizer.get_gps_for_frame(current_frame_idx)
                        
                        # Additional check: Static car detection
                        if self.static_car_detector and current_gps:
                            should_process = self.static_car_detector.should_process_frame(current_gps, current_frame_idx)
                            if not should_process:
                                self.logger.debug(f"Frame {current_frame_idx}: Skipped due to stationary car")

                if not should_process:
                    current_frame_idx += 1
                    continue
                
                # Apply resolution scaling
                if resolution_scale < 1.0:
                    scaled_width = int(frame.shape[1] * resolution_scale)
                    scaled_height = int(frame.shape[0] * resolution_scale)
                    frame = cv2.resize(frame, (scaled_width, scaled_height))
                
                # Start timing
                start_time = time.time()
                
                track_params = self.config.get_ultralytics_track_params()
                results = self.model.track(frame, **track_params)

                # Convert Ultralytics results to our format
                detections = []
                tracks = []
               
                # APPLY OVERLAP FIXING - CORRECTED METHOD NAME
                if results[0].boxes is not None and hasattr(results[0].boxes, 'id') and results[0].boxes.id is not None:
                    # Fix overlapping boxes and consolidate IDs
                    fixed_results = self.overlap_fixer.fix_tracking_results(
                        results, current_gps, current_frame_idx
                    )
                    
                    # Process fixed results
                    for fixed_detection in fixed_results:
                        # Create Detection object
                        detection = Detection(
                            bbox=fixed_detection['bbox'],
                            score=fixed_detection['score'],
                            class_id=fixed_detection['class_id'],
                            frame_id=current_frame_idx
                        )
                        detections.append(detection)
                        
                        # Use consolidated track ID
                        track_id = fixed_detection['track_id']
                        
                        # Create Track object for visualization
                        track = Track(
                            track_id=track_id,
                            detections=[detection],
                            state='confirmed'
                        )
                        tracks.append(track)
                        
                        # Calculate depth using lightpost height (4 meters)
                        bbox_height = detection.bbox[3] - detection.bbox[1]
                        focal_length = 1400
                        lightpost_height = 4.0
                        estimated_depth = (lightpost_height * focal_length) / bbox_height
                        
                        # Store depth information
                        if not hasattr(self, 'track_depths'):
                            self.track_depths = {}
                        if track_id not in self.track_depths:
                            self.track_depths[track_id] = []
                        self.track_depths[track_id].append(estimated_depth)
                        
                        # Convert to GPS coordinates if we have GPS data
                        if current_gps:
                            bbox_center_x = (detection.bbox[0] + detection.bbox[2]) / 2
                            image_width = frame.shape[1]
                            
                            pixels_from_center = bbox_center_x - (image_width / 2)
                            degrees_per_pixel = 60.0 / image_width
                            bearing_offset = pixels_from_center * degrees_per_pixel
                            object_bearing = current_gps.heading + bearing_offset
                            
                            import math
                            lat_offset = (estimated_depth * math.cos(math.radians(object_bearing))) / 111000
                            lon_offset = (estimated_depth * math.sin(math.radians(object_bearing))) / (111000 * math.cos(math.radians(current_gps.latitude)))
                            
                            object_lat = current_gps.latitude + lat_offset
                            object_lon = current_gps.longitude + lon_offset
                            
                            # Store GPS location with CONSOLIDATED track ID
                            if not hasattr(self, 'track_gps_locations'):
                                self.track_gps_locations = {}
                            if track_id not in self.track_gps_locations:
                                self.track_gps_locations[track_id] = []
                            
                            location_data = {
                                'latitude': object_lat,
                                'longitude': object_lon,
                                'depth': estimated_depth,
                                'bearing': object_bearing,
                                'frame': current_frame_idx,
                                'confidence': fixed_detection['score'],
                                'class_id': fixed_detection['class_id'],
                                'original_track_id': fixed_detection.get('original_track_id', track_id)  # Keep original for debugging
                            }
                            self.track_gps_locations[track_id].append(location_data)
                            
                            # Log with both original and consolidated IDs
                            orig_id = fixed_detection.get('original_track_id', track_id)
                            id_msg = f" (was {orig_id})" if orig_id != track_id else ""
                            print(f"Track {track_id}{id_msg}: GPS ({object_lat:.6f}, {object_lon:.6f}) depth {estimated_depth:.1f}m")

                # Get motion statistics
                motion_stats = {
                    'motion_detected': False,
                    'avg_translation': 0,
                    'total_tracks': len(tracks) if tracks else 0
                }
                
                # Auto-adjust configuration based on motion
                if (self.auto_adjust_motion and 
                    processed_frames % self.motion_adjustment_frequency == 0 and
                    motion_estimates):
                    
                    avg_motion = np.mean(motion_estimates[-self.motion_adjustment_frequency:])
                    
                    self.logger.info(f"Adjusted config for motion level: {avg_motion:.1f}px/frame")
                
                # Update GPS tracks
                if current_gps:
                    self._update_gps_tracks(tracks, current_gps, current_frame_idx)
                
                # Store track data
                for track in tracks:
                    if track.track_id not in all_tracks:
                        all_tracks[track.track_id] = []
                    
                    track_data = {
                        'frame': current_frame_idx,
                        'bbox': track.to_tlbr().tolist(),
                        'score': track.detections[-1].score if track.detections else 0,
                        'state': track.state,
                        'hits': track.hits,
                        'has_gps': current_gps is not None,
                        'motion_compensated': motion_stats.get('motion_detected', False)
                    }
                    all_tracks[track.track_id].append(track_data)
                
                # Real-time visualization
                if self.show_realtime and self.visualizer:
                    frame_info = {
                        'frame_idx': current_frame_idx,
                        'total_frames': frame_count,
                        'fps': fps,
                        'processed_frames': processed_frames,
                        'motion_detected': motion_stats.get('motion_detected', False),
                        'avg_motion': motion_stats.get('avg_translation', 0),
                        'config_adjusted': hasattr(self, 'adaptive_config')
                    }
                    
                    gps_info = None
                    if current_gps:
                        gps_info = {
                            'latitude': current_gps.latitude,
                            'longitude': current_gps.longitude,
                            'heading': current_gps.heading
                        }
                    
                    should_continue = self.visualizer.visualize_frame(
                        frame, detections, tracks, gps_info, frame_info
                    )
                    
                    if not should_continue:
                        self.logger.info("User requested quit")
                        break
                
                # Save to output video
                if out_writer:
                    vis_frame = self._create_motion_aware_visualization(
                        frame, tracks, motion_stats, current_gps
                    )
                    
                    if resolution_scale < 1.0:
                        vis_frame = cv2.resize(vis_frame, (width, height))
                    
                    out_writer.write(vis_frame)
                
                # Performance monitoring
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                
                # Progress logging
                processed_frames += 1
                if processed_frames % 100 == 0:
                    avg_time = np.mean(self.processing_times[-50:])
                    avg_motion_time = np.mean(self.motion_compensation_times[-50:])
                    
                    self.logger.info(
                        f"Processed {processed_frames} frames - "
                        f"Avg: {avg_time*1000:.1f}ms/frame, "
                        f"Motion: {avg_motion_time*1000:.1f}ms"
                    )
                    
                    if motion_estimates:
                        recent_motion = np.mean(motion_estimates[-50:])
                        self.logger.info(f"Recent motion: {recent_motion:.1f}px/frame")
                
                current_frame_idx += 1
                
        except KeyboardInterrupt:
            self.logger.info("Processing interrupted by user")
        except Exception as e:
            self.logger.error(f"Error processing video: {e}")
            raise
        finally:
            cap.release()
            if out_writer:
                out_writer.release()
            if self.visualizer:
                self.visualizer.close()
            cv2.destroyAllWindows()
        
        # Calculate geolocations with motion compensation
        if self.gps_synchronizer and processed_frames > 0:
            self.logger.info("Calculating motion-compensated geolocations...")
            self._calculate_motion_aware_geolocations(all_tracks)
        
        # Performance summary
        if self.processing_times:
            avg_processing = np.mean(self.processing_times) * 1000
            avg_motion_comp = np.mean(self.motion_compensation_times) * 1000
            effective_fps = 1.0 / np.mean(self.processing_times)
            
            self.logger.info("=== MOTION-AWARE PROCESSING SUMMARY ===")
            self.logger.info(f"Processed frames: {processed_frames}")
            self.logger.info(f"Processing FPS: {effective_fps:.1f}")
            self.logger.info(f"Avg frame time: {avg_processing:.1f}ms")
            self.logger.info(f"Motion compensation: {avg_motion_comp:.1f}ms")
            
            if motion_estimates:
                avg_motion = np.mean(motion_estimates)
                max_motion = np.max(motion_estimates)
                self.logger.info(f"Average motion: {avg_motion:.1f}px/frame")
                self.logger.info(f"Maximum motion: {max_motion:.1f}px/frame")

        # Static car detection summary - ADD THIS BLOCK
        if self.static_car_detector:
            static_stats = self.static_car_detector.get_statistics()
            self.logger.info("=== STATIC CAR DETECTION SUMMARY ===")
            self.logger.info(f"Frames skipped due to stationary car: {static_stats['skipped_frames']}")
            self.logger.info(f"Stationary periods detected: {static_stats['stationary_periods_count']}")
            if static_stats['stationary_periods_count'] > 0:
                self.logger.info(f"Total stationary time: {static_stats['total_stationary_time']:.1f}s")
                self.logger.info(f"Avg stationary duration: {static_stats['avg_stationary_duration']:.1f}s")
            self.logger.info(f"Processing efficiency gain: {static_stats['efficiency_gain']}")

        # Save results
        if save_results:
            self._save_motion_aware_results(all_tracks, video_path, motion_estimates)
        
        return all_tracks
    
    def _create_motion_aware_visualization(self, 
                                          frame: np.ndarray,
                                          tracks: List[Track],
                                          motion_stats: Dict,
                                          gps_data: Optional[GPSData]) -> np.ndarray:
        """Create visualization with motion information"""
        
        # Draw basic tracks
        vis_frame = draw_tracks(frame, tracks, show_trajectory=True)
        
        # Add motion information overlay
        info_lines = [
            f"Tracks: {len(tracks)}",
            f"Motion: {motion_stats.get('avg_translation', 0):.1f}px",
            f"Config: {'Adaptive' if self.auto_adjust_motion else 'Fixed'}",
        ]
        
        if gps_data:
            info_lines.extend([
                f"GPS: {gps_data.latitude:.5f}",
                f"     {gps_data.longitude:.5f}"
            ])
        
        # Add text overlay
        y_offset = 30
        for line in info_lines:
            cv2.putText(vis_frame, line, (10, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            y_offset += 25
        
        # Add motion indicator
        if motion_stats.get('motion_detected', False):
            cv2.putText(vis_frame, "MOTION DETECTED", (10, vis_frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        return vis_frame
    
    def _update_gps_tracks(self, tracks: List[Track], gps_data: GPSData, frame_idx: int):
        """Update GPS data for active tracks"""
        for track in tracks:
            if track.track_id not in self.gps_tracks:
                self.gps_tracks[track.track_id] = []
            self.gps_tracks[track.track_id].append(gps_data)
    
    def _calculate_motion_aware_geolocations(self, all_tracks: Dict[int, List[Dict]]):
        """Calculate geolocations considering camera motion"""
        
        for track_id, track_history in all_tracks.items():
            # Filter for well-tracked objects
            if len(track_history) < self.config.min_static_frames:
                continue
            
            # Check GPS availability
            gps_frames = [t for t in track_history if t.get('has_gps', False)]
            if len(gps_frames) < self.config.min_gps_points:
                continue
            
            # Check if object is motion-compensated static
            if not self._is_motion_compensated_static(track_history):
                continue
            
            # Calculate geolocation with motion awareness
            geolocation = self._estimate_motion_aware_geolocation(track_id, track_history)
            
            if geolocation and geolocation.reliability > 0.2:
                self.track_locations[track_id] = geolocation
                self.logger.debug(
                    f"Track {track_id} geolocated (motion-aware): "
                    f"({geolocation.latitude:.6f}, {geolocation.longitude:.6f}) "
                    f"reliability: {geolocation.reliability:.2f}"
                )
    
    def _is_motion_compensated_static(self, track_history: List[Dict]) -> bool:
        """Check if object is static considering motion compensation"""
        
        # If track has motion compensation applied, use relaxed criteria
        motion_compensated_frames = [t for t in track_history if t.get('motion_compensated', False)]
        
        if len(motion_compensated_frames) > len(track_history) * 0.5:
            # Majority of frames were motion compensated
            # Use more lenient static criteria
            return len(track_history) >= self.config.min_static_frames
        else:
            # Use standard static analysis
            if len(track_history) < 5:
                return False
            
            centers = []
            for detection in track_history:
                bbox = detection['bbox']
                center_x = (bbox[0] + bbox[2]) / 2
                center_y = (bbox[1] + bbox[3]) / 2
                centers.append([center_x, center_y])
            
            centers = np.array(centers)
            std_movement = np.std(centers, axis=0)
            max_movement = np.max(std_movement)
            
            # Use motion-adjusted threshold
            return max_movement < self.adaptive_config.static_threshold
    
    def _estimate_motion_aware_geolocation(self, 
                                          track_id: int,
                                          track_history: List[Dict]) -> Optional[GeoLocation]:
        """Estimate geolocation with motion compensation awareness"""
        
        if track_id not in self.gps_tracks or len(self.gps_tracks[track_id]) < 2:
            return None
        
        gps_points = self.gps_tracks[track_id]
        
        # Use multiple GPS points for better accuracy in motion scenarios
        estimated_positions = []
        
        for i, gps_point in enumerate(gps_points[::2]):  # Use every other GPS point
            # Find corresponding detection
            detection = None
            for hist_entry in track_history:
                if hist_entry.get('has_gps', False):
                    detection = hist_entry
                    break
            
            if detection is None:
                continue
            
            bbox = detection['bbox']
            
            # Enhanced distance estimation for motion scenarios
            distance = self._estimate_object_distance_motion_aware(bbox, gps_point)
            
            # Calculate object offset with motion compensation
            lateral_offset, forward_offset = self._calculate_object_offset_motion_aware(
                bbox, distance, gps_point
            )
            
            # Convert to GPS coordinates
            obj_lat, obj_lon = self._gps_offset_to_coordinates(
                gps_point, lateral_offset, forward_offset
            )
            
            estimated_positions.append({
                'lat': obj_lat,
                'lon': obj_lon,
                'distance': distance,
                'confidence': detection['score'],
                'motion_compensated': detection.get('motion_compensated', False)
            })
        
        if not estimated_positions:
            return None
        
        # Calculate weighted average (favor motion-compensated positions)
        weights = []
        for pos in estimated_positions:
            weight = pos['confidence']
            if pos['motion_compensated']:
                weight *= 1.5  # Boost motion-compensated positions
            weights.append(weight)
        
        weights = np.array(weights)
        weights = weights / np.sum(weights)
        
        avg_lat = np.sum([pos['lat'] * w for pos, w in zip(estimated_positions, weights)])
        avg_lon = np.sum([pos['lon'] * w for pos, w in zip(estimated_positions, weights)])
        
        # Enhanced reliability calculation for motion scenarios
        lat_std = np.std([pos['lat'] for pos in estimated_positions])
        lon_std = np.std([pos['lon'] for pos in estimated_positions])
        position_std = np.sqrt(lat_std**2 + lon_std**2)
        
        # Motion compensation factor
        motion_comp_ratio = len([p for p in estimated_positions if p['motion_compensated']]) / len(estimated_positions)
        motion_bonus = motion_comp_ratio * 0.2
        
        reliability = (1.0 / (1.0 + position_std * 5000)) + motion_bonus
        reliability = min(1.0, reliability)
        
        # Accuracy estimation
        earth_radius = 6378137.0
        lat_error = lat_std * earth_radius * np.pi / 180
        lon_error = lon_std * earth_radius * np.pi / 180 * np.cos(np.radians(avg_lat))
        accuracy = np.sqrt(lat_error**2 + lon_error**2)
        
        return GeoLocation(
            latitude=avg_lat,
            longitude=avg_lon,
            accuracy=max(0.5, accuracy),
            reliability=reliability,
            timestamp=estimated_positions[-1]['confidence']  # Use last confidence as timestamp placeholder
        )
    
    def _estimate_object_distance_motion_aware(self, bbox: List[float], gps_data: GPSData) -> float:
        """Estimate distance with motion compensation"""
        # Enhanced distance estimation considering camera motion
        real_object_width = 0.3  # meters (LED light post width)
        bbox_width = bbox[2] - bbox[0]
        
        if bbox_width > 0:
            # Base distance calculation
            focal_length = 1400  # Approximate for GoPro
            base_distance = (real_object_width * focal_length) / bbox_width
            
            # Adjust for camera motion (moving camera sees objects differently)
            # This is a simplified adjustment - could be more sophisticated
            motion_factor = 1.0  # Could be based on GPS speed/heading changes
            
            adjusted_distance = base_distance * motion_factor
            return max(1.0, min(adjusted_distance, 200.0))
        
        return 50.0  # Default distance
    
    def get_track_statistics(self):
        """Get tracking statistics from the underlying tracker"""
        return {
            'total_tracks': len(self.track_locations) if hasattr(self, 'track_locations') else 0,
            'active_tracks': 0,  # Ultralytics handles this internally
            'lost_tracks': 0,    # Ultralytics handles this internally  
            'removed_tracks': 0, # Ultralytics handles this internally
            'frame_id': getattr(self, 'frame_count', 0)
        }

    def _calculate_object_offset_motion_aware(self, 
                                            bbox: List[float], 
                                            distance: float,
                                            gps_data: GPSData) -> Tuple[float, float]:
        """Calculate object offset with motion awareness"""
        center_x = (bbox[0] + bbox[2]) / 2
        center_y = (bbox[1] + bbox[3]) / 2
        
        # Camera parameters
        image_width = 1920  # Adjust based on actual resolution
        image_height = 1080
        
        dx_px = center_x - (image_width / 2)
        dy_px = center_y - (image_height / 2)
        
        # Angle calculation with motion compensation
        angle_per_pixel = 0.0005  # Calibrated value
        angle_x = dx_px * angle_per_pixel
        angle_y = dy_px * angle_per_pixel
        
        # Calculate offsets in camera coordinate system
        lateral_offset = distance * np.sin(angle_x)
        forward_offset = distance * np.cos(angle_x)
        
        return lateral_offset, forward_offset
    
    def _gps_offset_to_coordinates(self, 
                                  base_gps: GPSData,
                                  lateral_offset: float,
                                  forward_offset: float) -> Tuple[float, float]:
        """Convert offsets to GPS coordinates"""
        R = 6378137.0  # Earth radius
        heading_rad = np.radians(base_gps.heading)
        
        # Rotate offsets to world coordinates
        east_offset = lateral_offset * np.cos(heading_rad) + forward_offset * np.sin(heading_rad)
        north_offset = -lateral_offset * np.sin(heading_rad) + forward_offset * np.cos(heading_rad)
        
        # Convert to GPS
        lat_offset = north_offset / R * 180 / np.pi
        lon_offset = east_offset / (R * np.cos(np.radians(base_gps.latitude))) * 180 / np.pi
        
        return base_gps.latitude + lat_offset, base_gps.longitude + lon_offset
    
    def _save_motion_aware_results(self, 
                                  all_tracks: Dict[int, List[Dict]],
                                  video_path: str,
                                  motion_estimates: List[float]):
        """Save results with motion awareness information"""
        
        results_path = Path(video_path).with_suffix('.json')
        geojson_path = Path(video_path).with_suffix('.geojson')

        # Enhanced metadata with motion information
        metadata = {
            'total_tracks': len(all_tracks),
            'geolocated_objects': len(self.track_locations),
            'auto_motion_adjustment': self.auto_adjust_motion,
            'motion_statistics': {
                'avg_motion_per_frame': np.mean(motion_estimates) if motion_estimates else 0,
                'max_motion_per_frame': np.max(motion_estimates) if motion_estimates else 0,
                'motion_frames': len(motion_estimates)
            },
            'config_used': self.adaptive_config.__dict__
        }
        
        # Save JSON results
        results = {
            'metadata': metadata,
            'tracks': all_tracks,
            'track_locations': {
                str(track_id): {
                    'latitude': loc.latitude,
                    'longitude': loc.longitude,
                    'accuracy': loc.accuracy,
                    'reliability': loc.reliability,
                    'motion_compensated': True
                }
                for track_id, loc in self.track_locations.items()
            }
        }
        
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Save GeoJSON
        self._export_motion_aware_geojson(geojson_path)
        
        self.logger.info(f"Saved motion-aware results to {results_path}")

    def _export_motion_aware_geojson(self, output_path: Path):
        """Export GeoJSON with Kalman-filtered GPS deduplication"""
        
        features = []
        
        # DEBUG: Analyze all tracks (your existing debug code)
        if hasattr(self, 'track_gps_locations'):
            self.logger.info("🔍 TRACK ANALYSIS DEBUG:")
            self.logger.info(f"   Total tracks with GPS data: {len(self.track_gps_locations)}")
            
            tracks_by_detection_count = {}
            for track_id, locations in self.track_gps_locations.items():
                detection_count = len(locations)
                if detection_count not in tracks_by_detection_count:
                    tracks_by_detection_count[detection_count] = 0
                tracks_by_detection_count[detection_count] += 1
                
                # Log individual track details
                if detection_count >= self.config.min_detections_for_export:
                    avg_lat = sum(loc['latitude'] for loc in locations) / len(locations)
                    avg_lon = sum(loc['longitude'] for loc in locations) / len(locations)
                    avg_confidence = sum(loc['confidence'] for loc in locations) / len(locations)
                    
                    status = "✅ EXPORTED" if detection_count >= self.config.min_detections_for_export else "❌ TOO FEW"
                    self.logger.info(f"   Track {track_id}: {detection_count} detections, "
                                f"avg_conf: {avg_confidence:.2f}, "
                                f"pos: ({avg_lat:.6f}, {avg_lon:.6f}) - {status}")
            
            # Summary by detection count
            self.logger.info("   Detection count summary:")
            for count in sorted(tracks_by_detection_count.keys()):
                self.logger.info(f"     {tracks_by_detection_count[count]} tracks with {count} detections")
            
            self.logger.info(f"   Current export threshold: >= {self.config.min_detections_for_export} detections")
            
            # Count how many would be exported with different thresholds
            thresholds_to_test = [1, 2, 3, 4, 5]
            for threshold in thresholds_to_test:
                count = len([t for t, locs in self.track_gps_locations.items() if len(locs) >= threshold])
                self.logger.info(f"     With threshold >= {threshold}: {count} tracks would be exported")

        # Export features (your existing export code)
        if hasattr(self, 'track_gps_locations'):
            for track_id, locations in self.track_gps_locations.items():
                if len(locations) >= self.config.min_detections_for_export:
                    # Average the GPS coordinates for this track
                    avg_lat = sum(loc['latitude'] for loc in locations) / len(locations)
                    avg_lon = sum(loc['longitude'] for loc in locations) / len(locations)
                    avg_depth = sum(loc['depth'] for loc in locations) / len(locations)
                    avg_confidence = sum(loc['confidence'] for loc in locations) / len(locations)
                    
                    feature = {
                        "type": "Feature", 
                        "geometry": {
                            "type": "Point",
                            "coordinates": [float(avg_lon), float(avg_lat)]
                        },
                        "properties": {
                            "track_id": int(track_id),
                            "confidence": round(float(avg_confidence), 3),
                            "estimated_distance_m": round(float(avg_depth), 1),
                            "detection_count": int(len(locations)),
                            "class_id": int(locations[0]['class_id']),
                            "processing_method": "ultralytics_tracking"
                        }
                    }
                    features.append(feature)
        
        # KALMAN DEDUPLICATION - NEW SECTION
        if features:
            # Apply Kalman GPS deduplication
            deduplicated_features = self.kalman_deduplicator.deduplicate_locations(features)
        else:
            deduplicated_features = features
        
        # Create GeoJSON with deduplication metadata
        geojson = {
            "type": "FeatureCollection",
            "features": deduplicated_features,
            "metadata": {
                "generator": "Argus Track with Kalman GPS Deduplication",
                "total_locations": len(deduplicated_features),
                "original_locations": len(features),
                "duplicates_removed": len(features) - len(deduplicated_features),
                "merge_distance_m": 3.0,
                "processing_method": "kalman_filtered_gps_deduplication"
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(geojson, f, indent=2)
        
        self.logger.info(f"Exported {len(deduplicated_features)} Kalman-filtered locations to {output_path}")

    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive tracking statistics"""
        return {
            'geolocated_objects': len(self.track_locations),
            'avg_reliability': np.mean([loc.reliability for loc in self.track_locations.values()]) if self.track_locations else 0,
            'config_adaptive': self.auto_adjust_motion
        }
    
    def _analyze_tracking_issues(self):
        """
        Analyze tracking data to identify common issues and patterns
        """
        if not hasattr(self, 'track_gps_locations') or not self.track_gps_locations:
            return
        
        self.logger.info("🔍 TRACKING ISSUE ANALYSIS:")
        
        # Issue 1: Short-lived tracks (potential fragmentation)
        short_tracks = [tid for tid, locs in self.track_gps_locations.items() if len(locs) <= 3]
        if short_tracks:
            self.logger.info(f"   📉 Short tracks (≤3 detections): {len(short_tracks)} tracks")
            self.logger.info(f"      Track IDs: {short_tracks[:10]}{'...' if len(short_tracks) > 10 else ''}")
        
        # Issue 2: Track GPS clustering analysis  
        track_positions = {}
        for track_id, locations in self.track_gps_locations.items():
            if len(locations) >= 2:
                avg_lat = sum(loc['latitude'] for loc in locations) / len(locations)
                avg_lon = sum(loc['longitude'] for loc in locations) / len(locations)
                track_positions[track_id] = (avg_lat, avg_lon)
        
        # Find tracks that are very close to each other (potential duplicates/fragments)
        close_track_pairs = []
        track_ids = list(track_positions.keys())
        
        for i, tid1 in enumerate(track_ids):
            for tid2 in track_ids[i+1:]:
                lat1, lon1 = track_positions[tid1]
                lat2, lon2 = track_positions[tid2]
                distance = self._calculate_gps_distance(lat1, lon1, lat2, lon2)
                
                if distance <= 5.0:  # Within 5 meters
                    close_track_pairs.append((tid1, tid2, distance))
        
        if close_track_pairs:
            self.logger.info(f"   🎯 Potentially related tracks (≤5m apart): {len(close_track_pairs)} pairs")
            for tid1, tid2, dist in close_track_pairs[:5]:  # Show first 5
                det1 = len(self.track_gps_locations[tid1])
                det2 = len(self.track_gps_locations[tid2])
                self.logger.info(f"      Tracks {tid1}({det1} det) ↔ {tid2}({det2} det): {dist:.1f}m apart")
        
        # Issue 3: Track detection count distribution
        detection_counts = [len(locs) for locs in self.track_gps_locations.values()]
        if detection_counts:
            avg_detections = sum(detection_counts) / len(detection_counts)
            max_detections = max(detection_counts)
            min_detections = min(detection_counts)
            
            self.logger.info(f"   📊 Detection count stats:")
            self.logger.info(f"      Average: {avg_detections:.1f}, Range: {min_detections}-{max_detections}")
            
            # Identify outliers (very high detection counts - possible stuck tracks)
            high_detection_tracks = [tid for tid, locs in self.track_gps_locations.items() 
                                if len(locs) > avg_detections * 3]
            if high_detection_tracks:
                self.logger.info(f"      High-detection tracks: {high_detection_tracks}")
        
        # Issue 4: Track ID gaps analysis (fragmentation indicator)
        all_track_ids = sorted(self.track_gps_locations.keys())
        if len(all_track_ids) > 1:
            max_id = max(all_track_ids)
            actual_tracks = len(all_track_ids)
            id_efficiency = actual_tracks / max_id if max_id > 0 else 1.0
            
            self.logger.info(f"   🔢 Track ID efficiency: {id_efficiency:.2f} ({actual_tracks}/{max_id})")
            if id_efficiency < 0.7:
                self.logger.info(f"      ⚠️  Low efficiency suggests track fragmentation")
        
        # Issue 5: Temporal analysis - look for time gaps
        track_time_spans = {}
        for track_id, locations in self.track_gps_locations.items():
            if len(locations) >= 2:
                frames = [loc['frame'] for loc in locations]
                frame_span = max(frames) - min(frames)
                frame_gaps = []
                
                sorted_frames = sorted(frames)
                for i in range(1, len(sorted_frames)):
                    gap = sorted_frames[i] - sorted_frames[i-1]
                    if gap > 10:  # Gap larger than expected
                        frame_gaps.append(gap)
                
                if frame_gaps:
                    track_time_spans[track_id] = max(frame_gaps)
        
        if track_time_spans:
            tracks_with_gaps = len([gap for gap in track_time_spans.values() if gap > 20])
            self.logger.info(f"   ⏱️  Tracks with temporal gaps (>20 frames): {tracks_with_gaps}")

    def _calculate_gps_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between GPS points in meters"""
        import numpy as np
        R = 6378137.0
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        a = (np.sin(dlat/2)**2 + 
            np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c
    
class OverlapFixer:
    """
    Fixes Ultralytics tracking issues in real-time:
    1. Removes overlapping bounding boxes in same frame
    2. Prevents multiple IDs for same object
    3. Consolidates fragmented track IDs
    """
    
    def __init__(self, overlap_threshold: float = 0.5, distance_threshold: float = 3.0):
        """
        Initialize overlap fixer
        
        Args:
            overlap_threshold: IoU threshold for detecting overlaps (0.5 = 50% overlap)
            distance_threshold: GPS distance threshold for same object (meters)
        """
        self.overlap_threshold = overlap_threshold
        self.distance_threshold = distance_threshold
        self.logger = logging.getLogger(f"{__name__}.OverlapFixer")
        
        # Track ID management
        self.id_mapping = {}  # original_id -> consolidated_id
        self.next_consolidated_id = 1
        self.track_positions = {}  # track_id -> recent GPS positions
        
    def fix_tracking_results(self, ultralytics_results, current_gps: Optional[GPSData], 
                           frame_id: int) -> List[Dict]:
        """
        Fix Ultralytics tracking results in real-time
        
        Args:
            ultralytics_results: Raw results from model.track()
            current_gps: Current GPS data
            frame_id: Current frame number
            
        Returns:
            Fixed list of detections with consolidated track IDs
        """
        if not ultralytics_results[0].boxes or ultralytics_results[0].boxes.id is None:
            return []
        
        # Extract raw detections
        raw_detections = self._extract_detections(ultralytics_results[0], frame_id)
        
        # Step 1: Remove overlapping bounding boxes in same frame
        non_overlapping = self._remove_overlapping_boxes(raw_detections)
        
        # Step 2: Consolidate track IDs (prevent multiple IDs for same object)
        consolidated = self._consolidate_track_ids(non_overlapping, current_gps)
        
        self.logger.debug(f"Frame {frame_id}: {len(raw_detections)} → {len(non_overlapping)} → {len(consolidated)} detections")
        
        return consolidated
    
    def _extract_detections(self, results, frame_id: int) -> List[Dict]:
        """Extract detections from Ultralytics results"""
        detections = []
        
        boxes = results.boxes.xyxy.cpu().numpy()
        scores = results.boxes.conf.cpu().numpy()
        classes = results.boxes.cls.cpu().numpy().astype(int)
        track_ids = results.boxes.id.cpu().numpy().astype(int)
        
        for i, (box, score, cls_id, track_id) in enumerate(zip(boxes, scores, classes, track_ids)):
            detections.append({
                'bbox': box,
                'score': score,
                'class_id': cls_id,
                'track_id': track_id,
                'frame': frame_id
            })
        
        return detections
    
    def _remove_overlapping_boxes(self, detections: List[Dict]) -> List[Dict]:
        """Remove overlapping bounding boxes in same frame"""
        if len(detections) <= 1:
            return detections
        
        # Calculate IoU matrix
        n = len(detections)
        keep_indices = list(range(n))
        
        for i in range(n):
            if i not in keep_indices:
                continue
                
            for j in range(i + 1, n):
                if j not in keep_indices:
                    continue
                
                # Calculate IoU
                iou = self._calculate_iou(detections[i]['bbox'], detections[j]['bbox'])
                
                if iou > self.overlap_threshold:
                    # Keep the detection with higher confidence
                    if detections[i]['score'] >= detections[j]['score']:
                        keep_indices.remove(j)
                        self.logger.debug(f"   Removed overlapping box: track {detections[j]['track_id']} "
                                        f"(IoU: {iou:.2f} with track {detections[i]['track_id']})")
                    else:
                        keep_indices.remove(i)
                        self.logger.debug(f"   Removed overlapping box: track {detections[i]['track_id']} "
                                        f"(IoU: {iou:.2f} with track {detections[j]['track_id']})")
                        break
        
        return [detections[i] for i in keep_indices]
    
    def _consolidate_track_ids(self, detections: List[Dict], current_gps: Optional[GPSData]) -> List[Dict]:
        """Consolidate track IDs to prevent multiple IDs for same object"""
        
        for detection in detections:
            original_id = detection['track_id']
            
            # Check if this is a new track that should be merged with existing
            consolidated_id = self._get_consolidated_id(detection, current_gps)
            
            # Update detection with consolidated ID
            detection['original_track_id'] = original_id
            detection['track_id'] = consolidated_id
            
            # Update track position history
            if current_gps and consolidated_id not in self.track_positions:
                self.track_positions[consolidated_id] = []
            
            if current_gps:
                # Calculate GPS position for this detection
                gps_pos = self._calculate_detection_gps(detection, current_gps)
                if gps_pos:
                    self.track_positions[consolidated_id].append({
                        'lat': gps_pos[0],
                        'lon': gps_pos[1],
                        'frame': detection['frame']
                    })
                    
                    # Keep only recent positions
                    if len(self.track_positions[consolidated_id]) > 10:
                        self.track_positions[consolidated_id] = self.track_positions[consolidated_id][-10:]
        
        return detections
    
    def _get_consolidated_id(self, detection: Dict, current_gps: Optional[GPSData]) -> int:
        """Get consolidated track ID for detection"""
        original_id = detection['track_id']
        
        # If we've seen this original ID before, return its mapping
        if original_id in self.id_mapping:
            return self.id_mapping[original_id]
        
        # Check if this detection is close to any existing tracks
        if current_gps:
            detection_gps = self._calculate_detection_gps(detection, current_gps)
            
            if detection_gps:
                # Find existing tracks within distance threshold
                for existing_id, positions in self.track_positions.items():
                    if not positions:
                        continue
                    
                    # Check distance to most recent position
                    recent_pos = positions[-1]
                    distance = self._gps_distance(
                        detection_gps[0], detection_gps[1],
                        recent_pos['lat'], recent_pos['lon']
                    )
                    
                    if distance <= self.distance_threshold:
                        # This detection is close to existing track - merge them
                        self.id_mapping[original_id] = existing_id
                        self.logger.info(f"   🔗 Merged track {original_id} into {existing_id} "
                                       f"(distance: {distance:.1f}m)")
                        return existing_id
        
        # This is a genuinely new track
        new_consolidated_id = self.next_consolidated_id
        self.id_mapping[original_id] = new_consolidated_id
        self.next_consolidated_id += 1
        
        return new_consolidated_id
    
    def _calculate_detection_gps(self, detection: Dict, gps: GPSData) -> Optional[Tuple[float, float]]:
        """Calculate GPS coordinates for detection"""
        try:
            bbox = detection['bbox']
            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            
            # Simple depth estimation
            bbox_height = bbox[3] - bbox[1]
            if bbox_height > 0:
                focal_length = 1400
                lightpost_height = 4.0
                estimated_depth = (lightpost_height * focal_length) / bbox_height
                
                # Convert to GPS offset
                image_width = 1920  # Assume standard resolution
                pixels_from_center = center_x - (image_width / 2)
                degrees_per_pixel = 60.0 / image_width
                bearing_offset = pixels_from_center * degrees_per_pixel
                object_bearing = gps.heading + bearing_offset
                
                import math
                lat_offset = (estimated_depth * math.cos(math.radians(object_bearing))) / 111000
                lon_offset = (estimated_depth * math.sin(math.radians(object_bearing))) / (111000 * math.cos(math.radians(gps.latitude)))
                
                object_lat = gps.latitude + lat_offset
                object_lon = gps.longitude + lon_offset
                
                return (object_lat, object_lon)
        except:
            pass
        
        return None
    
    def _calculate_iou(self, box1: np.ndarray, box2: np.ndarray) -> float:
        """Calculate IoU between two bounding boxes"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def _gps_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between GPS points in meters"""
        import numpy as np
        R = 6378137.0
        lat1_rad = np.radians(lat1)
        lat2_rad = np.radians(lat2)
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        
        a = (np.sin(dlat/2)**2 + 
             np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        
        return R * c

================
File: argus_track/__init__.py
================
# argus_track/__init__.py - FIXED imports

"""
Argus Track: Stereo ByteTrack Light Post Tracking System
========================================================

A specialized implementation of ByteTrack for tracking light posts in stereo video sequences
with GPS integration for precise 3D geolocation estimation.

Key Features:
- Stereo vision processing with 3D triangulation
- Optimized for static/slow-moving objects
- GPS data integration for geolocation
- YOLOv11 support for advanced object detection
- Modular architecture with clear separation of concerns
- Comprehensive logging and error handling
- Type hints and documentation throughout

Author: Argus Track Team
Date: 2025
License: MIT
"""

from argus_track.__version__ import __version__
from argus_track.config import TrackerConfig, StereoCalibrationConfig, DetectorConfig
from argus_track.core import Detection, Track, GPSData
from argus_track.core.stereo import StereoDetection, StereoFrame, StereoTrack
from argus_track.trackers import EnhancedLightPostTracker
from argus_track.detectors import ObjectDetector
from argus_track.detectors.yolov11 import YOLOv11Detector
from argus_track.stereo import StereoMatcher, StereoTriangulator, StereoCalibrationManager


__all__ = [
    "__version__",
    "TrackerConfig",
    "StereoCalibrationConfig", 
    "DetectorConfig",
    "Detection",
    "Track",
    "GPSData",
    "StereoDetection",
    "StereoFrame", 
    "StereoTrack",
    "EnhancedLightPostTracker",
    "YOLOv11Detector",
    "ObjectDetector",
    "StereoMatcher",
    "StereoTriangulator", 
    "StereoCalibrationManager"
]

================
File: argus_track/config.py
================
# argus_track/config.py (UPDATED WITH TRACK CONSOLIDATION)

"""Configuration classes for ArgusTrack with Track Consolidation"""

from dataclasses import dataclass, field
from typing import Optional, Dict, Any
import yaml
import json
import pickle
import numpy as np
from pathlib import Path

@dataclass
class TrackConsolidationConfig:
    """Configuration for track ID consolidation and reappearance handling"""
    
    # === REAPPEARANCE DETECTION ===
    max_gap_frames: int = 15                    # Max frames to remember lost tracks
    reappearance_iou_threshold: float = 0.3     # IoU threshold for reappearance
    reappearance_spatial_threshold: float = 80  # Pixel distance for reappearance
    
    # === DUPLICATE ID PREVENTION ===
    duplicate_spatial_threshold: float = 50     # Pixels apart to consider different objects
    duplicate_size_threshold: float = 0.7       # Size ratio threshold (±30%)
    duplicate_iou_threshold: float = 0.5        # IoU threshold for duplicate detection
    
    # === ID MANAGEMENT ===
    prefer_lower_id: bool = True                # Always keep lower track ID
    enable_id_consolidation: bool = True        # Enable real-time ID fixing
    enable_reappearance_detection: bool = True  # Enable gap-bridging
    
    # === MEMORY MANAGEMENT ===
    max_track_memory: int = 100                 # Max tracks to keep in memory
    cleanup_interval_frames: int = 30           # Clean old tracks every N frames
    
    # === DEBUGGING ===
    log_consolidations: bool = True             # Log ID consolidations
    log_reappearances: bool = True              # Log track reappearances

# Add performance optimization settings
@dataclass
class PerformanceConfig:
    """Performance optimization settings for 10fps processing"""
    target_fps: float = 10.0                   # Target processing speed
    max_processing_time_ms: float = 100.0      # Max time per frame
    enable_gpu_acceleration: bool = True       # Use GPU if available
    
    # Memory management
    max_memory_usage_mb: float = 2048.0        # Max memory usage
    cleanup_frequency: int = 100               # Cleanup every N frames
    
    # Feature extraction optimization
    visual_feature_decimation: int = 1         # Extract features every N detections
    motion_prediction_decimation: int = 1      # Predict motion every N frames
    
    # Multi-threading settings
    enable_parallel_processing: bool = False   # Parallel processing (experimental)
    num_worker_threads: int = 2                # Worker threads if enabled

@dataclass
class TrackerConfig:
    """Configuration for simplified light post tracker (monocular)"""
    
    # === DETECTION PARAMETERS ===
    detection_conf: float = 0.15               # Detection confidence threshold
    detection_iou: float = 0.8                 # NMS IoU threshold
    tracker_type: str = "bytetrack.yaml"       # Ultralytics tracker config
    max_detections: int = 50                   # Max detections per frame
    
    # === GPS SYNCHRONIZATION ===
    gps_frame_interval: int = 6                # Process every 6th frame for GPS sync
    enable_gps_extraction: bool = True         # Auto-extract GPS from video metadata
    
    # === TRACK CONSOLIDATION ===
    track_consolidation: TrackConsolidationConfig = field(
        default_factory=TrackConsolidationConfig
    )
    
    # === OUTPUT SETTINGS ===
    export_json: bool = True                   # Export JSON frame data
    export_csv: bool = True                    # Export CSV GPS data
    min_detections_for_export: int = 2         # Minimum detections to include in output
    
    # === STATIC CAR DETECTION ===
    enable_static_car_detection: bool = True   # Enable static car frame skipping
    static_movement_threshold_m: float = 0.9   # Minimum movement to consider moving
    static_time_threshold_s: float = 5.0       # Time before starting to skip frames
    
    # === REAL-TIME DISPLAY ===
    display_gps_frames_only: bool = True       # Only display GPS-synchronized frames
    display_track_history: bool = True         # Show track trajectories
    display_consolidation_info: bool = True    # Show ID consolidation info
    performance: PerformanceConfig = field(default_factory=PerformanceConfig)
    
    # Enhanced processing flags
    enable_enhanced_visualization: bool = True  # Show motion/visual info
    enable_performance_monitoring: bool = True  # Monitor performance
    enable_adaptive_quality: bool = True        # Adapt quality based on performance
    
    def optimize_for_realtime(self):
        """Optimize configuration for real-time processing at 10fps"""
        # Adjust detection parameters for speed
        self.detection_conf = max(0.2, self.detection_conf)  # Higher threshold for speed
        self.max_detections = min(25, self.max_detections)   # Fewer detections for speed
        
        # Optimize track consolidation for speed
        tc = self.track_consolidation
        tc.max_gap_frames = min(10, tc.max_gap_frames)       # Shorter memory for speed
        tc.cleanup_interval_frames = min(30, tc.cleanup_interval_frames)
        
        # Optimize visual features for speed
        tc.feature_similarity_threshold = min(0.7, tc.feature_similarity_threshold)  # Higher threshold
        
        # Set performance targets
        self.performance.target_fps = 10.0
        self.performance.max_processing_time_ms = 100.0
        
        self.logger.info("Configuration optimized for real-time 10fps processing")
    
    def optimize_for_accuracy(self):
        """Optimize configuration for maximum accuracy (slower processing)"""
        # Lower thresholds for better detection
        self.detection_conf = 0.1
        self.max_detections = 50
        
        # Enhanced track consolidation for accuracy
        tc = self.track_consolidation
        tc.max_gap_frames = 20
        tc.feature_similarity_threshold = 0.5  # Lower for more matches
        tc.enable_visual_features = True
        tc.enable_motion_prediction = True
        
        # Relaxed performance targets
        self.performance.target_fps = 5.0
        self.performance.max_processing_time_ms = 200.0
        
        self.logger.info("Configuration optimized for maximum accuracy")

    @classmethod
    def create_simplified_tracker(cls) -> 'TrackerConfig':
        """Create configuration for enhanced simplified tracking with all features"""
        return cls(
            # Optimized detection settings for 10fps processing
            detection_conf=0.15,              # Lower threshold to catch more objects
            detection_iou=0.6,                # Moderate NMS for 10fps
            tracker_type="bytetrack.yaml",
            max_detections=30,
            
            # GPS settings optimized for 10fps
            gps_frame_interval=6,             # Process every 6th frame (60fps → 10fps)
            enable_gps_extraction=True,
            
            # ENHANCED track consolidation with ALL features enabled
            track_consolidation=TrackConsolidationConfig(
                # === MOTION PREDICTION SETTINGS ===
                max_gap_frames=12,                      # 1.2 seconds at 10fps
                reappearance_iou_threshold=0.4,         # Moderate for 10fps
                reappearance_spatial_threshold=60,      # Pixels for 10fps processing
                
                # === DUPLICATE PREVENTION ===
                duplicate_spatial_threshold=40,         # Tighter duplicate detection
                duplicate_size_threshold=0.75,          # Size similarity required
                duplicate_iou_threshold=0.5,            # Overlap threshold
                
                # === ID MANAGEMENT ===
                prefer_lower_id=True,
                enable_id_consolidation=True,           # ✅ ENABLED
                enable_reappearance_detection=True,     # ✅ ENABLED (fixed for 10fps)
                
                # === MEMORY MANAGEMENT ===
                max_track_memory=150,                   # More memory for 10fps
                cleanup_interval_frames=50,             # Cleanup every 5 seconds
                
                # === DEBUGGING ===
                log_consolidations=True,
                log_reappearances=True,
                
                # === ✅ MOTION PREDICTION FEATURES ===
                enable_motion_prediction=True,          # ✅ ENABLED
                enable_visual_features=True,            # ✅ ENABLED
                feature_similarity_threshold=0.6,       # Visual similarity threshold
                motion_weight=0.4,                      # 40% motion, 60% visual
                feature_weight=0.6,
            ),
            
            # Output settings
            export_json=True,
            export_csv=True,
            min_detections_for_export=2,            # Lower for 10fps
            
            # Static car detection (optimized for 10fps)
            enable_static_car_detection=True,
            static_movement_threshold_m=1.0,        # Slightly higher for 10fps
            static_time_threshold_s=8.0,            # 8 seconds before skipping
            
            # Real-time display settings
            display_gps_frames_only=True,
            display_track_history=True,
            display_consolidation_info=True
        )

    # Add method to get enhanced tracking parameters
    def get_ultralytics_track_params(self) -> dict:
        """Get enhanced parameters for model.track() call with motion prediction"""
        base_params = self.get_ultralytics_track_params()
        
        # Enhanced parameters for motion prediction and visual features
        enhanced_params = {
            **base_params,
            'persist': True,
            'tracker': self.tracker_type,
            'conf': self.detection_conf,
            'iou': self.detection_iou,
            'max_det': self.max_detections,
            'verbose': False,
            # Enhanced tracking parameters
            'half': False,                    # Full precision for better features
            'device': None,                   # Auto-detect best device
            'classes': None,                  # Track all classes
            'agnostic_nms': False,           # Class-specific NMS
            'retina_masks': False,           # No masks needed
            'embed': None,                   # No embedding override
        }
        
        return enhanced_params

    @classmethod
    def create_simplified_tracker(cls) -> 'TrackerConfig':
        """Create configuration for simplified tracking (no depth/geolocation)"""
        return cls(
            # Optimized detection settings
            detection_conf=0.20,
            detection_iou=0.5,
            tracker_type="bytetrack.yaml",
            max_detections=30,
            
            # GPS settings
            gps_frame_interval=6,
            enable_gps_extraction=True,
            
            # Track consolidation with LESS AGGRESSIVE settings
            track_consolidation=TrackConsolidationConfig(
                max_gap_frames=8,                      # REDUCED: Less aggressive reappearance
                reappearance_iou_threshold=0.5,        # HIGHER: More strict reappearance
                reappearance_spatial_threshold=40,     # SMALLER: Closer proximity required
                duplicate_spatial_threshold=30,        # SMALLER: Tighter duplicate detection
                duplicate_size_threshold=0.8,          # HIGHER: More similar size required
                duplicate_iou_threshold=0.6,           # HIGHER: More overlap required
                prefer_lower_id=True,
                enable_id_consolidation=True,
                enable_reappearance_detection=False,   # DISABLE: Causing old ID reuse
                log_consolidations=True,
                log_reappearances=True
            ),
            
            # Output settings
            export_json=True,
            export_csv=True,
            min_detections_for_export=2,
            
            # Static car detection
            enable_static_car_detection=True,
            static_movement_threshold_m=0.9,
            static_time_threshold_s=5.0,
            
            # Real-time display
            display_gps_frames_only=True,
            display_track_history=True,
            display_consolidation_info=True
        )
    
    def get_ultralytics_track_params(self) -> dict:
        """Get parameters for model.track() call"""
        return {
            'persist': True,
            'tracker': self.tracker_type,
            'conf': self.detection_conf,
            'iou': self.detection_iou,
            'max_det': self.max_detections,
            'verbose': False  # Reduce verbosity for cleaner output
        }

@dataclass
class DetectorConfig:
    """Configuration for object detectors"""
    model_path: str
    config_path: str = ""
    target_classes: Optional[list] = None
    confidence_threshold: float = 0.5
    nms_threshold: float = 0.4
    model_type: str = "yolov11"

@dataclass
class StereoCalibrationConfig:
    """Stereo camera calibration parameters (kept for backward compatibility)"""
    camera_matrix_left: np.ndarray
    camera_matrix_right: np.ndarray
    dist_coeffs_left: np.ndarray
    dist_coeffs_right: np.ndarray
    R: np.ndarray
    T: np.ndarray
    E: Optional[np.ndarray] = None
    F: Optional[np.ndarray] = None
    P1: Optional[np.ndarray] = None
    P2: Optional[np.ndarray] = None
    Q: Optional[np.ndarray] = None
    baseline: float = 0.0
    image_width: int = 1920
    image_height: int = 1080
    
    @classmethod
    def from_pickle(cls, calibration_path: str) -> 'StereoCalibrationConfig':
        """Load stereo calibration from pickle file"""
        try:
            with open(calibration_path, 'rb') as f:
                calib_data = pickle.load(f)
        except Exception as e:
            raise IOError(f"Failed to load calibration file {calibration_path}: {e}")
        
        baseline = calib_data.get('baseline', 0.0)
        if baseline == 0.0 and 'T' in calib_data:
            baseline = float(np.linalg.norm(calib_data['T']))
        
        return cls(
            camera_matrix_left=calib_data['camera_matrix_left'],
            camera_matrix_right=calib_data['camera_matrix_right'],
            dist_coeffs_left=calib_data['dist_coeffs_left'],
            dist_coeffs_right=calib_data['dist_coeffs_right'],
            R=calib_data['R'],
            T=calib_data['T'],
            E=calib_data.get('E'),
            F=calib_data.get('F'),
            P1=calib_data.get('P1'),
            P2=calib_data.get('P2'),
            Q=calib_data.get('Q'),
            baseline=baseline,
            image_width=calib_data.get('image_width', 1920),
            image_height=calib_data.get('image_height', 1080)
        )

@dataclass
class CameraConfig:
    """Camera calibration parameters (backward compatibility)"""
    camera_matrix: list
    distortion_coeffs: list
    image_width: int
    image_height: int
    
    @classmethod
    def from_file(cls, calibration_path: str) -> 'CameraConfig':
        """Load camera configuration from file"""
        with open(calibration_path, 'r') as f:
            data = json.load(f)
        return cls(**data)

================
File: argus_track/main.py
================
# argus_track/main.py (UPDATED FOR SIMPLIFIED TRACKING)

import argparse
import logging
from pathlib import Path
from typing import Optional
import time
import numpy as np

from argus_track.config import TrackerConfig
from argus_track import __version__
from argus_track.trackers.simplified_lightpost_tracker import SimplifiedLightPostTracker
from argus_track.utils import setup_logging


def main():
    """Main function for simplified tracking with ID consolidation"""
    parser = argparse.ArgumentParser(
        description=f"Argus Track: Simplified Light Post Tracking System v{__version__}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Basic tracking with automatic GPS extraction
    python main.py input.mp4 --model model.pt --show-realtime
    
    # Batch processing without visualization
    python main.py input.mp4 --model model.pt --no-realtime
    
    # Custom output paths
    python main.py input.mp4 --model model.pt --json-output results.json --csv-output gps.csv
        """
    )
    
    # Basic arguments
    parser.add_argument('input_video', type=str, help='Path to input video file')
    parser.add_argument('--model', type=str, required=True, help='Path to YOLO model file')
    
    # Output arguments
    parser.add_argument('--output-video', type=str, help='Path for output visualization video')
    parser.add_argument('--json-output', type=str, help='Path for JSON output file')
    parser.add_argument('--csv-output', type=str, help='Path for CSV output file')
    
    # Processing arguments
    parser.add_argument('--show-realtime', action='store_true', default=True,
                       help='Show real-time visualization (default: True)')
    parser.add_argument('--no-realtime', action='store_true',
                       help='Disable real-time visualization')
    
    # Tracking parameters
    parser.add_argument('--detection-conf', type=float, default=0.20,
                       help='Detection confidence threshold (default: 0.20)')
    parser.add_argument('--gps-interval', type=int, default=6,
                       help='GPS frame interval (default: 6 - every 6th frame)')
    
    # Track consolidation parameters
    parser.add_argument('--disable-consolidation', action='store_true',
                       help='Disable track ID consolidation')
    parser.add_argument('--disable-reappearance', action='store_true',
                       help='Disable track reappearance detection')
    parser.add_argument('--max-gap-frames', type=int, default=15,
                       help='Max frames to remember lost tracks (default: 15)')
    
    # Static car detection
    parser.add_argument('--disable-static-car', action='store_true',
                       help='Disable static car detection')
    
    # Logging
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    parser.add_argument('--log-file', type=str, help='Path to log file')
    parser.add_argument('--no-save', action='store_true', help='Do not save results')
    
    args = parser.parse_args()
    
    # Validate input
    if not Path(args.input_video).exists():
        print(f"❌ Error: Input video not found: {args.input_video}")
        return 1
    
    if not Path(args.model).exists():
        print(f"❌ Error: Model file not found: {args.model}")
        return 1
    
    # Handle real-time display settings
    show_realtime = args.show_realtime and not args.no_realtime
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    setup_logging(log_file=args.log_file, level=log_level)
    logger = logging.getLogger(__name__)
    
    logger.info(f"🚀 Argus Track v{__version__} - Simplified Tracking")
    logger.info(f"📹 Input video: {args.input_video}")
    logger.info(f"🤖 Model: {args.model}")
    logger.info(f"📺 Real-time display: {show_realtime}")
    
    try:
        # Create configuration
        config = TrackerConfig.create_simplified_tracker()
        
        # Apply command line overrides
        config.detection_conf = args.detection_conf
        config.gps_frame_interval = args.gps_interval
        
        # Track consolidation settings
        if args.disable_consolidation:
            config.track_consolidation.enable_id_consolidation = False
            logger.info("🔧 ID consolidation disabled")
        
        if args.disable_reappearance:
            config.track_consolidation.enable_reappearance_detection = False
            logger.info("🔧 Reappearance detection disabled")
        
        config.track_consolidation.max_gap_frames = args.max_gap_frames
        
        # Static car detection
        if args.disable_static_car:
            config.enable_static_car_detection = False
            logger.info("🔧 Static car detection disabled")
        
        logger.info("📷 Configuration created")
        logger.info(f"   Detection confidence: {config.detection_conf}")
        logger.info(f"   GPS frame interval: {config.gps_frame_interval}")
        logger.info(f"   ID consolidation: {config.track_consolidation.enable_id_consolidation}")
        logger.info(f"   Reappearance detection: {config.track_consolidation.enable_reappearance_detection}")
        logger.info(f"   Max gap frames: {config.track_consolidation.max_gap_frames}")
        
        # Extract GPS data from video
        logger.info("🗺️ Extracting GPS data from video metadata...")
        gps_data = None
        
        try:
            from argus_track.utils.gps_extraction import extract_gps_from_stereo_videos
            
            # Use the same video file for both left and right (monocular)
            gps_data, extraction_method = extract_gps_from_stereo_videos(
                args.input_video, args.input_video, method='auto'
            )
            
            if gps_data:
                logger.info(f"✅ Extracted {len(gps_data)} GPS points using {extraction_method}")
                
                # Log GPS data range for verification
                if len(gps_data) > 0:
                    start_gps = gps_data[0]
                    end_gps = gps_data[-1]
                    logger.info(f"🗺️ GPS range: ({start_gps.latitude:.6f}, {start_gps.longitude:.6f}) to "
                               f"({end_gps.latitude:.6f}, {end_gps.longitude:.6f})")
                    logger.info(f"⏱️ Time range: {start_gps.timestamp:.1f}s to {end_gps.timestamp:.1f}s")
            else:
                logger.warning("⚠️ No GPS data found in video metadata")
                logger.warning("   Processing will continue without GPS synchronization")
                
        except ImportError as e:
            logger.error(f"❌ GPS extraction dependencies missing: {e}")
            logger.error("   Install required packages: pip install beautifulsoup4 lxml")
            gps_data = None
        except Exception as e:
            logger.error(f"❌ GPS extraction failed: {e}")
            gps_data = None
        
        # Initialize simplified tracker
        tracker = SimplifiedLightPostTracker(
            config=config,
            model_path=args.model,
            show_realtime=show_realtime,
            display_size=(1280, 720)
        )
        
        # Show helpful tips
        if show_realtime:
            logger.info("🖥️  Real-time visualization controls:")
            logger.info("   - Press 'q' to quit")
            logger.info("   - Press 'p' to pause/resume") 
            logger.info("   - Press 's' to save screenshot")
            logger.info("   - Only GPS-synchronized frames will be displayed")
        
        # Show processing info
        if gps_data:
            effective_frames = len(gps_data)
            logger.info(f"📍 GPS-synchronized processing: {effective_frames} frames to process")
            logger.info("   → Frames will be processed at GPS frequency (every 6th frame)")
        else:
            logger.warning("⚠️ No GPS data: Processing all frames")
        
        # Process video
        start_time = time.time()
        
        results = tracker.process_video(
            video_path=args.input_video,
            gps_data=gps_data,
            output_path=args.output_video,
            save_results=not args.no_save
        )
        
        processing_time = time.time() - start_time
        
        # Print results
        logger.info("🎉 PROCESSING COMPLETE!")
        logger.info(f"⏱️  Total processing time: {processing_time:.1f} seconds")
        
        # Processing statistics
        logger.info("📊 PROCESSING STATISTICS:")
        logger.info(f"   Total frames in video: {results['total_frames']}")
        logger.info(f"   Processed frames: {results['processed_frames']}")
        logger.info(f"   Skipped (GPS sync): {results['skipped_frames_gps']}")
        logger.info(f"   Skipped (static car): {results['skipped_frames_static']}")
        logger.info(f"   Processing efficiency: {results['avg_fps']:.1f} FPS")
        
        # Track consolidation statistics
        track_stats = results['track_manager_stats']
        logger.info("🔧 TRACK CONSOLIDATION STATISTICS:")
        logger.info(f"   Active tracks: {track_stats['active_tracks']}")
        logger.info(f"   Total consolidations: {track_stats['total_consolidations']}")
        logger.info(f"   Total reappearances: {track_stats['total_reappearances']}")
        logger.info(f"   Tracks created: {track_stats['tracks_created']}")
        
        # Output statistics
        output_stats = results['output_summary']
        logger.info("📄 OUTPUT STATISTICS:")
        logger.info(f"   Unique tracks: {output_stats['unique_tracks']}")
        logger.info(f"   Total detections: {output_stats['total_detections']}")
        logger.info(f"   Avg detections/frame: {output_stats['avg_detections_per_frame']:.1f}")
        logger.info(f"   Frames with GPS: {output_stats['frames_with_gps']}")
        
        # Class distribution
        if output_stats['class_distribution']:
            logger.info("🏷️  CLASS DISTRIBUTION:")
            for class_name, count in output_stats['class_distribution'].items():
                logger.info(f"   {class_name}: {count} detections")
        
        # Output files
        if not args.no_save:
            logger.info("📁 OUTPUT FILES:")
            
            if 'json_output' in results:
                json_path = results['json_output']
                if Path(json_path).exists():
                    file_size = Path(json_path).stat().st_size / (1024 * 1024)
                    logger.info(f"   📄 JSON data: {json_path} ({file_size:.1f} MB)")
                    
                    # Custom JSON output path
                    if args.json_output:
                        custom_path = Path(args.json_output)
                        Path(json_path).rename(custom_path)
                        logger.info(f"   📄 Moved to: {custom_path}")
            
            if 'csv_output' in results:
                csv_path = results['csv_output']
                if Path(csv_path).exists():
                    file_size = Path(csv_path).stat().st_size / 1024
                    logger.info(f"   📍 GPS CSV: {csv_path} ({file_size:.1f} KB)")
                    
                    # Custom CSV output path
                    if args.csv_output:
                        custom_path = Path(args.csv_output)
                        Path(csv_path).rename(custom_path)
                        logger.info(f"   📍 Moved to: {custom_path}")
            
            if args.output_video and Path(args.output_video).exists():
                file_size = Path(args.output_video).stat().st_size / (1024 * 1024)
                logger.info(f"   🎬 Visualization video: {args.output_video} ({file_size:.1f} MB)")
        
        # Success summary
        logger.info("🏁 SUCCESS SUMMARY:")
        logger.info(f"   ✅ {results['processed_frames']} frames processed")
        logger.info(f"   ✅ {track_stats['total_consolidations']} ID consolidations applied")
        logger.info(f"   ✅ {track_stats['total_reappearances']} track reappearances handled")
        logger.info(f"   ✅ {output_stats['unique_tracks']} unique objects tracked")
        
        if gps_data:
            logger.info(f"   ✅ GPS data synchronized for {output_stats['frames_with_gps']} frames")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("❌ Processing interrupted by user (Ctrl+C)")
        return 1
    except Exception as e:
        logger.error(f"❌ Error during processing: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit(main())




================================================================
End of Codebase
================================================================
