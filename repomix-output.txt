This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: argus_track/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
argus_track/
  analysis/
    __init__.py
    static_analyzer.py
  core/
    __init__.py
    detection.py
    gps.py
    stereo.py
    track.py
  detectors/
    __init__.py
    base.py
    mock.py
    yolo.py
    yolov11.py
  filters/
    __init__.py
    kalman.py
  stereo/
    __init__.py
    calibration.py
    matching.py
    triangulation.py
  trackers/
    __init__.py
    bytetrack.py
    lightpost_tracker.py
    stereo_lightpost_tracker.py
  utils/
    __init__.py
    config_validator.py
    gps_extraction.py
    gps_sync_tracker.py
    gps_utils.py
    io.py
    iou.py
    performance.py
    visualization.py
  __init__.py
  __version__.py
  config.py
  exceptions.py
  main.py
  requirements.txt

================================================================
Files
================================================================

================
File: argus_track/utils/gps_sync_tracker.py
================
"""
GPS-synchronized frame processing implementation for Argus Track
FIXED: Only process frames when GPS data is actually available
"""

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Any, Set
import cv2
import time

from ..core import GPSData

# Configure logging
logger = logging.getLogger(__name__)

class GPSSynchronizer:
    """
    Synchronizes video frame processing with actual GPS data points
    ONLY processes frames that have real GPS measurements
    """
    
    def __init__(self, 
                 gps_data: List[GPSData], 
                 video_fps: float, 
                 gps_fps: float = 10.0):
        """
        Initialize GPS synchronizer - FIXED VERSION
        
        Args:
            gps_data: List of actual GPS data points
            video_fps: Video frame rate in FPS
            gps_fps: Expected GPS data rate in Hz (for validation only)
        """
        if not gps_data:
            self.gps_data = []
            self.sync_frames = set()
            self.frame_to_gps = {}
            return
            
        self.gps_data = sorted(gps_data, key=lambda x: x.timestamp)
        self.video_fps = video_fps
        self.gps_fps = gps_fps
        
        # Calculate video start time (assume GPS and video start together)
        self.video_start_time = self.gps_data[0].timestamp
        
        # FIXED: Only create mappings for frames where we have actual GPS data
        self.frame_to_gps: Dict[int, int] = {}  # frame_idx -> gps_data_idx
        self.sync_frames: Set[int] = set()  # Only frames with GPS data
        
        # Generate the mapping - ONLY for actual GPS points
        self._generate_gps_frame_mapping()
        
        logger.info(f"GPS Synchronizer initialized:")
        logger.info(f"  📊 GPS points available: {len(self.gps_data)}")
        logger.info(f"  🎬 Video FPS: {video_fps}")
        logger.info(f"  📍 Frames to process: {len(self.sync_frames)}")
        logger.info(f"  ⏱️  Processing ratio: {len(self.sync_frames)}/{int(video_fps * self._get_video_duration()):.0f} frames")
        
    def _get_video_duration(self) -> float:
        """Calculate video duration based on GPS data"""
        if len(self.gps_data) < 2:
            return 0.0
        return self.gps_data[-1].timestamp - self.gps_data[0].timestamp
    
    def _generate_gps_frame_mapping(self) -> None:
        """Generate mapping ONLY for frames with actual GPS data"""
        if not self.gps_data:
            return
        
        for gps_idx, gps_point in enumerate(self.gps_data):
            # Calculate which video frame corresponds to this GPS timestamp
            time_offset = gps_point.timestamp - self.video_start_time
            frame_number = int(time_offset * self.video_fps)
            
            # Only map frames that are valid
            if frame_number >= 0:
                self.frame_to_gps[frame_number] = gps_idx
                self.sync_frames.add(frame_number)
        
        logger.info(f"GPS-Video Mapping:")
        logger.info(f"  📍 GPS points: {len(self.gps_data)}")
        logger.info(f"  🎬 Mapped frames: {len(self.sync_frames)}")
        
        if self.sync_frames:
            min_frame = min(self.sync_frames)
            max_frame = max(self.sync_frames)
            logger.info(f"  📊 Frame range: {min_frame} to {max_frame}")
            
            # Show actual GPS frequency
            frame_intervals = []
            sorted_frames = sorted(self.sync_frames)
            for i in range(1, len(sorted_frames)):
                interval = sorted_frames[i] - sorted_frames[i-1]
                frame_intervals.append(interval)
            
            if frame_intervals:
                avg_interval = np.mean(frame_intervals)
                actual_gps_freq = self.video_fps / avg_interval if avg_interval > 0 else 0
                logger.info(f"  🔄 Actual GPS frequency: {actual_gps_freq:.1f} Hz")
                logger.info(f"  📏 Average frame interval: {avg_interval:.1f} frames")
    
    def should_process_frame(self, frame_idx: int) -> bool:
        """
        FIXED: Only process frames that have actual GPS data
        
        Args:
            frame_idx: Frame index
            
        Returns:
            True ONLY if frame has GPS data, False otherwise
        """
        return frame_idx in self.sync_frames
    
    def get_gps_for_frame(self, frame_idx: int) -> Optional[GPSData]:
        """
        Get GPS data for a specific frame
        
        Args:
            frame_idx: Frame index
            
        Returns:
            GPS data for the frame or None if not available
        """
        gps_idx = self.frame_to_gps.get(frame_idx)
        if gps_idx is not None and gps_idx < len(self.gps_data):
            return self.gps_data[gps_idx]
        return None
    
    def get_all_sync_frames(self) -> List[int]:
        """
        Get all frames that should be processed (only GPS frames)
        
        Returns:
            Sorted list of frame indices with GPS data
        """
        return sorted(list(self.sync_frames))
    
    def get_sync_frames_count(self) -> int:
        """
        Get number of frames to process (only GPS frames)
        
        Returns:
            Number of frames with GPS data
        """
        return len(self.sync_frames)
    
    def get_next_sync_frame(self, current_frame: int) -> Optional[int]:
        """
        Get the next frame with GPS data
        
        Args:
            current_frame: Current frame index
            
        Returns:
            Next frame index with GPS data or None if no more
        """
        sync_frames = sorted(list(self.sync_frames))
        for frame in sync_frames:
            if frame > current_frame:
                return frame
        return None
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about GPS-synchronized processing
        
        Returns:
            Dictionary with processing statistics
        """
        if not self.sync_frames:
            return {
                'gps_points': 0,
                'sync_frames': 0,
                'processing_ratio': 0.0,
                'avg_gps_frequency': 0.0
            }
        
        sync_frames = sorted(list(self.sync_frames))
        frame_intervals = np.diff(sync_frames) if len(sync_frames) > 1 else [0]
        
        avg_interval = np.mean(frame_intervals) if len(frame_intervals) > 0 else 0
        actual_gps_freq = self.video_fps / avg_interval if avg_interval > 0 else 0
        
        # Calculate total video frames in the GPS time range
        if len(sync_frames) >= 2:
            frame_span = max(sync_frames) - min(sync_frames)
            processing_ratio = len(sync_frames) / (frame_span + 1) if frame_span > 0 else 1.0
        else:
            processing_ratio = 1.0 if sync_frames else 0.0
        
        return {
            'gps_points': len(self.gps_data),
            'sync_frames': len(self.sync_frames),
            'processing_ratio': processing_ratio,
            'avg_gps_frequency': actual_gps_freq,
            'frame_range': (min(sync_frames), max(sync_frames)) if sync_frames else (0, 0),
            'avg_frame_interval': avg_interval
        }


def create_gps_synchronizer(gps_data: List[GPSData], 
                           video_fps: float, 
                           gps_fps: float = 10.0) -> GPSSynchronizer:
    """
    Create a GPS synchronizer for frame processing
    FIXED: Only processes frames with actual GPS data
    
    Args:
        gps_data: List of actual GPS data points
        video_fps: Video frame rate in FPS
        gps_fps: Expected GPS data rate in Hz (for validation)
        
    Returns:
        GPS synchronizer instance
    """
    return GPSSynchronizer(gps_data, video_fps, gps_fps)

================
File: argus_track/analysis/static_analyzer.py
================
"""Enhanced static object analysis"""

import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.cluster import DBSCAN

from ..core import Track, Detection


class StaticObjectAnalyzer:
    """Advanced analysis of static objects in tracking data"""
    
    def __init__(self, 
                 position_threshold: float = 2.0,
                 velocity_threshold: float = 0.5,
                 min_observations: int = 10,
                 stability_window: int = 30):
        """
        Initialize static object analyzer
        
        Args:
            position_threshold: Maximum position variance for static classification
            velocity_threshold: Maximum velocity for static classification
            min_observations: Minimum observations required
            stability_window: Window size for stability analysis
        """
        self.position_threshold = position_threshold
        self.velocity_threshold = velocity_threshold
        self.min_observations = min_observations
        self.stability_window = stability_window
    
    def analyze_track(self, track: Track) -> Dict[str, float]:
        """
        Analyze a single track for static behavior
        
        Args:
            track: Track to analyze
            
        Returns:
            Dictionary with analysis metrics
        """
        if len(track.detections) < self.min_observations:
            return {
                'is_static': False,
                'confidence': 0.0,
                'position_variance': float('inf'),
                'velocity_mean': float('inf'),
                'stability_score': 0.0
            }
        
        # Extract positions
        positions = np.array([det.center for det in track.detections])
        
        # Calculate position variance
        position_variance = np.std(positions, axis=0).mean()
        
        # Calculate velocities
        if len(positions) > 1:
            velocities = np.diff(positions, axis=0)
            velocity_mean = np.abs(velocities).mean()
        else:
            velocity_mean = 0.0
        
        # Calculate stability score using sliding window
        stability_scores = []
        for i in range(len(positions) - self.stability_window + 1):
            window = positions[i:i + self.stability_window]
            window_variance = np.std(window, axis=0).mean()
            stability_scores.append(1.0 / (1.0 + window_variance))
        
        stability_score = np.mean(stability_scores) if stability_scores else 0.0
        
        # Determine if static
        is_static = (
            position_variance < self.position_threshold and
            velocity_mean < self.velocity_threshold
        )
        
        # Calculate confidence
        confidence = min(1.0, stability_score * (1.0 - velocity_mean / self.velocity_threshold))
        
        return {
            'is_static': is_static,
            'confidence': confidence,
            'position_variance': position_variance,
            'velocity_mean': velocity_mean,
            'stability_score': stability_score
        }
    
    def find_clusters(self, tracks: Dict[int, Track], 
                     eps: float = 50.0, 
                     min_samples: int = 2) -> Dict[int, int]:
        """
        Find clusters of static objects
        
        Args:
            tracks: Dictionary of tracks
            eps: Maximum distance between points in a cluster
            min_samples: Minimum samples in a cluster
            
        Returns:
            Dictionary mapping track_id to cluster_id
        """
        # Filter static tracks
        static_tracks = {}
        positions = []
        track_ids = []
        
        for track_id, track in tracks.items():
            analysis = self.analyze_track(track)
            if analysis['is_static']:
                static_tracks[track_id] = track
                positions.append(track.trajectory[-1])  # Use last position
                track_ids.append(track_id)
        
        if len(positions) < min_samples:
            return {}
        
        # Perform clustering
        positions = np.array(positions)
        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(positions)
        
        # Map track IDs to clusters
        cluster_mapping = {}
        for track_id, cluster_id in zip(track_ids, clustering.labels_):
            if cluster_id != -1:  # Ignore noise points
                cluster_mapping[track_id] = cluster_id
        
        return cluster_mapping
    
    def merge_duplicate_tracks(self, tracks: Dict[int, Track],
                              distance_threshold: float = 30.0) -> Dict[int, List[int]]:
        """
        Identify duplicate tracks of the same static object
        
        Args:
            tracks: Dictionary of tracks
            distance_threshold: Maximum distance to consider duplicates
            
        Returns:
            Dictionary mapping primary track_id to list of duplicate track_ids
        """
        static_tracks = []
        for track_id, track in tracks.items():
            analysis = self.analyze_track(track)
            if analysis['is_static']:
                static_tracks.append((track_id, track))
        
        duplicates = {}
        processed = set()
        
        for i, (track_id1, track1) in enumerate(static_tracks):
            if track_id1 in processed:
                continue
                
            duplicates[track_id1] = []
            
            for j, (track_id2, track2) in enumerate(static_tracks[i+1:], i+1):
                if track_id2 in processed:
                    continue
                
                # Calculate distance between average positions
                pos1 = np.mean([det.center for det in track1.detections], axis=0)
                pos2 = np.mean([det.center for det in track2.detections], axis=0)
                distance = np.linalg.norm(pos1 - pos2)
                
                if distance < distance_threshold:
                    duplicates[track_id1].append(track_id2)
                    processed.add(track_id2)
        
        # Remove entries with no duplicates
        return {k: v for k, v in duplicates.items() if v}
    
    def calculate_persistence_score(self, track: Track, 
                                   total_frames: int) -> float:
        """
        Calculate persistence score for a track
        
        Args:
            track: Track to analyze
            total_frames: Total frames in video
            
        Returns:
            Persistence score between 0 and 1
        """
        if total_frames == 0:
            return 0.0
        
        # Calculate presence ratio
        presence_ratio = track.age / total_frames
        
        # Calculate detection density
        if track.age > 0:
            detection_density = len(track.detections) / track.age
        else:
            detection_density = 0.0
        
        # Combine metrics
        persistence_score = presence_ratio * detection_density
        
        return min(1.0, persistence_score)

================
File: argus_track/core/__init__.py
================
"""Core data structures for ByteTrack Light Post Tracking System"""

from .detection import Detection
from .track import Track
from .gps import GPSData

__all__ = ["Detection", "Track", "GPSData"]

================
File: argus_track/core/detection.py
================
"""Detection data structure"""

from dataclasses import dataclass
import numpy as np


@dataclass
class Detection:
    """Single object detection"""
    bbox: np.ndarray                   # [x1, y1, x2, y2] format
    score: float                       # Confidence score [0, 1]
    class_id: int                      # Object class ID
    frame_id: int                      # Frame number
    
    @property
    def tlbr(self) -> np.ndarray:
        """Get bounding box in top-left, bottom-right format"""
        return self.bbox
    
    @property
    def xywh(self) -> np.ndarray:
        """Get bounding box in center-x, center-y, width, height format"""
        x1, y1, x2, y2 = self.bbox
        return np.array([
            (x1 + x2) / 2,  # center x
            (y1 + y2) / 2,  # center y
            x2 - x1,        # width
            y2 - y1         # height
        ])
    
    @property
    def area(self) -> float:
        """Calculate bounding box area"""
        x1, y1, x2, y2 = self.bbox
        return (x2 - x1) * (y2 - y1)
    
    @property
    def center(self) -> np.ndarray:
        """Get center point of bounding box"""
        x1, y1, x2, y2 = self.bbox
        return np.array([(x1 + x2) / 2, (y1 + y2) / 2])
    
    def to_dict(self) -> dict:
        """Convert to dictionary representation"""
        return {
            'bbox': self.bbox.tolist(),
            'score': self.score,
            'class_id': self.class_id,
            'frame_id': self.frame_id
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'Detection':
        """Create from dictionary representation"""
        return cls(
            bbox=np.array(data['bbox']),
            score=data['score'],
            class_id=data['class_id'],
            frame_id=data['frame_id']
        )

================
File: argus_track/core/gps.py
================
"""GPS data structure"""

from dataclasses import dataclass
from typing import Dict, Any


@dataclass
class GPSData:
    """GPS data for a single frame"""
    timestamp: float
    latitude: float
    longitude: float
    altitude: float
    heading: float
    accuracy: float = 1.0              # GPS accuracy in meters
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'timestamp': self.timestamp,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'altitude': self.altitude,
            'heading': self.heading,
            'accuracy': self.accuracy
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'GPSData':
        """Create from dictionary representation"""
        return cls(**data)
    
    @classmethod
    def from_csv_line(cls, line: str) -> 'GPSData':
        """Create from CSV line"""
        parts = line.strip().split(',')
        if len(parts) < 5:
            raise ValueError(f"Invalid GPS data line: {line}")
        
        return cls(
            timestamp=float(parts[0]),
            latitude=float(parts[1]),
            longitude=float(parts[2]),
            altitude=float(parts[3]),
            heading=float(parts[4]),
            accuracy=float(parts[5]) if len(parts) > 5 else 1.0
        )

================
File: argus_track/core/stereo.py
================
# argus_track/core/stereo.py (NEW FILE)

"""Stereo vision data structures and utilities"""

from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Any
import numpy as np

from .detection import Detection


@dataclass
class StereoDetection:
    """Stereo detection pair from left and right cameras"""
    left_detection: Detection
    right_detection: Detection
    disparity: float                   # Pixel disparity between left/right
    depth: float                       # Estimated depth in meters
    world_coordinates: np.ndarray      # 3D coordinates in camera frame
    stereo_confidence: float           # Confidence of stereo match [0,1]
    
    @property
    def center_3d(self) -> np.ndarray:
        """Get 3D center point"""
        return self.world_coordinates
    
    @property
    def left_center(self) -> np.ndarray:
        """Get left camera center point"""
        return self.left_detection.center
    
    @property
    def right_center(self) -> np.ndarray:
        """Get right camera center point"""
        return self.right_detection.center
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'left_detection': self.left_detection.to_dict(),
            'right_detection': self.right_detection.to_dict(),
            'disparity': self.disparity,
            'depth': self.depth,
            'world_coordinates': self.world_coordinates.tolist(),
            'stereo_confidence': self.stereo_confidence
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StereoDetection':
        """Create from dictionary representation"""
        return cls(
            left_detection=Detection.from_dict(data['left_detection']),
            right_detection=Detection.from_dict(data['right_detection']),
            disparity=data['disparity'],
            depth=data['depth'],
            world_coordinates=np.array(data['world_coordinates']),
            stereo_confidence=data['stereo_confidence']
        )


@dataclass
class StereoFrame:
    """Stereo frame pair with synchronized detections"""
    frame_id: int
    timestamp: float
    left_frame: np.ndarray
    right_frame: np.ndarray
    left_detections: List[Detection]
    right_detections: List[Detection]
    stereo_detections: List[StereoDetection]
    gps_data: Optional['GPSData'] = None
    
    @property
    def has_gps(self) -> bool:
        """Check if frame has GPS data"""
        return self.gps_data is not None
    
    def get_stereo_count(self) -> int:
        """Get number of successful stereo matches"""
        return len(self.stereo_detections)


@dataclass
class StereoTrack:
    """Extended track with stereo 3D information"""
    track_id: int
    stereo_detections: List[StereoDetection]
    world_trajectory: List[np.ndarray]  # 3D trajectory in world coordinates
    gps_trajectory: List[np.ndarray]    # GPS coordinate trajectory
    estimated_location: Optional['GeoLocation'] = None
    depth_consistency: float = 0.0      # Measure of depth consistency
    
    @property
    def is_static_3d(self) -> bool:
        """Check if object is static in 3D space"""
        if len(self.world_trajectory) < 3:
            return False
        
        positions = np.array(self.world_trajectory)
        std_dev = np.std(positions, axis=0)
        
        # Object is static if movement in any axis is < 1 meter
        return np.all(std_dev < 1.0)
    
    @property
    def average_depth(self) -> float:
        """Get average depth of all detections"""
        if not self.stereo_detections:
            return 0.0
        return np.mean([det.depth for det in self.stereo_detections])
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            'track_id': self.track_id,
            'stereo_detections': [det.to_dict() for det in self.stereo_detections[-10:]],  # Last 10
            'world_trajectory': [pos.tolist() for pos in self.world_trajectory[-20:]],     # Last 20
            'gps_trajectory': [pos.tolist() for pos in self.gps_trajectory[-20:]],        # Last 20
            'estimated_location': self.estimated_location.__dict__ if self.estimated_location else None,
            'depth_consistency': self.depth_consistency,
            'is_static_3d': self.is_static_3d,
            'average_depth': self.average_depth
        }

================
File: argus_track/core/track.py
================
"""Track data structure"""

from dataclasses import dataclass, field
from typing import List, Optional
import numpy as np

from .detection import Detection


@dataclass
class Track:
    """Represents a tracked object through multiple frames"""
    track_id: int
    detections: List[Detection] = field(default_factory=list)
    kalman_filter: Optional['KalmanBoxTracker'] = None
    state: str = 'tentative'           # tentative, confirmed, lost, removed
    hits: int = 0                      # Number of successful updates
    age: int = 0                       # Total frames since creation
    time_since_update: int = 0         # Frames since last update
    start_frame: int = 0
    
    @property
    def is_confirmed(self) -> bool:
        """Check if track is confirmed (has enough hits)"""
        return self.state == 'confirmed'
    
    @property
    def is_active(self) -> bool:
        """Check if track is currently active"""
        return self.state in ['tentative', 'confirmed']
    
    def to_tlbr(self) -> np.ndarray:
        """Get current position in tlbr format"""
        if self.kalman_filter is None:
            return self.detections[-1].tlbr if self.detections else np.zeros(4)
        return self.kalman_filter.get_state()
    
    @property
    def last_detection(self) -> Optional[Detection]:
        """Get the most recent detection"""
        return self.detections[-1] if self.detections else None
    
    @property
    def trajectory(self) -> List[np.ndarray]:
        """Get trajectory as list of center points"""
        return [det.center for det in self.detections]
    
    def to_dict(self) -> dict:
        """Convert to dictionary representation"""
        return {
            'track_id': self.track_id,
            'state': self.state,
            'hits': self.hits,
            'age': self.age,
            'time_since_update': self.time_since_update,
            'start_frame': self.start_frame,
            'detections': [det.to_dict() for det in self.detections[-10:]]  # Last 10 detections
        }

================
File: argus_track/detectors/base.py
================
"""Base detector interface"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any
import numpy as np


class ObjectDetector(ABC):
    """Abstract base class for object detection modules"""
    
    @abstractmethod
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect objects in a frame
        
        Args:
            frame: Input image as numpy array
            
        Returns:
            List of detections with keys: bbox, score, class_name, class_id
        """
        pass
    
    @abstractmethod
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        pass
    
    def set_target_classes(self, target_classes: List[str]) -> None:
        """Set specific classes to detect"""
        self.target_classes = target_classes

================
File: argus_track/detectors/mock.py
================
"""Mock detector for testing"""

import numpy as np
from typing import List, Dict, Any
import random

from .base import ObjectDetector


class MockDetector(ObjectDetector):
    """Mock detector for testing purposes"""
    
    def __init__(self, target_classes: List[str] = None):
        """
        Initialize mock detector
        
        Args:
            target_classes: List of class names to detect
        """
        self.class_names = [
            'light_post', 'street_light', 'pole', 
            'traffic_light', 'stop_sign', 'person'
        ]
        self.target_classes = target_classes or self.class_names
        self.frame_count = 0
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Generate mock detections for testing
        
        Args:
            frame: Input image
            
        Returns:
            List of mock detections
        """
        h, w = frame.shape[:2]
        detections = []
        
        # Generate stable mock detections with slight variations
        base_positions = [
            (100, 100, 150, 300),
            (400, 120, 450, 320),
            (700, 90, 750, 290)
        ]
        
        for i, (x1, y1, x2, y2) in enumerate(base_positions):
            # Add some noise to make it more realistic
            noise = 5 * np.sin(self.frame_count * 0.1 + i)
            
            x1 += int(noise)
            y1 += int(noise * 0.5)
            x2 += int(noise)
            y2 += int(noise * 0.5)
            
            # Ensure bounds
            x1 = max(0, min(x1, w))
            x2 = max(0, min(x2, w))
            y1 = max(0, min(y1, h))
            y2 = max(0, min(y2, h))
            
            # Random class from target classes
            class_name = random.choice(self.target_classes)
            class_id = self.class_names.index(class_name) if class_name in self.class_names else 0
            
            detections.append({
                'bbox': [x1, y1, x2, y2],
                'score': 0.85 + random.uniform(-0.1, 0.1),
                'class_name': class_name,
                'class_id': class_id
            })
        
        self.frame_count += 1
        return detections
    
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        return self.class_names.copy()

================
File: argus_track/detectors/yolo.py
================
"""YOLO detector implementation"""

import cv2
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path

from .base import ObjectDetector


class YOLODetector(ObjectDetector):
    """YOLO-based object detector implementation"""
    
    def __init__(self, model_path: str, config_path: str, 
                 target_classes: Optional[List[str]] = None,
                 confidence_threshold: float = 0.5,
                 nms_threshold: float = 0.4):
        """
        Initialize YOLO detector
        
        Args:
            model_path: Path to YOLO weights
            config_path: Path to YOLO config
            target_classes: List of class names to detect (None for all)
            confidence_threshold: Minimum confidence for detections
            nms_threshold: Non-maximum suppression threshold
        """
        self.logger = logging.getLogger(f"{__name__}.YOLODetector")
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        
        try:
            # Initialize YOLO model (using OpenCV's DNN module)
            self.net = cv2.dnn.readNet(model_path, config_path)
            
            # Set backend preference (CUDA if available)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            
            # Get output layer names
            layer_names = self.net.getLayerNames()
            self.output_layers = [layer_names[i - 1] 
                                for i in self.net.getUnconnectedOutLayers()]
            
            # Load class names
            names_path = Path(config_path).with_suffix('.names')
            if names_path.exists():
                with open(names_path, 'r') as f:
                    self.class_names = [line.strip() for line in f.readlines()]
            else:
                self.logger.warning(f"Class names file not found: {names_path}")
                self.class_names = [f"class_{i}" for i in range(80)]  # Default COCO classes
            
            self.target_classes = target_classes or self.class_names
            self.logger.info(f"Initialized YOLO detector with {len(self.class_names)} classes")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize YOLO detector: {e}")
            raise
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect objects in frame using YOLO
        
        Args:
            frame: Input image
            
        Returns:
            List of detections
        """
        height, width = frame.shape[:2]
        
        # Prepare input
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), 
                                   True, crop=False)
        self.net.setInput(blob)
        
        # Run inference
        outputs = self.net.forward(self.output_layers)
        
        # Extract detections
        boxes = []
        confidences = []
        class_ids = []
        
        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                
                # Filter by confidence and target classes
                class_name = self.class_names[class_id]
                if confidence > self.confidence_threshold and class_name in self.target_classes:
                    # Convert YOLO format to pixel coordinates
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    
                    # Calculate bounding box
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        
        # Apply non-maximum suppression
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 
                                  self.confidence_threshold, 
                                  self.nms_threshold)
        
        # Format results
        detections = []
        if len(indexes) > 0:
            for i in indexes.flatten():
                x, y, w, h = boxes[i]
                detections.append({
                    'bbox': [x, y, x + w, y + h],  # Convert to tlbr format
                    'score': confidences[i],
                    'class_name': self.class_names[class_ids[i]],
                    'class_id': class_ids[i]
                })
        
        return detections
    
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        return self.class_names.copy()
    
    def set_backend(self, backend: str = 'cpu') -> None:
        """
        Set computation backend
        
        Args:
            backend: 'cpu', 'cuda', or 'opencl'
        """
        if backend.lower() == 'cuda':
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
        elif backend.lower() == 'opencl':
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)
        else:
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
        
        self.logger.info(f"Set backend to: {backend}")

================
File: argus_track/stereo/__init__.py
================
"""Stereo vision processing modules"""

from .matching import StereoMatcher
from .triangulation import StereoTriangulator
from .calibration import StereoCalibrationManager

__all__ = ["StereoMatcher", "StereoTriangulator", "StereoCalibrationManager"]

================
File: argus_track/stereo/calibration.py
================
# argus_track/stereo/calibration.py (NEW FILE)

"""Stereo camera calibration management"""

import cv2
import numpy as np
import pickle
from typing import Optional, Tuple, List
import logging
from pathlib import Path

from ..config import StereoCalibrationConfig


class StereoCalibrationManager:
    """
    Manages stereo camera calibration data and provides rectification utilities
    """
    
    def __init__(self, calibration: Optional[StereoCalibrationConfig] = None):
        """
        Initialize calibration manager
        
        Args:
            calibration: Pre-loaded calibration data
        """
        self.calibration = calibration
        self.logger = logging.getLogger(f"{__name__}.StereoCalibrationManager")
        
        # Rectification maps (computed when needed)
        self.left_map1 = None
        self.left_map2 = None
        self.right_map1 = None
        self.right_map2 = None
        
    @classmethod
    def from_pickle_file(cls, calibration_path: str) -> 'StereoCalibrationManager':
        """
        Load calibration from pickle file
        
        Args:
            calibration_path: Path to calibration pickle file
            
        Returns:
            StereoCalibrationManager instance
        """
        calibration = StereoCalibrationConfig.from_pickle(calibration_path)
        return cls(calibration)
    
    def compute_rectification_maps(self, 
                                  image_size: Optional[Tuple[int, int]] = None,
                                  alpha: float = 0.0) -> bool:
        """
        Compute rectification maps for stereo pair
        
        Args:
            image_size: (width, height) of images, uses calibration size if None
            alpha: Free scaling parameter (0=crop, 1=no crop)
            
        Returns:
            True if successful
        """
        if self.calibration is None:
            self.logger.error("No calibration data available")
            return False
        
        if image_size is None:
            image_size = (self.calibration.image_width, self.calibration.image_height)
        
        try:
            # Compute rectification transforms
            R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
                self.calibration.camera_matrix_left,
                self.calibration.dist_coeffs_left,
                self.calibration.camera_matrix_right,
                self.calibration.dist_coeffs_right,
                image_size,
                self.calibration.R,
                self.calibration.T,
                alpha=alpha
            )
            
            # Update calibration with computed matrices
            self.calibration.P1 = P1
            self.calibration.P2 = P2
            self.calibration.Q = Q
            
            # Compute rectification maps
            self.left_map1, self.left_map2 = cv2.initUndistortRectifyMap(
                self.calibration.camera_matrix_left,
                self.calibration.dist_coeffs_left,
                R1, P1, image_size,
                cv2.CV_32FC1
            )
            
            self.right_map1, self.right_map2 = cv2.initUndistortRectifyMap(
                self.calibration.camera_matrix_right,
                self.calibration.dist_coeffs_right,
                R2, P2, image_size,
                cv2.CV_32FC1
            )
            
            self.logger.info("Successfully computed rectification maps")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to compute rectification maps: {e}")
            return False
    
    def rectify_image_pair(self, 
                          left_image: np.ndarray, 
                          right_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Rectify stereo image pair
        
        Args:
            left_image: Left camera image
            right_image: Right camera image
            
        Returns:
            (rectified_left, rectified_right) images
        """
        if self.left_map1 is None or self.left_map2 is None:
            # Compute maps if not available
            image_size = (left_image.shape[1], left_image.shape[0])
            if not self.compute_rectification_maps(image_size):
                self.logger.warning("Using non-rectified images")
                return left_image, right_image
        
        # Apply rectification
        left_rectified = cv2.remap(
            left_image, self.left_map1, self.left_map2, cv2.INTER_LINEAR
        )
        right_rectified = cv2.remap(
            right_image, self.right_map1, self.right_map2, cv2.INTER_LINEAR
        )
        
        return left_rectified, right_rectified
    
    def validate_calibration(self) -> Tuple[bool, List[str]]:
        """
        Validate calibration data
        
        Returns:
            (is_valid, error_messages)
        """
        if self.calibration is None:
            return False, ["No calibration data loaded"]
        
        errors = []
        
        # Check camera matrices
        if self.calibration.camera_matrix_left.shape != (3, 3):
            errors.append("Invalid left camera matrix shape")
        
        if self.calibration.camera_matrix_right.shape != (3, 3):
            errors.append("Invalid right camera matrix shape")
        
        # Check distortion coefficients
        if len(self.calibration.dist_coeffs_left) < 4:
            errors.append("Invalid left distortion coefficients")
        
        if len(self.calibration.dist_coeffs_right) < 4:
            errors.append("Invalid right distortion coefficients")
        
        # Check rotation and translation
        if self.calibration.R.shape != (3, 3):
            errors.append("Invalid rotation matrix shape")
        
        if self.calibration.T.shape != (3, 1) and self.calibration.T.shape != (3,):
            errors.append("Invalid translation vector shape")
        
        # Check baseline
        if self.calibration.baseline <= 0:
            # Try to compute from translation vector
            if self.calibration.T.shape == (3, 1):
                baseline = float(np.linalg.norm(self.calibration.T))
            else:
                baseline = float(np.linalg.norm(self.calibration.T))
            
            if baseline <= 0:
                errors.append("Invalid baseline distance")
            else:
                self.calibration.baseline = baseline
                self.logger.info(f"Computed baseline: {baseline:.3f}m")
        
        # Check image dimensions
        if self.calibration.image_width <= 0 or self.calibration.image_height <= 0:
            errors.append("Invalid image dimensions")
        
        is_valid = len(errors) == 0
        
        if is_valid:
            self.logger.info("Calibration validation passed")
        else:
            self.logger.error(f"Calibration validation failed: {errors}")
        
        return is_valid, errors
    
    def get_calibration_summary(self) -> dict:
        """Get summary of calibration parameters"""
        if self.calibration is None:
            return {"status": "No calibration loaded"}
        
        return {
            "baseline": f"{self.calibration.baseline:.3f}m",
            "image_size": f"{self.calibration.image_width}x{self.calibration.image_height}",
            "left_focal_length": f"{self.calibration.camera_matrix_left[0,0]:.1f}px",
            "right_focal_length": f"{self.calibration.camera_matrix_right[0,0]:.1f}px",
            "has_rectification": self.calibration.P1 is not None,
            "has_maps": self.left_map1 is not None
        }
    
    def create_sample_calibration(self, 
                                 image_width: int = 1920,
                                 image_height: int = 1080,
                                 baseline: float = 0.12) -> StereoCalibrationConfig:
        """
        Create sample calibration for testing (GoPro Hero 11 approximate values)
        
        Args:
            image_width: Image width in pixels
            image_height: Image height in pixels
            baseline: Baseline distance in meters
            
        Returns:
            Sample calibration configuration
        """
        # Approximate GoPro Hero 11 parameters
        focal_length = 1400  # pixels
        cx = image_width / 2
        cy = image_height / 2
        
        # Camera matrices
        camera_matrix = np.array([
            [focal_length, 0, cx],
            [0, focal_length, cy],
            [0, 0, 1]
        ], dtype=np.float64)
        
        # Distortion coefficients (approximate for GoPro)
        dist_coeffs = np.array([-0.3, 0.1, 0, 0, 0], dtype=np.float64)
        
        # Stereo parameters (assuming cameras are aligned horizontally)
        R = np.eye(3, dtype=np.float64)  # No rotation between cameras
        T = np.array([[baseline], [0], [0]], dtype=np.float64)  # Horizontal translation
        
        calibration = StereoCalibrationConfig(
            camera_matrix_left=camera_matrix,
            camera_matrix_right=camera_matrix,
            dist_coeffs_left=dist_coeffs,
            dist_coeffs_right=dist_coeffs,
            R=R,
            T=T,
            baseline=baseline,
            image_width=image_width,
            image_height=image_height
        )
        
        self.logger.info(f"Created sample calibration with {baseline}m baseline")
        return calibration
    
    def save_calibration(self, output_path: str) -> bool:
        """
        Save calibration to pickle file
        
        Args:
            output_path: Path for output file
            
        Returns:
            True if successful
        """
        if self.calibration is None:
            self.logger.error("No calibration to save")
            return False
        
        try:
            self.calibration.save_pickle(output_path)
            self.logger.info(f"Saved calibration to {output_path}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to save calibration: {e}")
            return False

================
File: argus_track/stereo/triangulation.py
================
# argus_track/stereo/triangulation.py (NEW FILE)

"""3D triangulation and coordinate transformation"""

import cv2
import numpy as np
from typing import List, Tuple, Optional
import logging

from ..core.stereo import StereoDetection
from ..config import StereoCalibrationConfig
from ..core import GPSData
from ..utils.gps_utils import CoordinateTransformer, GeoLocation


class StereoTriangulator:
    """
    Handles 3D triangulation and coordinate system transformations
    from camera coordinates to world coordinates to GPS coordinates.
    """
    
    def __init__(self, 
                 calibration: StereoCalibrationConfig,
                 coordinate_transformer: Optional[CoordinateTransformer] = None):
        """
        Initialize triangulator
        
        Args:
            calibration: Stereo camera calibration
            coordinate_transformer: GPS coordinate transformer
        """
        self.calibration = calibration
        self.coordinate_transformer = coordinate_transformer
        self.logger = logging.getLogger(f"{__name__}.StereoTriangulator")
        
        # Camera extrinsics (if available)
        self.camera_position = None  # GPS position of camera
        self.camera_orientation = None  # Camera orientation relative to world
        
    def set_camera_pose(self, 
                       gps_position: GPSData, 
                       orientation_angles: Optional[Tuple[float, float, float]] = None):
        """
        Set camera pose for world coordinate transformation
        
        Args:
            gps_position: GPS position of the camera
            orientation_angles: (roll, pitch, yaw) in degrees
        """
        self.camera_position = gps_position
        if orientation_angles:
            self.camera_orientation = np.array(orientation_angles) * np.pi / 180  # Convert to radians
        
        # Update coordinate transformer
        if self.coordinate_transformer is None:
            self.coordinate_transformer = CoordinateTransformer(
                reference_lat=gps_position.latitude,
                reference_lon=gps_position.longitude
            )
    
    def triangulate_points(self, stereo_detections: List[StereoDetection]) -> List[np.ndarray]:
        """
        Triangulate 3D points from stereo detections
        
        Args:
            stereo_detections: List of stereo detection pairs
            
        Returns:
            List of 3D points in camera coordinate system
        """
        points_3d = []
        
        for stereo_det in stereo_detections:
            # Get 2D points
            left_point = stereo_det.left_detection.center
            right_point = stereo_det.right_detection.center
            
            # Triangulate
            point_3d = self._triangulate_single_point(left_point, right_point)
            points_3d.append(point_3d)
        
        return points_3d
    
    def _triangulate_single_point(self, 
                                 left_point: np.ndarray, 
                                 right_point: np.ndarray) -> np.ndarray:
        """Triangulate single 3D point from stereo pair"""
        
        # Prepare points for OpenCV triangulation
        left_pt = left_point.reshape(2, 1).astype(np.float32)
        right_pt = right_point.reshape(2, 1).astype(np.float32)
        
        # Use projection matrices if available
        if self.calibration.P1 is not None and self.calibration.P2 is not None:
            points_4d = cv2.triangulatePoints(
                self.calibration.P1,
                self.calibration.P2,
                left_pt,
                right_pt
            )
            
            # Convert from homogeneous coordinates
            if points_4d[3, 0] != 0:
                point_3d = points_4d[:3, 0] / points_4d[3, 0]
            else:
                point_3d = points_4d[:3, 0]
                
            return point_3d
        else:
            # Fallback triangulation using basic stereo geometry
            return self._basic_triangulation(left_point, right_point)
    
    def _basic_triangulation(self, left_point: np.ndarray, right_point: np.ndarray) -> np.ndarray:
        """Basic triangulation without projection matrices"""
        # Calculate disparity
        disparity = left_point[0] - right_point[0]
        
        if disparity <= 0:
            return np.array([0, 0, float('inf')])
        
        # Camera parameters
        fx = self.calibration.camera_matrix_left[0, 0]
        fy = self.calibration.camera_matrix_left[1, 1]
        cx = self.calibration.camera_matrix_left[0, 2]
        cy = self.calibration.camera_matrix_left[1, 2]
        baseline = self.calibration.baseline
        
        # Calculate depth
        depth = (baseline * fx) / disparity
        
        # Calculate 3D coordinates
        x = (left_point[0] - cx) * depth / fx
        y = (left_point[1] - cy) * depth / fy
        z = depth
        
        return np.array([x, y, z])
    
    def camera_to_world_coordinates(self, 
                                   camera_points: List[np.ndarray],
                                   gps_data: GPSData) -> List[np.ndarray]:
        """
        Transform camera coordinates to world coordinates
        
        Args:
            camera_points: 3D points in camera coordinate system
            gps_data: GPS data for camera pose
            
        Returns:
            3D points in world coordinate system
        """
        world_points = []
        
        for cam_point in camera_points:
            # Apply camera rotation and translation
            world_point = self._transform_camera_to_world(cam_point, gps_data)
            world_points.append(world_point)
        
        return world_points
    
    def _transform_camera_to_world(self, 
                                  camera_point: np.ndarray, 
                                  gps_data: GPSData) -> np.ndarray:
        """Transform single point from camera to world coordinates"""
        
        # If we have camera orientation, apply rotation
        if self.camera_orientation is not None:
            # Create rotation matrix from Euler angles
            roll, pitch, yaw = self.camera_orientation
            
            # Rotation matrices
            Rx = np.array([[1, 0, 0],
                          [0, np.cos(roll), -np.sin(roll)],
                          [0, np.sin(roll), np.cos(roll)]])
            
            Ry = np.array([[np.cos(pitch), 0, np.sin(pitch)],
                          [0, 1, 0],
                          [-np.sin(pitch), 0, np.cos(pitch)]])
            
            Rz = np.array([[np.cos(yaw), -np.sin(yaw), 0],
                          [np.sin(yaw), np.cos(yaw), 0],
                          [0, 0, 1]])
            
            # Combined rotation matrix
            R = Rz @ Ry @ Rx
            
            # Apply rotation
            world_point = R @ camera_point
        else:
            # Assume camera is level and facing forward
            # Simple transformation: camera Z -> world X, camera X -> world Y, camera Y -> world Z
            world_point = np.array([camera_point[2], camera_point[0], -camera_point[1]])
        
        return world_point
    
    def world_to_gps_coordinates(self, 
                                world_points: List[np.ndarray],
                                reference_gps: GPSData) -> List[GeoLocation]:
        """
        Convert world coordinates to GPS coordinates
        
        Args:
            world_points: 3D points in world coordinate system
            reference_gps: Reference GPS position
            
        Returns:
            List of GPS locations
        """
        if self.coordinate_transformer is None:
            self.coordinate_transformer = CoordinateTransformer(
                reference_lat=reference_gps.latitude,
                reference_lon=reference_gps.longitude
            )
        
        gps_locations = []
        
        for world_point in world_points:
            # Use X, Y coordinates for GPS conversion (ignore Z/altitude)
            local_x = world_point[0]
            local_y = world_point[1]
            
            # Convert to GPS
            lat, lon = self.coordinate_transformer.local_to_gps(local_x, local_y)
            
            # Create GeoLocation with estimated accuracy
            location = GeoLocation(
                latitude=lat,
                longitude=lon,
                accuracy=self._estimate_gps_accuracy(world_point),
                reliability=0.8,  # Base reliability for stereo triangulation
                timestamp=reference_gps.timestamp
            )
            
            gps_locations.append(location)
        
        return gps_locations
    
    def _estimate_gps_accuracy(self, world_point: np.ndarray) -> float:
        """Estimate GPS accuracy based on triangulation quality"""
        # Accuracy degrades with distance
        distance = np.linalg.norm(world_point)
        
        # Base accuracy (1m) + distance-dependent error
        base_accuracy = 1.0
        distance_error = distance * 0.01  # 1cm per meter of distance
        
        estimated_accuracy = base_accuracy + distance_error
        
        # Cap at reasonable maximum
        return min(estimated_accuracy, 10.0)
    
    def estimate_object_location(self, 
                                stereo_track: 'StereoTrack',
                                gps_history: List[GPSData]) -> Optional[GeoLocation]:
        """
        Estimate final GPS location for a static object track
        
        Args:
            stereo_track: Stereo track with 3D trajectory
            gps_history: GPS data history for the track
            
        Returns:
            Estimated GPS location or None
        """
        if not stereo_track.is_static_3d or len(gps_history) < 3:
            return None
        
        # Get all world coordinates for the track
        world_coords = stereo_track.world_trajectory
        
        if len(world_coords) < 3:
            return None
        
        # Calculate average world position
        avg_world_pos = np.mean(world_coords, axis=0)
        
        # Use middle GPS point as reference
        mid_gps = gps_history[len(gps_history) // 2]
        
        # Convert to GPS
        gps_locations = self.world_to_gps_coordinates([avg_world_pos], mid_gps)
        
        if gps_locations:
            location = gps_locations[0]
            
            # Calculate reliability based on trajectory consistency
            if len(world_coords) > 1:
                positions = np.array(world_coords)
                std_dev = np.std(positions, axis=0)
                max_std = np.max(std_dev)
                
                # High reliability if standard deviation is low
                reliability = 1.0 / (1.0 + max_std)
                location.reliability = min(1.0, max(0.1, reliability))
            
            return location
        
        return None
    
    def validate_triangulation(self, 
                              stereo_detection: StereoDetection,
                              max_depth: float = 100.0,
                              min_depth: float = 1.0) -> bool:
        """
        Validate triangulation result
        
        Args:
            stereo_detection: Stereo detection to validate
            max_depth: Maximum reasonable depth
            min_depth: Minimum reasonable depth
            
        Returns:
            True if triangulation is valid
        """
        depth = stereo_detection.depth
        
        # Check depth range
        if not (min_depth <= depth <= max_depth):
            return False
        
        # Check if 3D coordinates are reasonable
        world_coords = stereo_detection.world_coordinates
        
        # Check for NaN or infinite values
        if not np.all(np.isfinite(world_coords)):
            return False
        
        # Check if coordinates are within reasonable bounds
        max_coord = 1000.0  # 1km from camera
        if np.any(np.abs(world_coords) > max_coord):
            return False
        
        return True

================
File: argus_track/utils/__init__.py
================
"""Utility functions for ByteTrack system"""

from .iou import calculate_iou, calculate_iou_matrix
from .visualization import draw_tracks, create_track_overlay
from .io import save_tracking_results, load_gps_data, setup_logging
from .gps_utils import GPSInterpolator, CoordinateTransformer
from .performance import PerformanceMonitor, PerformanceMetrics
from .config_validator import ConfigValidator, ConfigLoader

__all__ = [
    "calculate_iou",
    "calculate_iou_matrix",
    "draw_tracks",
    "create_track_overlay",
    "save_tracking_results",
    "load_gps_data",
    "setup_logging",
    "GPSInterpolator",
    "CoordinateTransformer",
    "PerformanceMonitor",
    "PerformanceMetrics",
    "ConfigValidator",
    "ConfigLoader"
]

================
File: argus_track/utils/config_validator.py
================
"""Configuration validation utilities"""

from typing import Dict, List, Any, Optional
from dataclasses import fields
import yaml
import json
from pathlib import Path

from ..config import TrackerConfig, DetectorConfig, CameraConfig


class ConfigValidator:
    """Validate and sanitize configuration parameters"""
    
    @staticmethod
    def validate_tracker_config(config: TrackerConfig) -> List[str]:
        """
        Validate tracker configuration
        
        Args:
            config: TrackerConfig instance
            
        Returns:
            List of validation errors
        """
        errors = []
        
        # Validate thresholds
        if not 0 <= config.track_thresh <= 1:
            errors.append(f"track_thresh must be between 0 and 1, got {config.track_thresh}")
        
        if not 0 <= config.match_thresh <= 1:
            errors.append(f"match_thresh must be between 0 and 1, got {config.match_thresh}")
        
        # Validate buffer sizes
        if config.track_buffer < 1:
            errors.append(f"track_buffer must be at least 1, got {config.track_buffer}")
        
        if config.track_buffer > 300:
            errors.append(f"track_buffer is very large ({config.track_buffer}), this may cause memory issues")
        
        # Validate area threshold
        if config.min_box_area < 0:
            errors.append(f"min_box_area must be non-negative, got {config.min_box_area}")
        
        # Validate static detection parameters
        if config.static_threshold <= 0:
            errors.append(f"static_threshold must be positive, got {config.static_threshold}")
        
        if config.min_static_frames < 1:
            errors.append(f"min_static_frames must be at least 1, got {config.min_static_frames}")
        
        return errors
    
    @staticmethod
    def validate_detector_config(config: DetectorConfig) -> List[str]:
        """
        Validate detector configuration
        
        Args:
            config: DetectorConfig instance
            
        Returns:
            List of validation errors
        """
        errors = []
        
        # Check paths exist
        if not Path(config.model_path).exists():
            errors.append(f"Model path does not exist: {config.model_path}")
        
        if not Path(config.config_path).exists():
            errors.append(f"Config path does not exist: {config.config_path}")
        
        # Validate thresholds
        if not 0 <= config.confidence_threshold <= 1:
            errors.append(f"confidence_threshold must be between 0 and 1, got {config.confidence_threshold}")
        
        if not 0 <= config.nms_threshold <= 1:
            errors.append(f"nms_threshold must be between 0 and 1, got {config.nms_threshold}")
        
        # Validate target classes
        if config.target_classes is not None and not config.target_classes:
            errors.append("target_classes is empty, no objects will be detected")
        
        return errors
    
    @staticmethod
    def validate_camera_config(config: CameraConfig) -> List[str]:
        """
        Validate camera configuration
        
        Args:
            config: CameraConfig instance
            
        Returns:
            List of validation errors
        """
        errors = []
        
        # Validate camera matrix
        if len(config.camera_matrix) != 3 or len(config.camera_matrix[0]) != 3:
            errors.append("camera_matrix must be a 3x3 matrix")
        
        # Validate distortion coefficients
        if len(config.distortion_coeffs) < 4:
            errors.append("distortion_coeffs must have at least 4 elements")
        
        # Validate image dimensions
        if config.image_width <= 0:
            errors.append(f"image_width must be positive, got {config.image_width}")
        
        if config.image_height <= 0:
            errors.append(f"image_height must be positive, got {config.image_height}")
        
        return errors
    
    @staticmethod
    def sanitize_config(config_dict: Dict[str, Any], 
                       config_class: type) -> Dict[str, Any]:
        """
        Sanitize configuration dictionary
        
        Args:
            config_dict: Raw configuration dictionary
            config_class: Target configuration class
            
        Returns:
            Sanitized configuration dictionary
        """
        # Get valid field names
        valid_fields = {f.name for f in fields(config_class)}
        
        # Filter out invalid fields
        sanitized = {
            k: v for k, v in config_dict.items() 
            if k in valid_fields
        }
        
        # Add missing fields with defaults
        for field in fields(config_class):
            if field.name not in sanitized and field.default is not None:
                sanitized[field.name] = field.default
        
        return sanitized
    
    @staticmethod
    def merge_configs(base_config: Dict[str, Any], 
                     override_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge two configuration dictionaries
        
        Args:
            base_config: Base configuration
            override_config: Override configuration
            
        Returns:
            Merged configuration
        """
        merged = base_config.copy()
        
        for key, value in override_config.items():
            if isinstance(value, dict) and key in merged and isinstance(merged[key], dict):
                merged[key] = ConfigValidator.merge_configs(merged[key], value)
            else:
                merged[key] = value
        
        return merged


class ConfigLoader:
    """Load and validate configuration from various sources"""
    
    @staticmethod
    def load_from_file(filepath: str) -> Dict[str, Any]:
        """
        Load configuration from file
        
        Args:
            filepath: Path to configuration file
            
        Returns:
            Configuration dictionary
        """
        path = Path(filepath)
        
        if not path.exists():
            raise FileNotFoundError(f"Configuration file not found: {filepath}")
        
        if path.suffix in ['.yaml', '.yml']:
            with open(filepath, 'r') as f:
                return yaml.safe_load(f)
        elif path.suffix == '.json':
            with open(filepath, 'r') as f:
                return json.load(f)
        else:
            raise ValueError(f"Unsupported configuration format: {path.suffix}")
    
    @staticmethod
    def create_tracker_config(config_source: Optional[str] = None,
                            overrides: Optional[Dict[str, Any]] = None) -> TrackerConfig:
        """
        Create validated TrackerConfig
        
        Args:
            config_source: Path to configuration file
            overrides: Dictionary of override values
            
        Returns:
            Validated TrackerConfig instance
        """
        # Load base configuration
        if config_source:
            config_dict = ConfigLoader.load_from_file(config_source)
        else:
            config_dict = {}
        
        # Apply overrides
        if overrides:
            config_dict = ConfigValidator.merge_configs(config_dict, overrides)
        
        # Sanitize configuration
        config_dict = ConfigValidator.sanitize_config(config_dict, TrackerConfig)
        
        # Create config instance
        config = TrackerConfig(**config_dict)
        
        # Validate
        errors = ConfigValidator.validate_tracker_config(config)
        if errors:
            raise ValueError(f"Configuration validation failed: {'; '.join(errors)}")
        
        return config

================
File: argus_track/utils/gps_utils.py
================
# argus_track/utils/gps_utils.py

"""Enhanced GPS utilities for tracking"""

import numpy as np
from typing import List, Tuple, Optional, Dict
from scipy.interpolate import interp1d
import pyproj
from dataclasses import dataclass

from ..core import GPSData


class GPSInterpolator:
    """Interpolate GPS data between frames"""
    
    def __init__(self, gps_data: List[GPSData]):
        """
        Initialize GPS interpolator
        
        Args:
            gps_data: List of GPS data points
        """
        self.gps_data = sorted(gps_data, key=lambda x: x.timestamp)
        self.timestamps = np.array([gps.timestamp for gps in self.gps_data])
        
        # Create interpolation functions
        self.lat_interp = interp1d(
            self.timestamps,
            [gps.latitude for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
        self.lon_interp = interp1d(
            self.timestamps,
            [gps.longitude for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
        self.heading_interp = interp1d(
            self.timestamps,
            [gps.heading for gps in self.gps_data],
            kind='linear',
            fill_value='extrapolate'
        )
    
    def interpolate(self, timestamp: float) -> GPSData:
        """
        Interpolate GPS data for a specific timestamp
        
        Args:
            timestamp: Target timestamp
            
        Returns:
            Interpolated GPS data
        """
        return GPSData(
            timestamp=timestamp,
            latitude=float(self.lat_interp(timestamp)),
            longitude=float(self.lon_interp(timestamp)),
            altitude=0.0,  # We're not focusing on altitude
            heading=float(self.heading_interp(timestamp)),
            accuracy=1.0  # Interpolated accuracy
        )
    
    def get_range(self) -> Tuple[float, float]:
        """Get timestamp range of GPS data"""
        return self.timestamps[0], self.timestamps[-1]


class CoordinateTransformer:
    """Transform between GPS coordinates and local coordinate systems"""
    
    def __init__(self, reference_lat: float, reference_lon: float):
        """
        Initialize transformer with reference point
        
        Args:
            reference_lat: Reference latitude
            reference_lon: Reference longitude
        """
        self.reference_lat = reference_lat
        self.reference_lon = reference_lon
        
        # Setup projections
        self.wgs84 = pyproj.CRS("EPSG:4326")  # GPS coordinates
        self.utm = pyproj.CRS(f"EPSG:{self._get_utm_zone()}")
        self.transformer = pyproj.Transformer.from_crs(
            self.wgs84, self.utm, always_xy=True
        )
        self.inverse_transformer = pyproj.Transformer.from_crs(
            self.utm, self.wgs84, always_xy=True
        )
        
        # Calculate reference point in UTM
        self.ref_x, self.ref_y = self.transformer.transform(
            reference_lon, reference_lat
        )
    
    def _get_utm_zone(self) -> int:
        """Get UTM zone for reference point"""
        zone = int((self.reference_lon + 180) / 6) + 1
        if self.reference_lat >= 0:
            return 32600 + zone  # Northern hemisphere
        else:
            return 32700 + zone  # Southern hemisphere
    
    def gps_to_local(self, lat: float, lon: float) -> Tuple[float, float]:
        """
        Convert GPS coordinates to local coordinate system
        
        Args:
            lat: Latitude
            lon: Longitude
            
        Returns:
            (x, y) in meters from reference point
        """
        utm_x, utm_y = self.transformer.transform(lon, lat)
        return utm_x - self.ref_x, utm_y - self.ref_y
    
    def local_to_gps(self, x: float, y: float) -> Tuple[float, float]:
        """
        Convert local coordinates to GPS
        
        Args:
            x: X coordinate in meters from reference
            y: Y coordinate in meters from reference
            
        Returns:
            (latitude, longitude)
        """
        utm_x = x + self.ref_x
        utm_y = y + self.ref_y
        lon, lat = self.inverse_transformer.transform(utm_x, utm_y)
        return lat, lon
    
    def distance(self, lat1: float, lon1: float, 
                 lat2: float, lon2: float) -> float:
        """
        Calculate distance between two GPS points
        
        Args:
            lat1, lon1: First point
            lat2, lon2: Second point
            
        Returns:
            Distance in meters
        """
        x1, y1 = self.gps_to_local(lat1, lon1)
        x2, y2 = self.gps_to_local(lat2, lon2)
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)


@dataclass
class GeoLocation:
    """Represents a geographic location with reliability information"""
    latitude: float
    longitude: float
    accuracy: float = 1.0  # Accuracy in meters
    reliability: float = 1.0  # Value between 0 and 1
    timestamp: Optional[float] = None


def sync_gps_with_frames(gps_data: List[GPSData], 
                         video_fps: float,
                         start_timestamp: Optional[float] = None) -> List[GPSData]:
    """
    Synchronize GPS data with video frames
    
    Args:
        gps_data: List of GPS data points
        video_fps: Video frame rate
        start_timestamp: Optional start timestamp
        
    Returns:
        List of GPS data aligned with frames
    """
    if not gps_data:
        return []
    
    # Sort GPS data by timestamp
    gps_data = sorted(gps_data, key=lambda x: x.timestamp)
    
    # Determine start timestamp
    if start_timestamp is None:
        start_timestamp = gps_data[0].timestamp
    
    # Create interpolator
    interpolator = GPSInterpolator(gps_data)
    
    # Generate frame-aligned GPS data
    frame_gps = []
    frame_duration = 1.0 / video_fps
    
    timestamp = start_timestamp
    while timestamp <= gps_data[-1].timestamp:
        frame_gps.append(interpolator.interpolate(timestamp))
        timestamp += frame_duration
    
    return frame_gps


def compute_average_location(locations: List[GPSData]) -> GeoLocation:
    """
    Compute the average location from multiple GPS points
    
    Args:
        locations: List of GPS data points
        
    Returns:
        Average location with reliability score
    """
    if not locations:
        return GeoLocation(0.0, 0.0, 0.0, 0.0)
    
    # Simple weighted average based on accuracy
    weights = np.array([1.0 / max(loc.accuracy, 0.1) for loc in locations])
    weights = weights / np.sum(weights)  # Normalize
    
    avg_lat = np.sum([loc.latitude * w for loc, w in zip(locations, weights)])
    avg_lon = np.sum([loc.longitude * w for loc, w in zip(locations, weights)])
    
    # Calculate reliability based on consistency of points
    if len(locations) > 1:
        # Create transformer using the first point as reference
        transformer = CoordinateTransformer(locations[0].latitude, locations[0].longitude)
        
        # Calculate standard deviation in meters
        distances = []
        for loc in locations:
            dist = transformer.distance(loc.latitude, loc.longitude, avg_lat, avg_lon)
            distances.append(dist)
        
        std_dev = np.std(distances)
        reliability = 1.0 / (1.0 + std_dev / 10.0)  # Decreases with higher standard deviation
        reliability = min(1.0, max(0.1, reliability))  # Clamp between 0.1 and 1.0
    else:
        reliability = 0.5  # Only one point, medium reliability
    
    # Average accuracy is the weighted average of individual accuracies
    avg_accuracy = np.sum([loc.accuracy * w for loc, w in zip(locations, weights)])
    
    # Use the latest timestamp
    latest_timestamp = max([loc.timestamp for loc in locations])
    
    return GeoLocation(
        latitude=avg_lat,
        longitude=avg_lon,
        accuracy=avg_accuracy,
        reliability=reliability,
        timestamp=latest_timestamp
    )


def filter_gps_outliers(locations: List[GPSData], 
                       threshold_meters: float = 30.0) -> List[GPSData]:
    """
    Filter outliers from GPS data using DBSCAN clustering
    
    Args:
        locations: List of GPS data points
        threshold_meters: Distance threshold for outlier detection
        
    Returns:
        Filtered list of GPS data points
    """
    if len(locations) <= 2:
        return locations
    
    from sklearn.cluster import DBSCAN
    
    # Create transformer using the first point as reference
    transformer = CoordinateTransformer(locations[0].latitude, locations[0].longitude)
    
    # Convert to local coordinates
    local_points = []
    for loc in locations:
        x, y = transformer.gps_to_local(loc.latitude, loc.longitude)
        local_points.append([x, y])
    
    # Cluster points
    clustering = DBSCAN(eps=threshold_meters, min_samples=1).fit(local_points)
    
    # Find the largest cluster
    labels = clustering.labels_
    unique_labels, counts = np.unique(labels, return_counts=True)
    largest_cluster = unique_labels[np.argmax(counts)]
    
    # Keep only points from the largest cluster
    filtered_locations = [loc for i, loc in enumerate(locations) if labels[i] == largest_cluster]
    
    return filtered_locations

================
File: argus_track/utils/io.py
================
# argus_track/utils/io.py

"""I/O utilities for loading and saving tracking data"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import csv

import numpy as np

from ..core import GPSData, Track


def setup_logging(log_file: Optional[str] = None, 
                 level: int = logging.INFO) -> None:
    """
    Setup logging configuration
    
    Args:
        log_file: Optional log file path
        level: Logging level
    """
    handlers = [logging.StreamHandler()]
    
    if log_file:
        handlers.append(logging.FileHandler(log_file))
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


def save_tracking_results(tracks: Dict[int, List[Dict]], 
                         output_path: Path,
                         metadata: Optional[Dict[str, Any]] = None,
                         gps_tracks: Optional[Dict[int, List[GPSData]]] = None,
                         track_locations: Optional[Dict[int, Dict]] = None) -> None:
    """
    Save tracking results to JSON file
    
    Args:
        tracks: Dictionary of track histories
        output_path: Path for output file
        metadata: Optional metadata to include
        gps_tracks: Optional GPS data for tracks
        track_locations: Optional estimated locations for tracks
    """
    results = {
        'metadata': metadata or {},
        'tracks': tracks
    }
    
    # Add GPS data if provided
    if gps_tracks:
        results['gps_tracks'] = {
            track_id: [gps.to_dict() for gps in gps_list]
            for track_id, gps_list in gps_tracks.items()
        }
        
    # Add track locations if provided
    if track_locations:
        results['track_locations'] = track_locations
    
    # Save to file
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    logging.info(f"Saved tracking results to {output_path}")


def load_tracking_results(input_path: Path) -> Dict[str, Any]:
    """
    Load tracking results from JSON file
    
    Args:
        input_path: Path to input file
        
    Returns:
        Dictionary with tracking results
    """
    with open(input_path, 'r') as f:
        results = json.load(f)
    
    logging.info(f"Loaded tracking results from {input_path}")
    return results


def load_gps_data(gps_file: str) -> List[GPSData]:
    """
    Load GPS data from file
    
    Args:
        gps_file: Path to GPS data file (CSV format)
        
    Returns:
        List of GPS data points
    """
    gps_data = []
    
    with open(gps_file, 'r') as f:
        reader = csv.reader(f)
        # Skip header if exists
        header = next(reader, None)
        
        try:
            for row in reader:
                if len(row) >= 5:
                    gps_data.append(GPSData(
                        timestamp=float(row[0]),
                        latitude=float(row[1]),
                        longitude=float(row[2]),
                        altitude=float(row[3]) if len(row) > 3 else 0.0,
                        heading=float(row[4]) if len(row) > 4 else 0.0,
                        accuracy=float(row[5]) if len(row) > 5 else 1.0
                    ))
        except ValueError as e:
            logging.error(f"Error parsing GPS data: {e}")
            logging.error(f"Problematic row: {row}")
            raise
    
    logging.info(f"Loaded {len(gps_data)} GPS data points from {gps_file}")
    return gps_data


def save_gps_data(gps_data: List[GPSData], output_path: str) -> None:
    """
    Save GPS data to CSV file
    
    Args:
        gps_data: List of GPS data points
        output_path: Path for output file
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # Write header
        writer.writerow(['timestamp', 'latitude', 'longitude', 
                        'altitude', 'heading', 'accuracy'])
        
        # Write data
        for gps in gps_data:
            writer.writerow([
                gps.timestamp,
                gps.latitude,
                gps.longitude,
                gps.altitude,
                gps.heading,
                gps.accuracy
            ])
    
    logging.info(f"Saved {len(gps_data)} GPS data points to {output_path}")


def export_locations_to_csv(track_locations: Dict[int, Dict],
                           output_path: str) -> None:
    """
    Export estimated track locations to CSV
    
    Args:
        track_locations: Dictionary of track locations
        output_path: Output CSV path
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['track_id', 'latitude', 'longitude', 
                         'accuracy', 'reliability', 'timestamp'])
        
        for track_id, location in track_locations.items():
            writer.writerow([
                track_id,
                location['latitude'],
                location['longitude'],
                location.get('accuracy', 1.0),
                location.get('reliability', 1.0),
                location.get('timestamp', '')
            ])
    
    logging.info(f"Exported {len(track_locations)} locations to CSV: {output_path}")


def export_tracks_to_csv(tracks: Dict[int, Track], 
                        output_path: str) -> None:
    """
    Export track data to CSV format
    
    Args:
        tracks: Dictionary of tracks
        output_path: Path for output CSV file
    """
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # Write header
        writer.writerow(['track_id', 'frame', 'x1', 'y1', 'x2', 'y2', 
                        'state', 'hits', 'age'])
        
        # Write track data
        for track_id, track in tracks.items():
            for detection in track.detections:
                x1, y1, x2, y2 = detection.tlbr
                writer.writerow([
                    track_id,
                    detection.frame_id,
                    x1, y1, x2, y2,
                    track.state,
                    track.hits,
                    track.age
                ])
    
    logging.info(f"Exported tracks to CSV: {output_path}")


def load_config_from_file(config_path: str) -> Dict[str, Any]:
    """
    Load configuration from YAML or JSON file
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Configuration dictionary
    """
    path = Path(config_path)
    
# argus_track/utils/io.py (continued)

    if path.suffix == '.yaml' or path.suffix == '.yml':
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    elif path.suffix == '.json':
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        raise ValueError(f"Unsupported config file format: {path.suffix}")
    
    logging.info(f"Loaded configuration from {config_path}")
    return config


def export_to_geojson(track_locations: Dict[int, Dict], 
                     output_path: str,
                     properties: Optional[Dict[int, Dict]] = None) -> None:
    """
    Export track locations to GeoJSON format
    
    Args:
        track_locations: Dictionary of track locations
        output_path: Path for output GeoJSON file
        properties: Optional additional properties for each feature
    """
    features = []
    
    for track_id, location in track_locations.items():
        # Create basic properties
        feature_props = {
            'track_id': track_id,
            'accuracy': location.get('accuracy', 1.0),
            'reliability': location.get('reliability', 1.0)
        }
        
        # Add additional properties if provided
        if properties and track_id in properties:
            feature_props.update(properties[track_id])
            
        feature = {
            'type': 'Feature',
            'geometry': {
                'type': 'Point',
                'coordinates': [location['longitude'], location['latitude']]
            },
            'properties': feature_props
        }
        
        features.append(feature)
    
    geojson = {
        'type': 'FeatureCollection',
        'features': features
    }
    
    with open(output_path, 'w') as f:
        json.dump(geojson, f, indent=2)
    
    logging.info(f"Exported {len(features)} locations to GeoJSON: {output_path}")

================
File: argus_track/utils/iou.py
================
# argus_track/utils/iou.py

"""IoU (Intersection over Union) utilities for tracking"""

import numpy as np
from typing import List, Union
from numba import jit

from ..core import Track, Detection


@jit(nopython=True)
def calculate_iou_jit(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    """
    Calculate IoU between two bounding boxes (numba accelerated)
    
    Args:
        bbox1: First bbox in [x1, y1, x2, y2] format
        bbox2: Second bbox in [x1, y1, x2, y2] format
        
    Returns:
        IoU value between 0 and 1
    """
    # Get intersection coordinates
    x1 = max(bbox1[0], bbox2[0])
    y1 = max(bbox1[1], bbox2[1])
    x2 = min(bbox1[2], bbox2[2])
    y2 = min(bbox1[3], bbox2[3])
    
    # Calculate intersection area
    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)
    
    # Calculate union area
    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
    bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
    union_area = bbox1_area + bbox2_area - intersection_area
    
    # Avoid division by zero
    if union_area == 0:
        return 0.0
    
    return intersection_area / union_area


def calculate_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    """
    Calculate IoU between two bounding boxes
    
    Args:
        bbox1: First bbox in [x1, y1, x2, y2] format
        bbox2: Second bbox in [x1, y1, x2, y2] format
        
    Returns:
        IoU value between 0 and 1
    """
    return calculate_iou_jit(bbox1, bbox2)


@jit(nopython=True)
def calculate_iou_matrix_jit(bboxes1: np.ndarray, bboxes2: np.ndarray) -> np.ndarray:
    """
    Calculate IoU matrix between two sets of bounding boxes (numba accelerated)
    
    Args:
        bboxes1: First set of bboxes in [N, 4] format
        bboxes2: Second set of bboxes in [M, 4] format
        
    Returns:
        IoU matrix of shape [N, M]
    """
    n_bbox1 = bboxes1.shape[0]
    n_bbox2 = bboxes2.shape[0]
    iou_matrix = np.zeros((n_bbox1, n_bbox2))
    
    for i in range(n_bbox1):
        for j in range(n_bbox2):
            iou_matrix[i, j] = calculate_iou_jit(bboxes1[i], bboxes2[j])
    
    return iou_matrix


def calculate_iou_matrix(tracks_or_bboxes1: Union[List[Track], np.ndarray], 
                         detections_or_bboxes2: Union[List[Detection], np.ndarray]) -> np.ndarray:
    """
    Calculate IoU matrix between tracks and detections
    
    Args:
        tracks_or_bboxes1: List of tracks or array of bboxes
        detections_or_bboxes2: List of detections or array of bboxes
        
    Returns:
        IoU matrix of shape (len(tracks_or_bboxes1), len(detections_or_bboxes2))
    """
    # Handle different input types
    if isinstance(tracks_or_bboxes1, np.ndarray):
        bboxes1 = tracks_or_bboxes1
    else:
        bboxes1 = np.array([track.to_tlbr() for track in tracks_or_bboxes1])
    
    if isinstance(detections_or_bboxes2, np.ndarray):
        bboxes2 = detections_or_bboxes2
    else:
        bboxes2 = np.array([det.tlbr for det in detections_or_bboxes2])
    
    # Calculate IoU matrix
    return calculate_iou_matrix_jit(bboxes1, bboxes2)

================
File: argus_track/__version__.py
================
"""Version information for ByteTrack Light Post Tracking System"""

__version__ = "1.0.0"
__author__ = "Light Post Tracking Team"
__email__ = "joaquin.olivera@gmial.com"
__description__ = "ByteTrack implementation optimized for light post tracking with GPS integration"

================
File: argus_track/exceptions.py
================
"""Custom exceptions for Argus Track"""


class ArgusTrackError(Exception):
    """Base exception for Argus Track"""
    pass


class DetectorError(ArgusTrackError):
    """Raised when detector operations fail"""
    pass


class TrackerError(ArgusTrackError):
    """Raised when tracker operations fail"""
    pass


class ConfigurationError(ArgusTrackError):
    """Raised when configuration is invalid"""
    pass


class GPSError(ArgusTrackError):
    """Raised when GPS operations fail"""
    pass


class VideoError(ArgusTrackError):
    """Raised when video processing fails"""
    pass

================
File: argus_track/requirements.txt
================
# argus_track/requirements.txt (UPDATED WITH GPS EXTRACTION)

# Core dependencies
numpy>=1.19.0
scipy>=1.5.0
opencv-python>=4.5.0
filterpy>=1.4.5
numba>=0.53.0  # For JIT compilation

# PyTorch for YOLOv11 support
torch>=1.9.0
torchvision>=0.10.0
torchaudio>=0.9.0

# YOLOv11 specific
ultralytics>=8.0.0  # For YOLOv11 support

# Optional optimizations
lap>=0.4.0  # Faster Hungarian algorithm

# Visualization
matplotlib>=3.3.0
seaborn>=0.11.0

# Development dependencies
pytest>=6.0.0
pytest-benchmark>=3.4.0
black>=21.0
flake8>=3.9.0
mypy>=0.910

# Documentation
sphinx>=4.0.0
sphinx-rtd-theme>=0.5.0

# GPS support
pyproj>=3.0.0  # For GPS coordinate transformations
scikit-learn>=0.24.0  # For clustering in static analysis
pynvml>=11.0.0  # Optional: For GPU monitoring

# GPS visualization
folium>=0.12.0  # For interactive maps
geojson>=2.5.0  # For GeoJSON export

# Stereo vision and 3D processing
transforms3d>=0.3.1  # For 3D transformations
open3d>=0.13.0  # Optional: For 3D visualization

# Configuration and data handling
pyyaml>=5.4.0  # For YAML configuration files
pandas>=1.3.0  # For data analysis and CSV handling

# Image processing enhancements
Pillow>=8.0.0  # Enhanced image processing
scikit-image>=0.18.0  # Additional image processing algorithms

# GPS EXTRACTION DEPENDENCIES
# ============================

# HTML/XML parsing for GPS metadata
beautifulsoup4>=4.9.0  # For parsing GPS metadata from ExifTool output
lxml>=4.6.0  # XML parser backend for BeautifulSoup

# GPS metadata handling
GPSPhoto>=2.2.0  # For reading/writing GPS metadata in images (optional)

# GoPro telemetry extraction (optional but recommended)
gopro-overlay>=0.10.0  # For GoPro telemetry data extraction

# ExifTool integration (ExifTool must be installed separately)
# Note: ExifTool (https://exiftool.org/) must be installed on the system
# Windows: Download from https://exiftool.org/
# macOS: brew install exiftool
# Linux: sudo apt-get install libimage-exiftool-perl

# Video processing enhancements
ffmpeg-python>=0.2.0  # For video metadata extraction (alternative method)

# Time and date handling
python-dateutil>=2.8.0  # Enhanced date parsing

# Process management
psutil>=5.8.0  # For system monitoring during processing

================
File: argus_track/analysis/__init__.py
================
from .static_analyzer import StaticObjectAnalyzer

__all__ = ['StaticObjectAnalyzer']

================
File: argus_track/detectors/__init__.py
================
"""Object detectors for ByteTrack system"""

from .base import ObjectDetector
from .yolo import YOLODetector
from .mock import MockDetector
from .yolov11 import YOLOv11Detector

__all__ = ["ObjectDetector", "YOLODetector", "MockDetector", "YOLOv11Detector"]

================
File: argus_track/filters/__init__.py
================
"""Motion filters for tracking"""
from .kalman import KalmanBoxTracker, batch_predict_kalman

__all__ = ["KalmanBoxTracker", "batch_predict_kalman"]

================
File: argus_track/filters/kalman.py
================
# argus_track/filters/kalman.py

"""Kalman filter implementation for object tracking"""

import numpy as np
from filterpy.kalman import KalmanFilter
from typing import List, Optional

from ..core import Detection


class KalmanBoxTracker:
    """
    Kalman filter implementation optimized for static/slow-moving objects
    
    State vector: [x, y, w, h, vx, vy, vw, vh]
    where (x, y) is center position, (w, h) is width/height,
    and v* are the corresponding velocities
    """
    
    def __init__(self, initial_detection: Detection):
        """
        Initialize Kalman filter with detection
        
        Args:
            initial_detection: First detection to initialize the filter
        """
        # 8-dimensional state, 4-dimensional measurement
        self.kf = KalmanFilter(dim_x=8, dim_z=4)
        
        # State transition matrix (constant velocity model)
        self.kf.F = np.array([
            [1, 0, 0, 0, 1, 0, 0, 0],  # x = x + vx
            [0, 1, 0, 0, 0, 1, 0, 0],  # y = y + vy
            [0, 0, 1, 0, 0, 0, 1, 0],  # w = w + vw
            [0, 0, 0, 1, 0, 0, 0, 1],  # h = h + vh
            [0, 0, 0, 0, 1, 0, 0, 0],  # vx = vx
            [0, 0, 0, 0, 0, 1, 0, 0],  # vy = vy
            [0, 0, 0, 0, 0, 0, 1, 0],  # vw = vw
            [0, 0, 0, 0, 0, 0, 0, 1]   # vh = vh
        ])
        
        # Measurement matrix (we only measure position and size)
        self.kf.H = np.array([
            [1, 0, 0, 0, 0, 0, 0, 0],  # x
            [0, 1, 0, 0, 0, 0, 0, 0],  # y
            [0, 0, 1, 0, 0, 0, 0, 0],  # w
            [0, 0, 0, 1, 0, 0, 0, 0]   # h
        ])
        
        xywh = initial_detection.xywh
        self.kf.x[0] = xywh[0]  # center x
        self.kf.x[1] = xywh[1]  # center y
        self.kf.x[2] = xywh[2]  # width
        self.kf.x[3] = xywh[3]  # height
        self.kf.x[4:] = 0       # Zero initial velocity (static assumption)
        
        # Initial uncertainty (higher for velocities)
        self.kf.P[4:, 4:] *= 1000  # High uncertainty for velocities
        self.kf.P[:4, :4] *= 10    # Lower uncertainty for position
        
        # Process noise (very low for static objects)
        self.kf.Q[4:, 4:] *= 0.01  # Minimal velocity changes expected
        self.kf.Q[:4, :4] *= 0.1   # Small position changes expected
        
        # Measurement noise
        self.kf.R *= 10.0
        
        self.time_since_update = 0
        self.history: List[Detection] = []
        self.hits = 1
        self.age = 1
        
    def predict(self) -> np.ndarray:
        """
        Predict next state
        
        Returns:
            Predicted bounding box in tlbr format
        """
        # Handle numerical stability
        if np.trace(self.kf.P[4:, 4:]) > 1e4:
            self.kf.P[4:, 4:] *= 0.01
            
        self.kf.predict()
        self.age += 1
        self.time_since_update += 1
        
        return self.get_state()
    
    def update(self, detection: Detection) -> None:
        """
        Update filter with new detection
        
        Args:
            detection: New detection measurement
        """
        self.time_since_update = 0
        self.history.append(detection)
        self.hits += 1
        
        # Perform Kalman update
        self.kf.update(detection.xywh)

    def get_state(self) -> np.ndarray:
        """
        Get current state estimate
        
        Returns:
            Bounding box in tlbr format
        """
        x, y, w, h = self.kf.x[:4].flatten()  # Add .flatten() to fix shape
        return np.array([
            x - w/2,  # x1
            y - h/2,  # y1
            x + w/2,  # x2
            y + h/2   # y2
        ])
    
    def get_velocity(self) -> np.ndarray:
        """
        Get current velocity estimate
        
        Returns:
            Velocity vector [vx, vy]
        """
        return self.kf.x[4:6]


def batch_predict_kalman(kalman_trackers: List[KalmanBoxTracker]) -> np.ndarray:
    """
    Batch prediction for multiple Kalman filters
    
    Args:
        kalman_trackers: List of KalmanBoxTracker instances
        
    Returns:
        Array of predicted bounding boxes in tlbr format
    """
    if not kalman_trackers:
        return np.array([])
    
    # Collect predicted states
    predictions = np.zeros((len(kalman_trackers), 4))
    
    for i, tracker in enumerate(kalman_trackers):
        # Predict and get state
        tracker.predict()
        predictions[i] = tracker.get_state()
    
    return predictions

================
File: argus_track/stereo/matching.py
================
"""Stereo matching for object detections"""

import cv2
import numpy as np
from typing import List, Tuple, Optional, Dict
from scipy.optimize import linear_sum_assignment
import logging

from ..core import Detection
from ..core.stereo import StereoDetection
from ..config import StereoCalibrationConfig
from ..utils.iou import calculate_iou


class StereoMatcher:
    """
    Matches detections between left and right camera views using epipolar constraints
    and appearance similarity for robust stereo correspondence.
    """
    
    def __init__(self, 
                 calibration: StereoCalibrationConfig,
                 max_disparity: float = 1000.0,
                 min_disparity: float = -50.0,
                 epipolar_threshold: float = 16.0,
                 iou_threshold: float = 0.0,
                 cost_threshold: float = 0.8):  # Added cost threshold parameter
        """
        Initialize stereo matcher
        
        Args:
            calibration: Stereo camera calibration
            max_disparity: Maximum disparity in pixels
            min_disparity: Minimum disparity in pixels
            epipolar_threshold: Maximum distance from epipolar line in pixels
            iou_threshold: Minimum IoU for detection matching
            cost_threshold: Maximum matching cost to accept (lower is better)
        """
        self.calibration = calibration
        self.max_disparity = max_disparity
        self.min_disparity = min_disparity
        self.epipolar_threshold = epipolar_threshold
        self.iou_threshold = iou_threshold
        self.cost_threshold = cost_threshold  # Added this line
        self.logger = logging.getLogger(f"{__name__}.StereoMatcher")
        
        # Precompute rectification maps if available
        self.has_rectification = (calibration.P1 is not None and 
                                calibration.P2 is not None and 
                                calibration.Q is not None)
        
        if self.has_rectification:
            self.logger.info("Using rectified stereo matching")
        else:
            self.logger.info("Using unrectified stereo matching with epipolar constraints")
    
    def match_detections(self, 
                        left_detections: List[Detection], 
                        right_detections: List[Detection]) -> List[StereoDetection]:
        """
        Match detections between left and right cameras
        
        Args:
            left_detections: Detections from left camera
            right_detections: Detections from right camera
            
        Returns:
            List of stereo detection pairs
        """
        if not left_detections or not right_detections:
            return []
        
        # Calculate matching costs
        cost_matrix = self._calculate_matching_costs(left_detections, right_detections)
        
        # Apply Hungarian algorithm for optimal matching
        row_indices, col_indices = linear_sum_assignment(cost_matrix)
        
        stereo_detections = []
        
        for left_idx, right_idx in zip(row_indices, col_indices):
            cost = cost_matrix[left_idx, right_idx]
            
            # Filter matches by cost threshold
            if cost < 1.0:  # Cost of 1.0 means no valid match
                left_det = left_detections[left_idx]
                right_det = right_detections[right_idx]
                
                # Calculate disparity and depth
                disparity = self._calculate_disparity(left_det, right_det)
                depth = self._estimate_depth(disparity)
                
                # Calculate 3D world coordinates
                world_coords = self._triangulate_point(left_det, right_det)
                
                # Calculate stereo confidence
                confidence = self._calculate_stereo_confidence(left_det, right_det, cost)
                
                stereo_detection = StereoDetection(
                    left_detection=left_det,
                    right_detection=right_det,
                    disparity=disparity,
                    depth=depth,
                    world_coordinates=world_coords,
                    stereo_confidence=confidence
                )
                
                stereo_detections.append(stereo_detection)
        
        self.logger.debug(f"Matched {len(stereo_detections)} stereo pairs from "
                         f"{len(left_detections)} left and {len(right_detections)} right detections")
        
        return stereo_detections
    
    def _calculate_matching_costs(self, 
                                 left_detections: List[Detection], 
                                 right_detections: List[Detection]) -> np.ndarray:
        """Calculate cost matrix for detection matching"""
        n_left = len(left_detections)
        n_right = len(right_detections)
        cost_matrix = np.ones((n_left, n_right))  # Initialize with high cost
        
        for i, left_det in enumerate(left_detections):
            for j, right_det in enumerate(right_detections):
                # Check epipolar constraint
                if not self._check_epipolar_constraint(left_det, right_det):
                    continue
                
                # Check disparity range
                disparity = self._calculate_disparity(left_det, right_det)
                if not (self.min_disparity <= disparity <= self.max_disparity):
                    continue
                
                # Calculate geometric cost
                geometric_cost = self._calculate_geometric_cost(left_det, right_det)
                
                # Calculate appearance cost (simplified - could use features)
                appearance_cost = self._calculate_appearance_cost(left_det, right_det)
                
                # Combine costs
                total_cost = 0.7 * geometric_cost + 0.3 * appearance_cost
                cost_matrix[i, j] = total_cost
        
        return cost_matrix
    
    def _check_epipolar_constraint(self, left_det: Detection, right_det: Detection) -> bool:
        """Check if detections satisfy epipolar constraint"""
        if self.has_rectification:
            # For rectified images, epipolar lines are horizontal
            left_center = left_det.center
            right_center = right_det.center
            
            y_diff = abs(left_center[1] - right_center[1])
            return y_diff <= self.epipolar_threshold
        else:
            # Use fundamental matrix for unrectified images
            if self.calibration.F is None:
                # Fallback: assume roughly horizontal epipolar lines
                left_center = left_det.center
                right_center = right_det.center
                y_diff = abs(left_center[1] - right_center[1])
                return y_diff <= self.epipolar_threshold * 2
            
            # Proper epipolar constraint using fundamental matrix
            left_point = np.array([left_det.center[0], left_det.center[1], 1])
            right_point = np.array([right_det.center[0], right_det.center[1], 1])
            
            # Calculate epipolar line
            epipolar_line = self.calibration.F @ left_point
            
            # Distance from point to line
            distance = abs(np.dot(epipolar_line, right_point)) / np.sqrt(epipolar_line[0]**2 + epipolar_line[1]**2)
            
            return distance <= self.epipolar_threshold
    
    def _calculate_disparity(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate disparity between matched detections"""
        left_center = left_det.center
        right_center = right_det.center
        
        # Disparity is the horizontal difference
        disparity = left_center[0] - right_center[0]
        return max(0, disparity)  # Ensure positive disparity
    
    def _estimate_depth(self, disparity: float) -> float:
        """Estimate depth from disparity"""
        if disparity <= 0:
            return float('inf')
        
        # Use calibration baseline and focal length
        baseline = self.calibration.baseline
        focal_length = self.calibration.camera_matrix_left[0, 0]  # fx
        
        if baseline == 0 or focal_length == 0:
            self.logger.warning("Invalid calibration parameters for depth estimation")
            return float('inf')
        
        depth = (baseline * focal_length) / disparity
        return depth
    
    def _triangulate_point(self, left_det: Detection, right_det: Detection) -> np.ndarray:
        """Triangulate 3D point from stereo detections"""
        left_center = left_det.center
        right_center = right_det.center
        
        # Prepare points for triangulation
        left_point = np.array([left_center[0], left_center[1]], dtype=np.float32)
        right_point = np.array([right_center[0], right_center[1]], dtype=np.float32)
        
        if self.has_rectification and self.calibration.Q is not None:
            # Use Q matrix for rectified images
            disparity = self._calculate_disparity(left_det, right_det)
            
            # Create homogeneous point
            point_2d = np.array([left_center[0], left_center[1], disparity, 1])
            
            # Transform to 3D
            point_3d = self.calibration.Q @ point_2d
            
            if point_3d[3] != 0:
                point_3d = point_3d / point_3d[3]
            
            return point_3d[:3]
        else:
            # Use projection matrices if available
            if self.calibration.P1 is not None and self.calibration.P2 is not None:
                # Triangulate using OpenCV
                points_4d = cv2.triangulatePoints(
                    self.calibration.P1,
                    self.calibration.P2,
                    left_point.reshape(2, 1),
                    right_point.reshape(2, 1)
                )
                
                # Convert from homogeneous coordinates
                if points_4d[3, 0] != 0:
                    point_3d = points_4d[:3, 0] / points_4d[3, 0]
                else:
                    point_3d = points_4d[:3, 0]
                
                return point_3d
            else:
                # Fallback: simple depth estimation
                depth = self._estimate_depth(self._calculate_disparity(left_det, right_det))
                
                # Convert to 3D using camera intrinsics
                fx = self.calibration.camera_matrix_left[0, 0]
                fy = self.calibration.camera_matrix_left[1, 1]
                cx = self.calibration.camera_matrix_left[0, 2]
                cy = self.calibration.camera_matrix_left[1, 2]
                
                x = (left_center[0] - cx) * depth / fx
                y = (left_center[1] - cy) * depth / fy
                z = depth
                
                return np.array([x, y, z])
    
    def _calculate_geometric_cost(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate geometric matching cost"""
        # Size similarity
        left_area = left_det.area
        right_area = right_det.area
        
        if left_area == 0 or right_area == 0:
            size_cost = 1.0
        else:
            size_ratio = min(left_area, right_area) / max(left_area, right_area)
            size_cost = 1.0 - size_ratio
        
        # Y-coordinate difference (should be small for good stereo)
        y_diff = abs(left_det.center[1] - right_det.center[1])
        y_cost = min(1.0, y_diff / 50.0)  # Normalize by expected max difference
        
        # Disparity reasonableness
        disparity = self._calculate_disparity(left_det, right_det)
        if disparity < self.min_disparity or disparity > self.max_disparity:
            disparity_cost = 1.0
        else:
            # Prefer moderate disparities
            normalized_disparity = disparity / self.max_disparity
            disparity_cost = abs(normalized_disparity - 0.3)  # Prefer ~30% of max disparity
        
        return (size_cost + y_cost + disparity_cost) / 3.0
    
    def _calculate_appearance_cost(self, left_det: Detection, right_det: Detection) -> float:
        """Calculate appearance-based matching cost (simplified)"""
        # For now, use detection confidence similarity
        conf_diff = abs(left_det.score - right_det.score)
        
        # Class consistency
        class_cost = 0.0 if left_det.class_id == right_det.class_id else 1.0
        
        return (conf_diff + class_cost) / 2.0
    
    def _calculate_stereo_confidence(self, 
                                   left_det: Detection, 
                                   right_det: Detection, 
                                   matching_cost: float) -> float:
        """Calculate confidence for stereo match"""
        # Base confidence from detection scores
        base_confidence = (left_det.score + right_det.score) / 2.0
        
        # Reduce confidence based on matching cost
        matching_confidence = 1.0 - matching_cost
        
        # Combine confidences
        stereo_confidence = base_confidence * matching_confidence
        
        return min(1.0, max(0.0, stereo_confidence))

================
File: argus_track/trackers/__init__.py
================
"""Tracking algorithms"""

from .bytetrack import ByteTrack
from .stereo_lightpost_tracker import EnhancedStereoLightPostTracker
from .lightpost_tracker import EnhancedLightPostTracker

__all__ = ["ByteTrack", "EnhancedLightPostTracker",  "EnhancedStereoLightPostTracker"]

================
File: argus_track/trackers/bytetrack.py
================
# argus_track/trackers/bytetrack.py (UPDATED WITH TRACK MERGING)
"""ByteTrack implementation with enhanced track merging for static objects"""

import logging
from typing import List, Dict, Tuple
import numpy as np
from scipy.optimize import linear_sum_assignment
import gc
from ..config import TrackerConfig
from ..core import Detection, Track
from ..filters import KalmanBoxTracker, batch_predict_kalman
from ..utils import calculate_iou, calculate_iou_matrix


class ByteTrack:
    """
    Enhanced ByteTrack multi-object tracker optimized for static LED detection
    
    Key improvements:
    - Automatic duplicate track merging
    - Extended track lifetimes for static objects
    - Better handling of occlusions
    - More conservative track confirmation
    """
    
    def __init__(self, config: TrackerConfig):
        """Initialize ByteTrack with enhanced configuration"""
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.ByteTrack")
        
        # Track management
        self.tracks: Dict[int, Track] = {}
        self.track_id_counter = 0
        self.frame_id = 0
        
        # Track pools
        self.active_tracks: List[Track] = []
        self.lost_tracks: List[Track] = []
        self.removed_tracks: List[Track] = []
        
        # Track merging history to avoid re-merging
        self.merged_tracks: Dict[int, int] = {}  # merged_id -> main_id
        
        self.logger.info(f"Initialized ByteTrack with enhanced tracking parameters")
        self.logger.info(f"Track confirmation requires {config.min_hits} hits")
        self.logger.info(f"Track merging enabled: {config.enable_track_merging}")
        
    def update(self, detections: List[Detection]) -> List[Track]:
        """Update tracker with new detections and perform track merging"""
        self.frame_id += 1
        
        # Filter detections by size and confidence
        valid_detections = [d for d in detections 
                          if (d.area >= self.config.min_box_area and 
                              d.score >= self.config.track_thresh)]
        
        if len(valid_detections) == 0:
            self.logger.debug(f"Frame {self.frame_id}: No valid detections")
            return self._handle_no_detections()
        
        self.logger.debug(f"Frame {self.frame_id}: {len(valid_detections)} valid detections")
        
        # Split into high and low confidence
        high_conf_dets = []
        low_conf_dets = []
        
        for det in valid_detections:
            if det.score >= self.config.track_thresh * 1.5:  # Higher threshold for high confidence
                high_conf_dets.append(det)
            else:
                low_conf_dets.append(det)
        
        # Predict current tracks
        if self.active_tracks:
            kalman_trackers = [track.kalman_filter for track in self.active_tracks if track.kalman_filter]
            if kalman_trackers:
                batch_predict_kalman(kalman_trackers)
        
        # First association: high confidence detections with active tracks
        matches1, unmatched_tracks1, unmatched_dets1 = self._associate(
            self.active_tracks, 
            high_conf_dets,
            thresh=self.config.match_thresh
        )
        
        # Update matched tracks
        for track_idx, det_idx in matches1:
            track = self.active_tracks[track_idx]
            detection = high_conf_dets[det_idx]
            self._update_track(track, detection)
        
        # Second association: low confidence detections with unmatched tracks
        remaining_tracks = [self.active_tracks[i] for i in unmatched_tracks1]
        matches2, unmatched_tracks2, unmatched_dets2 = self._associate(
            remaining_tracks,
            low_conf_dets,
            thresh=self.config.match_thresh * 0.8  # More lenient for second stage
        )
        
        # Update with low confidence matches
        for track_idx, det_idx in matches2:
            track = remaining_tracks[track_idx]
            detection = low_conf_dets[det_idx]
            self._update_track(track, detection)
        
        # Handle unmatched tracks
        for idx in unmatched_tracks2:
            track = remaining_tracks[idx]
            self._mark_lost(track)
        
        # Create new tracks from unmatched high confidence detections
        new_tracks = []
        for idx in unmatched_dets1:
            detection = high_conf_dets[idx]
            new_track = self._create_track(detection)
            new_tracks.append(new_track)
        
        # Update track lists
        self._update_track_lists()
        
        # Perform track merging if enabled
        if self.config.enable_track_merging and self.frame_id % 30 == 0:  # Every 30 frames
            self._merge_duplicate_tracks()
        
        return self.active_tracks
    
    def _handle_no_detections(self) -> List[Track]:
        """Handle frames with no valid detections"""
        # Mark all active tracks as lost
        for track in self.active_tracks:
            self._mark_lost(track)
        
        self._update_track_lists()
        return self.active_tracks
    
    def _associate(self, tracks: List[Track], detections: List[Detection],
                   thresh: float) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:
        """Associate tracks with detections using IoU with distance penalty"""
        if len(tracks) == 0 or len(detections) == 0:
            return [], list(range(len(tracks))), list(range(len(detections)))
        
        # Calculate IoU matrix
        iou_matrix = calculate_iou_matrix(tracks, detections)
        
        # Add distance penalty for better static object tracking
        distance_matrix = self._calculate_distance_matrix(tracks, detections)
        
        # Combine IoU and distance (normalize distance and subtract from IoU)
        max_distance = np.max(distance_matrix) if np.max(distance_matrix) > 0 else 1.0
        normalized_distance = distance_matrix / max_distance
        combined_matrix = iou_matrix - 0.1 * normalized_distance  # Small distance penalty
        
        # Apply Hungarian algorithm
        cost_matrix = 1 - combined_matrix
        row_indices, col_indices = linear_sum_assignment(cost_matrix)
        
        # Filter matches by threshold
        matches = []
        unmatched_tracks = set(range(len(tracks)))
        unmatched_detections = set(range(len(detections)))
        
        for row, col in zip(row_indices, col_indices):
            if combined_matrix[row, col] >= thresh:
                matches.append((row, col))
                unmatched_tracks.discard(row)
                unmatched_detections.discard(col)
        
        return matches, list(unmatched_tracks), list(unmatched_detections)
    
    def _calculate_distance_matrix(self, tracks: List[Track], 
                                  detections: List[Detection]) -> np.ndarray:
        """Calculate center-to-center distance matrix"""
        if not tracks or not detections:
            return np.zeros((len(tracks), len(detections)))
        
        track_centers = []
        for track in tracks:
            bbox = track.to_tlbr()
            center = np.array([(bbox[0] + bbox[2])/2, (bbox[1] + bbox[3])/2])
            track_centers.append(center)
        
        det_centers = [det.center for det in detections]
        
        distance_matrix = np.zeros((len(tracks), len(detections)))
        for i, track_center in enumerate(track_centers):
            for j, det_center in enumerate(det_centers):
                distance_matrix[i, j] = np.linalg.norm(track_center - det_center)
        
        return distance_matrix
    
    def _update_track(self, track: Track, detection: Detection) -> None:
        """Update track with new detection"""
        track.kalman_filter.update(detection)
        track.detections.append(detection)
        track.hits += 1
        track.time_since_update = 0
        track.age += 1
        
        # Update track state with stricter confirmation requirements
        if (track.state == 'tentative' and 
            track.hits >= self.config.min_hits and 
            self._is_track_stable(track)):
            track.state = 'confirmed'
            self.logger.debug(f"Track {track.track_id} confirmed (hits: {track.hits})")
    
    def _is_track_stable(self, track: Track) -> bool:
        """Check if track has stable detections (for static objects)"""
        if len(track.detections) < 3:
            return False
        
        # Check position stability
        recent_centers = [det.center for det in track.detections[-5:]]
        if len(recent_centers) < 3:
            return True  # Not enough data, assume stable
        
        centers_array = np.array(recent_centers)
        std_dev = np.std(centers_array, axis=0)
        max_std = np.max(std_dev)
        
        return max_std < self.config.static_threshold
    
    def _create_track(self, detection: Detection) -> Track:
        """Create new track from detection"""
        track_id = self.track_id_counter
        self.track_id_counter += 1
        
        track = Track(
            track_id=track_id,
            detections=[detection],
            kalman_filter=KalmanBoxTracker(detection),
            state='tentative',
            hits=1,
            age=1,
            time_since_update=0,
            start_frame=self.frame_id
        )
        
        self.tracks[track_id] = track
        self.active_tracks.append(track)
        
        self.logger.debug(f"Created new track {track_id}")
        return track
    
    def _mark_lost(self, track: Track) -> None:
        """Mark track as lost"""
        if track.state == 'confirmed':
            track.state = 'lost'
        track.time_since_update += 1
        track.age += 1
        
        # Remove from active tracks
        if track in self.active_tracks:
            self.active_tracks.remove(track)
        
        # Add to lost tracks if not already there
        if track not in self.lost_tracks:
            self.lost_tracks.append(track)
    
    def _update_track_lists(self) -> None:
        """Update track lists based on current states"""
        # Handle lost tracks
        tracks_to_remove = []
        for track in self.lost_tracks:
            track.time_since_update += 1
            track.age += 1
            
            if track.time_since_update > self.config.max_time_lost:
                track.state = 'removed'
                tracks_to_remove.append(track)
                self.removed_tracks.append(track)
        
        # Remove from lost tracks
        for track in tracks_to_remove:
            self.lost_tracks.remove(track)
        
        # Handle very old tracks
        old_tracks = []
        for track in self.active_tracks:
            track.age += 1
            if track.age > self.config.max_track_age:
                old_tracks.append(track)
        
        for track in old_tracks:
            self._mark_lost(track)
    
    def _merge_duplicate_tracks(self) -> int:
        """Merge duplicate tracks that likely represent the same object"""
        if not self.config.enable_track_merging:
            return 0
        
        merged_count = 0
        confirmed_tracks = [t for t in self.active_tracks if t.state == 'confirmed']
        
        if len(confirmed_tracks) < 2:
            return 0
        
        # Find pairs of tracks to merge
        tracks_to_merge = []
        processed_tracks = set()
        
        for i, track1 in enumerate(confirmed_tracks):
            if track1.track_id in processed_tracks:
                continue
                
            for j, track2 in enumerate(confirmed_tracks[i+1:], i+1):
                if track2.track_id in processed_tracks:
                    continue
                
                if self._should_merge_tracks(track1, track2):
                    tracks_to_merge.append((track1, track2))
                    processed_tracks.add(track2.track_id)
                    break
        
        # Perform merging
        for track1, track2 in tracks_to_merge:
            if self._merge_tracks(track1, track2):
                merged_count += 1
                self.logger.info(f"Merged track {track2.track_id} into track {track1.track_id}")
        
        if merged_count > 0:
            self.logger.info(f"Merged {merged_count} duplicate tracks")
        
        return merged_count
    
    def _should_merge_tracks(self, track1: Track, track2: Track) -> bool:
        """Determine if two tracks should be merged"""
        # Both must be confirmed
        if track1.state != 'confirmed' or track2.state != 'confirmed':
            return False
        
        # Check if already merged
        if (track1.track_id in self.merged_tracks or 
            track2.track_id in self.merged_tracks):
            return False
        
        # Get current positions
        bbox1 = track1.to_tlbr()
        bbox2 = track2.to_tlbr()
        
        # Calculate IoU
        iou = calculate_iou(bbox1, bbox2)
        if iou < self.config.merge_iou_threshold:
            return False
        
        # Calculate center distance
        center1 = np.array([(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2])
        center2 = np.array([(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2])
        distance = np.linalg.norm(center1 - center2)
        
        if distance > self.config.merge_distance_threshold:
            return False
        
        # Check if both tracks are stable (static)
        if not (self._is_track_stable(track1) and self._is_track_stable(track2)):
            return False
        
        return True
    
    def _merge_tracks(self, main_track: Track, merge_track: Track) -> bool:
        """Merge merge_track into main_track"""
        try:
            # Keep the track with more hits as main
            if merge_track.hits > main_track.hits:
                main_track, merge_track = merge_track, main_track
            
            # Merge detections (keep recent ones from both)
            all_detections = main_track.detections + merge_track.detections
            # Sort by frame and keep unique frames
            frame_dict = {}
            for det in all_detections:
                if det.frame_id not in frame_dict or det.score > frame_dict[det.frame_id].score:
                    frame_dict[det.frame_id] = det
            
            main_track.detections = sorted(frame_dict.values(), key=lambda x: x.frame_id)
            
            # Update track statistics
            main_track.hits += merge_track.hits
            main_track.age = max(main_track.age, merge_track.age)
            
            # Remove merged track
            merge_track.state = 'removed'
            if merge_track in self.active_tracks:
                self.active_tracks.remove(merge_track)
            
            # Record the merge
            self.merged_tracks[merge_track.track_id] = main_track.track_id
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error merging tracks: {e}")
            return False
    
    def get_all_tracks(self) -> Dict[int, Track]:
        """Get all tracks (active, lost, and removed)"""
        return self.tracks.copy()
    
    def get_tracking_stats(self) -> Dict[str, int]:
        """Get current tracking statistics"""
        return {
            'active_tracks': len(self.active_tracks),
            'lost_tracks': len(self.lost_tracks),
            'removed_tracks': len(self.removed_tracks),
            'total_tracks': len(self.tracks),
            'merged_tracks': len(self.merged_tracks),
            'frame_id': self.frame_id
        }
    
    def reset(self) -> None:
        """Reset tracker to initial state"""
        self.tracks.clear()
        self.track_id_counter = 0
        self.frame_id = 0
        self.active_tracks.clear()
        self.lost_tracks.clear()
        self.removed_tracks.clear()
        self.merged_tracks.clear()
        self.logger.info("Tracker reset")

================
File: argus_track/trackers/lightpost_tracker.py
================
"""Light Post Tracker with FIXED GPS synchronization - Only process GPS frames"""
import json
import time
import logging
import numpy as np
import cv2
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

from ..config import TrackerConfig, CameraConfig
from ..core import Detection, Track, GPSData
from ..detectors import ObjectDetector
from .bytetrack import ByteTrack
from ..utils.visualization import draw_tracks, create_track_overlay, RealTimeVisualizer
from ..utils.io import save_tracking_results, load_gps_data
from ..utils.gps_utils import compute_average_location, filter_gps_outliers, GeoLocation, CoordinateTransformer
from ..utils.gps_sync_tracker import GPSSynchronizer  # Import the fixed synchronizer


class EnhancedLightPostTracker:
    """
    FIXED: Enhanced light post tracking system with proper GPS synchronization
    Only processes frames when GPS data is actually available
    """
    
    def __init__(self, config: TrackerConfig, 
                detector: ObjectDetector,
                camera_config: Optional[CameraConfig] = None,
                show_realtime: bool = False,
                display_size: Tuple[int, int] = (1280, 720),
                skip_frames: int = 0):
        """
        Initialize enhanced light post tracker - FIXED VERSION
        """
        self.config = config
        self.detector = detector
        self.tracker = ByteTrack(config)
        self.logger = logging.getLogger(f"{__name__}.EnhancedLightPostTracker")
        
        # Real-time visualization settings
        self.show_realtime = show_realtime
        self.display_size = display_size
        self.visualizer = None
        self.skip_frames = max(0, skip_frames)
        
        # Camera parameters for distance estimation
        self.camera_config = camera_config
        self.focal_length_px = 1400  # Default GoPro approximation
        self.image_width = 2704
        self.image_height = 2028
        self.camera_height = 1.5  # Estimated camera height in meters
        
        # Resolution scaling factor
        self.resolution_scale = 1.0
        
        # GPS tracking and geolocation
        self.gps_tracks: Dict[int, List[GPSData]] = {}
        self.gps_synchronizer: Optional[GPSSynchronizer] = None  # FIXED: GPS synchronizer
        self.track_locations: Dict[int, GeoLocation] = {}
        
        # Performance monitoring
        self.processing_times = []
        self.detection_times = []
        self.tracking_times = []
        self.visualization_times = []
        
        self.logger.info("Initialized FIXED enhanced light post tracker with GPS synchronization")
        if self.show_realtime:
            self.logger.info(f"Real-time visualization enabled with display size {display_size}")
        else:
            self.logger.info("Real-time visualization disabled for maximum performance")

    def process_video(self, video_path: str, 
                    gps_data: Optional[List[GPSData]] = None,
                    output_path: Optional[str] = None,
                    save_results: bool = True,
                    resolution_scale: float = 1.0) -> Dict[int, List[Dict]]:
        """
        FIXED: Process complete video with GPS synchronization
        Only processes frames where GPS data is available
        """
        self.logger.info(f"Processing video with FIXED GPS synchronization: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            error_msg = f"Could not open video: {video_path}"
            self.logger.error(error_msg)
            raise IOError(error_msg)
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Update camera parameters
        self.image_width = width
        self.image_height = height
        self.resolution_scale = resolution_scale
        
        self.logger.info(f"Video properties: {frame_count} total frames, {fps} FPS, {width}x{height}")
        
        # FIXED: Initialize GPS synchronizer if GPS data available
        if gps_data:
            self.gps_synchronizer = GPSSynchronizer(gps_data, fps, gps_fps=10.0)
            sync_stats = self.gps_synchronizer.get_processing_statistics()
            
            self.logger.info("🎯 GPS SYNCHRONIZATION ACTIVE:")
            self.logger.info(f"   📍 GPS points available: {sync_stats['gps_points']}")
            self.logger.info(f"   🎬 Frames to process: {sync_stats['sync_frames']}")
            self.logger.info(f"   📊 Processing ratio: {sync_stats['processing_ratio']:.3f}")
            self.logger.info(f"   🔄 GPS frequency: {sync_stats['avg_gps_frequency']:.1f} Hz")
            
            if sync_stats['sync_frames'] == 0:
                self.logger.error("❌ No frames to process - GPS synchronization failed!")
                return {}
        else:
            self.logger.warning("⚠️  No GPS data provided - processing all frames with skip pattern")
            self.gps_synchronizer = None
        
        # Initialize real-time visualizer if requested
        if self.show_realtime:
            self.visualizer = RealTimeVisualizer(
                window_name="Argus Track - LED Detection (GPS Sync)",
                display_size=self.display_size,
                show_info_panel=True
            )
        
        # Setup video writer if output path provided
        out_writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # FIXED: Process frames based on GPS synchronization
        all_tracks = {}
        current_frame_idx = 0
        processed_frames = 0
        skipped_frames = 0
        user_quit = False
        
        try:
            while True:
                # Read frame
                ret, frame = cap.read()
                if not ret or frame is None:
                    break
                
                # FIXED: Check if this frame should be processed
                should_process = False
                current_gps = None
                
                if self.gps_synchronizer:
                    # Only process frames with GPS data
                    should_process = self.gps_synchronizer.should_process_frame(current_frame_idx)
                    if should_process:
                        current_gps = self.gps_synchronizer.get_gps_for_frame(current_frame_idx)
                else:
                    # Fallback: use skip pattern if no GPS synchronizer
                    should_process = (self.skip_frames == 0) or (current_frame_idx % (self.skip_frames + 1) == 0)
                
                if not should_process:
                    current_frame_idx += 1
                    skipped_frames += 1
                    continue
                
                # Apply resolution scaling if enabled
                if self.resolution_scale < 1.0:
                    try:
                        scaled_width = int(frame.shape[1] * self.resolution_scale)
                        scaled_height = int(frame.shape[0] * self.resolution_scale)
                        frame = cv2.resize(frame, (scaled_width, scaled_height), 
                                        interpolation=cv2.INTER_AREA)
                    except Exception as e:
                        self.logger.error(f"Error scaling frame: {e}")
                
                # Process frame
                start_time = time.time()
                
                # Detect objects - with timing
                detection_start = time.time()
                raw_detections = self.detector.detect(frame)
                detection_time = time.time() - detection_start
                self.detection_times.append(detection_time)
                
                # Convert to Detection objects
                detections = []
                for i, det in enumerate(raw_detections):
                    detections.append(Detection(
                        bbox=np.array(det['bbox']),
                        score=det['score'],
                        class_id=det['class_id'],
                        frame_id=current_frame_idx
                    ))
                
                # Update tracker - with timing
                tracking_start = time.time()
                tracks = self.tracker.update(detections)
                tracking_time = time.time() - tracking_start
                self.tracking_times.append(tracking_time)
                
                # Update GPS data for active tracks (if GPS available)
                if current_gps:
                    self._update_gps_tracks(tracks, current_gps, current_frame_idx)
                
                # Store track data
                for track in tracks:
                    if track.track_id not in all_tracks:
                        all_tracks[track.track_id] = []
                    
                    # Enhanced track data with GPS frame info
                    track_data = {
                        'frame': current_frame_idx,
                        'bbox': track.to_tlbr().tolist(),
                        'score': track.detections[-1].score if track.detections else 0,
                        'state': track.state,
                        'hits': track.hits,
                        'has_gps': current_gps is not None,
                        'timestamp': current_gps.timestamp if current_gps else current_frame_idx / fps
                    }
                    all_tracks[track.track_id].append(track_data)
                
                # Real-time visualization (only if enabled)
                if self.show_realtime and self.visualizer:
                    viz_start_time = time.time()
                    
                    gps_info = None
                    if current_gps:
                        gps_info = {
                            'latitude': current_gps.latitude,
                            'longitude': current_gps.longitude,
                            'heading': current_gps.heading
                        }
                    
                    frame_info = {
                        'frame_idx': current_frame_idx,
                        'total_frames': frame_count,
                        'fps': fps,
                        'skipped_frames': skipped_frames,
                        'processed_frames': processed_frames,
                        'gps_sync': current_gps is not None
                    }
                    
                    # Show real-time visualization
                    try:
                        should_continue = self.visualizer.visualize_frame(
                            frame, detections, tracks, gps_info, frame_info
                        )
                        
                        if not should_continue:
                            user_quit = True
                            self.logger.info("User requested quit from visualization")
                            break
                    except Exception as viz_error:
                        self.logger.error(f"Visualization error: {viz_error}")
                    
                    # Track visualization performance
                    viz_time = time.time() - viz_start_time
                    self.visualization_times.append(viz_time)
                
                # Save to output video if requested
                if out_writer:
                    vis_frame = draw_tracks(frame, tracks)
                    if current_gps:
                        self._add_gps_overlay(vis_frame, current_gps, current_frame_idx)
                    
                    # Scale back to original size if needed
                    if self.resolution_scale < 1.0:
                        vis_frame = cv2.resize(vis_frame, (width, height), interpolation=cv2.INTER_LINEAR)
                    
                    out_writer.write(vis_frame)
                
                # Performance monitoring
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                
                processed_frames += 1
                
                # Progress logging
                if processed_frames % 50 == 0:  # Every 50 processed frames
                    avg_time = np.mean(self.processing_times[-50:]) if self.processing_times else 0
                    effective_fps = 1.0 / avg_time if avg_time > 0 else 0
                    
                    if self.gps_synchronizer:
                        total_gps_frames = self.gps_synchronizer.get_sync_frames_count()
                        progress = processed_frames / total_gps_frames * 100
                        self.logger.info(f"📍 GPS-synced progress: {processed_frames}/{total_gps_frames} "
                                       f"({progress:.1f}%) at {effective_fps:.1f} FPS")
                    else:
                        progress = current_frame_idx / frame_count * 100
                        self.logger.info(f"🎬 Frame progress: {current_frame_idx}/{frame_count} "
                                       f"({progress:.1f}%) - processed: {processed_frames}")
                
                current_frame_idx += 1
                
        except KeyboardInterrupt:
            self.logger.info("Processing interrupted by user (Ctrl+C)")
            user_quit = True
            
        except Exception as e:
            self.logger.error(f"Error processing video: {e}")
            raise
            
        finally:
            # Cleanup
            cap.release()
            if out_writer:
                out_writer.release()
            if self.visualizer:
                self.visualizer.close()
            cv2.destroyAllWindows()
        
        # FIXED: Calculate geolocations for tracked objects
        if self.gps_synchronizer and processed_frames > 0:
            self.logger.info("📍 Calculating object geolocations from GPS-synced data...")
            self._calculate_object_geolocations(all_tracks)
        
        # Performance summary
        if self.processing_times:
            avg_processing = np.mean(self.processing_times) * 1000  # ms
            effective_fps = 1.0 / np.mean(self.processing_times)
            
            self.logger.info("📊 PROCESSING SUMMARY:")
            self.logger.info(f"   🎬 Total video frames: {frame_count}")
            self.logger.info(f"   📍 Processed frames: {processed_frames}")
            self.logger.info(f"   ⏭️  Skipped frames: {skipped_frames}")
            self.logger.info(f"   ⚡ Processing FPS: {effective_fps:.1f}")
            self.logger.info(f"   ⏱️  Avg frame time: {avg_processing:.1f}ms")
            
            if self.gps_synchronizer:
                sync_stats = self.gps_synchronizer.get_processing_statistics()
                self.logger.info(f"   🎯 GPS sync ratio: {sync_stats['processing_ratio']:.3f}")
                self.logger.info(f"   📊 GPS frequency: {sync_stats['avg_gps_frequency']:.1f} Hz")
        
        # Save results if requested
        if save_results:
            results_path = Path(video_path).with_suffix('.json')
            geojson_path = Path(video_path).with_suffix('.geojson')
            
            sync_stats = self.gps_synchronizer.get_processing_statistics() if self.gps_synchronizer else {}
            
            self._save_enhanced_results(
                all_tracks, results_path, geojson_path,
                metadata={
                    'total_frames': frame_count,
                    'processed_frames': processed_frames,
                    'skipped_frames': skipped_frames,
                    'fps': fps,
                    'width': width,
                    'height': height,
                    'resolution_scale': self.resolution_scale,
                    'config': self.config.__dict__,
                    'gps_synchronization': sync_stats,
                    'geolocated_objects': len(self.track_locations),
                    'user_quit': user_quit,
                    'show_realtime': self.show_realtime,
                    'processing_mode': 'gps_synced' if self.gps_synchronizer else 'frame_skip'
                }
            )
        
        processed_tracks = len([t for t in all_tracks.values() if len(t) >= 3])
        geolocated_count = len(self.track_locations)
        
        self.logger.info("🎉 PROCESSING COMPLETE!")
        self.logger.info(f"   📊 Well-tracked objects: {processed_tracks}")
        self.logger.info(f"   📍 Geolocated static objects: {geolocated_count}")
        
        return all_tracks
    
    def _update_gps_tracks(self, tracks: List[Track], gps_data: GPSData, frame_idx: int) -> None:
        """Update GPS data for active tracks"""
        for track in tracks:
            if track.track_id not in self.gps_tracks:
                self.gps_tracks[track.track_id] = []
            
            # Add GPS data for this track
            self.gps_tracks[track.track_id].append(gps_data)
    
    def _calculate_object_geolocations(self, all_tracks: Dict[int, List[Dict]]) -> None:
        """Calculate real-world geolocations for tracked objects using GPS data"""
        
        for track_id, track_history in all_tracks.items():
            # Filter for static, well-tracked objects
            if len(track_history) < self.config.min_static_frames:
                continue
            
            # Check if track has GPS data
            gps_frames = [t for t in track_history if t.get('has_gps', False)]
            if len(gps_frames) < 3:
                continue
            
            # Analyze if object is static
            if not self._is_static_track(track_history):
                continue
            
            # Calculate geolocation
            geolocation = self._estimate_object_geolocation(track_id, track_history)
            
            if geolocation and geolocation.reliability > 0.3:
                self.track_locations[track_id] = geolocation
                self.logger.debug(
                    f"Track {track_id} geolocated: ({geolocation.latitude:.6f}, {geolocation.longitude:.6f}) "
                    f"reliability: {geolocation.reliability:.2f}"
                )
    
    def _is_static_track(self, track_history: List[Dict]) -> bool:
        """Determine if a track represents a static object"""
        if len(track_history) < 5:
            return False
        
        # Calculate movement of bounding box centers
        centers = []
        for detection in track_history:
            bbox = detection['bbox']
            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            centers.append([center_x, center_y])
        
        centers = np.array(centers)
        
        # Calculate standard deviation of movement
        std_movement = np.std(centers, axis=0)
        max_movement = np.max(std_movement)
        
        # Object is static if movement is below threshold
        return max_movement < self.config.static_threshold
    
    def _estimate_object_geolocation(self, track_id: int, 
                                   track_history: List[Dict]) -> Optional[GeoLocation]:
        """Estimate geolocation for a single tracked object"""
        
        # Get track's GPS data
        if track_id not in self.gps_tracks or len(self.gps_tracks[track_id]) < 3:
            return None
        
        gps_points = self.gps_tracks[track_id]
        estimated_positions = []
        
        # For each GPS point, estimate object location
        for i, gps_point in enumerate(gps_points):
            # Find corresponding detection in track history
            detection = None
            for hist_entry in track_history:
                if hist_entry.get('has_gps', False):
                    # This is a simplified mapping - could be improved
                    detection = hist_entry
                    break
            
            if detection is None:
                continue
            
            bbox = detection['bbox']
            
            # Estimate distance to object
            distance = self._estimate_object_distance(bbox)
            
            # Calculate object offset from camera
            lateral_offset, forward_offset = self._calculate_object_offset(bbox, distance)
            
            # Convert to GPS coordinates
            obj_lat, obj_lon = self._gps_offset_to_coordinates(
                gps_point, lateral_offset, forward_offset
            )
            
            estimated_positions.append({
                'lat': obj_lat,
                'lon': obj_lon,
                'distance': distance,
                'confidence': detection['score'],
                'gps_point': gps_point
            })
        
        if not estimated_positions:
            return None
        
        # Calculate average position (static object assumption)
        avg_lat = np.mean([p['lat'] for p in estimated_positions])
        avg_lon = np.mean([p['lon'] for p in estimated_positions])
        avg_confidence = np.mean([p['confidence'] for p in estimated_positions])
        
        # Calculate reliability based on position consistency
        lat_std = np.std([p['lat'] for p in estimated_positions])
        lon_std = np.std([p['lon'] for p in estimated_positions])
        position_std = np.sqrt(lat_std**2 + lon_std**2)
        
        # Convert position standard deviation to reliability score
        reliability = 1.0 / (1.0 + position_std * 10000)
        
        # Estimate accuracy (in meters)
        earth_radius = 6378137.0
        lat_error = lat_std * earth_radius * np.pi / 180
        lon_error = lon_std * earth_radius * np.pi / 180 * np.cos(np.radians(avg_lat))
        accuracy = np.sqrt(lat_error**2 + lon_error**2)
        
        return GeoLocation(
            latitude=avg_lat,
            longitude=avg_lon,
            accuracy=max(1.0, accuracy),
            reliability=reliability,
            timestamp=estimated_positions[-1]['gps_point'].timestamp
        )
    
    def _estimate_object_distance(self, bbox: List[float]) -> float:
        """Estimate distance to object based on bounding box size"""
        real_object_width = 0.3  # meters
        bbox_width = bbox[2] - bbox[0]
        
        if bbox_width > 0:
            distance = (real_object_width * self.focal_length_px) / bbox_width
            return max(1.0, min(distance, 200.0))
        
        return 50.0
    
    def _calculate_object_offset(self, bbox: List[float], distance: float) -> Tuple[float, float]:
        """Calculate object offset from camera position"""
        center_x = (bbox[0] + bbox[2]) / 2
        center_y = (bbox[1] + bbox[3]) / 2
        
        dx_px = center_x - (self.image_width / 2)
        dy_px = center_y - (self.image_height / 2)
        
        angle_per_pixel = 0.0005
        angle_x = dx_px * angle_per_pixel
        angle_y = dy_px * angle_per_pixel
        
        lateral_offset = distance * np.sin(angle_x)
        forward_offset = distance * np.cos(angle_x)
        
        return lateral_offset, forward_offset
    
    def _gps_offset_to_coordinates(self, base_gps: GPSData, 
                                  lateral_offset: float, 
                                  forward_offset: float) -> Tuple[float, float]:
        """Convert local offsets to GPS coordinates"""
        R = 6378137.0
        heading_rad = np.radians(base_gps.heading)
        
        east_offset = lateral_offset * np.cos(heading_rad) + forward_offset * np.sin(heading_rad)
        north_offset = -lateral_offset * np.sin(heading_rad) + forward_offset * np.cos(heading_rad)
        
        lat_offset = north_offset / R * 180 / np.pi
        lon_offset = east_offset / (R * np.cos(np.radians(base_gps.latitude))) * 180 / np.pi
        
        object_lat = base_gps.latitude + lat_offset
        object_lon = base_gps.longitude + lon_offset
        
        return object_lat, object_lon
    
    def _add_gps_overlay(self, frame: np.ndarray, gps_data: GPSData, frame_idx: int) -> None:
        """Add GPS information overlay to frame"""
        overlay_text = [
            f"Frame: {frame_idx}",
            f"GPS: {gps_data.latitude:.6f}, {gps_data.longitude:.6f}",
            f"Heading: {gps_data.heading:.1f}°",
            f"Objects tracked: {len(self.tracker.active_tracks)}",
            f"GPS SYNC: ACTIVE"  # Show GPS sync status
        ]
        
        y_offset = 30
        for text in overlay_text:
            cv2.putText(frame, text, (10, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            y_offset += 25
    
    def _save_enhanced_results(self, all_tracks: Dict[int, List[Dict]], 
                              json_path: Path, geojson_path: Path,
                              metadata: Dict[str, Any]) -> None:
        """Save enhanced results with geolocations"""
        
        # Save standard JSON results
        results = {
            'metadata': metadata,
            'tracks': all_tracks,
            'gps_tracks': {
                str(track_id): [gps.to_dict() for gps in gps_list]
                for track_id, gps_list in self.gps_tracks.items()
            },
            'track_locations': {
                str(track_id): {
                    'latitude': loc.latitude,
                    'longitude': loc.longitude,
                    'accuracy': loc.accuracy,
                    'reliability': loc.reliability,
                    'timestamp': loc.timestamp
                }
                for track_id, loc in self.track_locations.items()
            }
        }
        
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"Saved tracking results to {json_path}")
        
        # Save GeoJSON with geolocated objects
        self._export_geojson(geojson_path)
    
    def _export_geojson(self, output_path: Path, min_reliability: float = 0.3) -> None:
        """Export geolocated objects to GeoJSON format"""
        
        reliable_locations = {
            track_id: location for track_id, location in self.track_locations.items()
            if location.reliability >= min_reliability
        }
        
        features = []
        
        for track_id, location in reliable_locations.items():
            class_name = f"Led-{150 if track_id % 2 == 0 else 240}"
            track_info = self._get_track_summary(track_id)
            
            feature = {
                "type": "Feature",
                "geometry": {
                    "type": "Point",
                    "coordinates": [location.longitude, location.latitude]
                },
                "properties": {
                    "track_id": track_id,
                    "class_name": class_name,
                    "confidence": track_info.get('avg_confidence', 0.0),
                    "reliability": round(location.reliability, 3),
                    "accuracy_meters": round(location.accuracy, 1),
                    "detection_count": track_info.get('detection_count', 0),
                    "gps_synced": True,  # Mark as GPS synchronized
                    "processing_method": "gps_frame_sync"
                }
            }
            features.append(feature)
        
        geojson = {
            "type": "FeatureCollection",
            "features": features,
            "metadata": {
                "generator": "Argus Track Enhanced Light Post Tracker - GPS Synced",
                "total_locations": len(features),
                "min_reliability_threshold": min_reliability,
                "coordinate_system": "WGS84",
                "gps_based_geolocation": True,
                "processing_mode": "gps_synchronized_frames",
                "sync_statistics": self.gps_synchronizer.get_processing_statistics() if self.gps_synchronizer else {}
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(geojson, f, indent=2)
        
        self.logger.info(f"Exported {len(features)} GPS-synced geolocated objects to GeoJSON: {output_path}")
    
    def _get_track_summary(self, track_id: int) -> Dict[str, Any]:
        """Get summary statistics for a track"""
        if track_id not in self.gps_tracks:
            return {}
        
        gps_points = len(self.gps_tracks[track_id])
        
        return {
            'avg_confidence': 0.8,
            'detection_count': gps_points,
            'gps_synchronized': True
        }

    def get_enhanced_tracking_statistics(self) -> Dict[str, Any]:
        """Get comprehensive tracking statistics with GPS sync info"""
        base_stats = self.get_track_statistics()
        
        # Calculate performance metrics
        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0
        effective_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0
        
        # GPS synchronization statistics
        sync_stats = self.gps_synchronizer.get_processing_statistics() if self.gps_synchronizer else {}
        
        enhanced_stats = {
            **base_stats,
            'geolocated_objects': len(self.track_locations),
            'avg_reliability': np.mean([loc.reliability for loc in self.track_locations.values()]) if self.track_locations else 0.0,
            'avg_accuracy_meters': np.mean([loc.accuracy for loc in self.track_locations.values()]) if self.track_locations else 0.0,
            'gps_synchronization': sync_stats,
            'processing_fps': effective_fps,
            'avg_processing_ms': avg_processing_time * 1000,
            'processing_mode': 'gps_synced' if self.gps_synchronizer else 'frame_skip'
        }
        
        return enhanced_stats

    def get_track_statistics(self) -> Dict[str, Any]:
        """Get basic tracking statistics"""
        tracks = self.tracker.get_all_tracks()
        
        return {
            'total_tracks': len(tracks),
            'active_tracks': len(self.tracker.active_tracks),
            'lost_tracks': len(self.tracker.lost_tracks),
            'removed_tracks': len(self.tracker.removed_tracks),
            'total_frames': self.tracker.frame_id,
            'avg_track_length': np.mean([track.age for track in tracks.values()]) if tracks else 0,
            'static_objects': len([track_id for track_id in self.track_locations.keys()]),
            'located_objects': len(self.track_locations)
        }

================
File: argus_track/trackers/stereo_lightpost_tracker.py
================
# argus_track/trackers/stereo_lightpost_tracker.py (UPDATED)

"""Enhanced Stereo Light Post Tracker with Integrated GPS Extraction"""

import cv2
import numpy as np
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
import json

from ..config import TrackerConfig, StereoCalibrationConfig
from ..core import Detection, GPSData
from ..core.stereo import StereoDetection, StereoFrame, StereoTrack
from ..detectors import ObjectDetector
from ..stereo import StereoMatcher, StereoTriangulator, StereoCalibrationManager
from ..trackers import ByteTrack
from ..utils.visualization import draw_tracks
from ..utils.io import save_tracking_results
from ..utils.gps_utils import sync_gps_with_frames, GeoLocation
from ..utils.gps_extraction import extract_gps_from_stereo_videos, save_gps_to_csv


class EnhancedStereoLightPostTracker:
    """
    Enhanced stereo light post tracking system with integrated GPS extraction
    
    This class includes:
    1. Automatic GPS extraction from GoPro videos
    2. Stereo object detection and matching
    3. 3D triangulation for depth estimation
    4. Multi-object tracking with ByteTrack
    5. GPS data synchronization (every 6th frame)
    6. 3D to GPS coordinate transformation
    7. Static object analysis and geolocation
    """
    
    def __init__(self, 
                 config: TrackerConfig,
                 detector: ObjectDetector,
                 stereo_calibration: StereoCalibrationConfig):
        """
        Initialize enhanced stereo light post tracker
        
        Args:
            config: Tracker configuration
            detector: Object detection module
            stereo_calibration: Stereo camera calibration
        """
        self.config = config
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.EnhancedStereoLightPostTracker")
        
        # Stereo processing components
        self.calibration_manager = StereoCalibrationManager(stereo_calibration)
        self.stereo_matcher = StereoMatcher(
            calibration=stereo_calibration,
            epipolar_threshold=16.0,
            iou_threshold=config.stereo_match_threshold
        )
        self.triangulator = StereoTriangulator(stereo_calibration)
        
        # Tracking components
        self.left_tracker = ByteTrack(config)
        self.right_tracker = ByteTrack(config)
        
        # Stereo tracking data
        self.stereo_tracks: Dict[int, StereoTrack] = {}
        self.stereo_frames: List[StereoFrame] = []
        self.track_id_counter = 0
        
        # GPS and geolocation
        self.gps_data_history: List[GPSData] = []
        self.estimated_locations: Dict[int, GeoLocation] = {}
        self.gps_extraction_method: str = 'none'
        
        # Performance monitoring
        self.processing_times = []
        self.frame_count = 0
        
        # Validate calibration
        is_valid, errors = self.calibration_manager.validate_calibration()
        if not is_valid:
            self.logger.warning(f"Calibration validation failed: {errors}")
        
        self.logger.info("Initialized enhanced stereo light post tracker")
        self.logger.info(f"Calibration: {self.calibration_manager.get_calibration_summary()}")
    
    def process_stereo_video_with_auto_gps(self, 
                                          left_video_path: str,
                                          right_video_path: str,
                                          output_path: Optional[str] = None,
                                          save_results: bool = True,
                                          gps_extraction_method: str = 'auto',
                                          save_extracted_gps: bool = True) -> Dict[int, StereoTrack]:
        """
        Process stereo video pair with automatic GPS extraction
        
        Args:
            left_video_path: Path to left camera video
            right_video_path: Path to right camera video
            output_path: Optional path for output video
            save_results: Whether to save tracking results
            gps_extraction_method: GPS extraction method ('auto', 'exiftool', 'gopro_api')
            save_extracted_gps: Whether to save extracted GPS data to CSV
            
        Returns:
            Dictionary of stereo tracks
        """
        self.logger.info("=== Enhanced Stereo Processing with GPS Extraction ===")
        self.logger.info(f"Left video: {left_video_path}")
        self.logger.info(f"Right video: {right_video_path}")
        
        # Step 1: Extract GPS data from videos
        self.logger.info("Step 1: Extracting GPS data from videos...")
        gps_data, method_used = extract_gps_from_stereo_videos(
            left_video_path, right_video_path, gps_extraction_method
        )
        
        self.gps_extraction_method = method_used
        
        if gps_data:
            self.logger.info(f"✅ Successfully extracted {len(gps_data)} GPS points using {method_used}")
            
            # Save extracted GPS data if requested
            if save_extracted_gps:
                gps_csv_path = Path(left_video_path).with_suffix('.csv')
                save_gps_to_csv(gps_data, str(gps_csv_path))
                self.logger.info(f"Saved GPS data to: {gps_csv_path}")
        else:
            self.logger.warning("⚠️  No GPS data extracted - proceeding without geolocation")
            gps_data = None
        
        # Step 2: Process stereo video with extracted GPS data
        self.logger.info("Step 2: Processing stereo video with tracking...")
        return self.process_stereo_video(
            left_video_path=left_video_path,
            right_video_path=right_video_path,
            gps_data=gps_data,
            output_path=output_path,
            save_results=save_results
        )
    
    def process_stereo_video(self, 
                            left_video_path: str,
                            right_video_path: str,
                            gps_data: Optional[List[GPSData]] = None,
                            output_path: Optional[str] = None,
                            save_results: bool = True) -> Dict[int, StereoTrack]:
        """
        Process stereo video pair with tracking and geolocation
        
        Args:
            left_video_path: Path to left camera video
            right_video_path: Path to right camera video
            gps_data: Optional GPS data synchronized with frames
            output_path: Optional path for output video
            save_results: Whether to save tracking results
            
        Returns:
            Dictionary of stereo tracks
        """
        self.logger.info(f"Processing stereo videos: {left_video_path}, {right_video_path}")
        
        # Open video captures
        left_cap = cv2.VideoCapture(left_video_path)
        right_cap = cv2.VideoCapture(right_video_path)
        
        if not left_cap.isOpened() or not right_cap.isOpened():
            error_msg = "Could not open one or both video files"
            self.logger.error(error_msg)
            raise IOError(error_msg)
        
        # Get video properties
        fps = left_cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(left_cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(left_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(left_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        video_duration = total_frames / fps
        
        self.logger.info(f"Video properties: {total_frames} frames, {fps} FPS, {width}x{height}")
        self.logger.info(f"Video duration: {video_duration:.1f} seconds")
        
        # Setup video writer if output requested
        out_writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            # Create side-by-side output
            out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))
        
        # Synchronize GPS data with frame rate if available
        if gps_data:
            from ..utils.gps_extraction import GoProGPSExtractor
            extractor = GoProGPSExtractor(fps_video=fps, fps_gps=10.0)
            
            # Synchronize GPS data to match video timeline
            gps_frame_data = extractor.synchronize_with_video(
                gps_data, video_duration, target_fps=10.0
            )
            self.logger.info(f"Synchronized {len(gps_frame_data)} GPS points with video timeline")
        else:
            gps_frame_data = []
        
        # Process frames
        processed_frames = 0
        
        try:
            while True:
                # Read frame pair
                left_ret, left_frame = left_cap.read()
                right_ret, right_frame = right_cap.read()
                
                if not left_ret or not right_ret:
                    break
                
                start_time = time.time()
                
                # Process every 6th frame (GPS synchronization strategy)
                if processed_frames % self.config.gps_frame_interval == 0:
                    gps_index = processed_frames // self.config.gps_frame_interval
                    current_gps = gps_frame_data[gps_index] if gps_index < len(gps_frame_data) else None
                    
                    # Process stereo frame
                    stereo_frame = self._process_stereo_frame_pair(
                        left_frame, right_frame, processed_frames, current_gps
                    )
                    
                    if stereo_frame:
                        self.stereo_frames.append(stereo_frame)
                        
                        # Update tracking
                        self._update_stereo_tracking(stereo_frame)
                
                # Visualize if output requested
                if out_writer:
                    vis_frame = self._create_stereo_visualization(left_frame, right_frame)
                    out_writer.write(vis_frame)
                
                # Performance monitoring
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                
                # Progress logging
                if processed_frames % 300 == 0:  # Every 10 seconds at 30fps
                    avg_time = np.mean(self.processing_times[-100:]) if self.processing_times else 0
                    progress = processed_frames / total_frames * 100
                    self.logger.info(
                        f"Processed {processed_frames}/{total_frames} frames "
                        f"({progress:.1f}%) Avg time: {avg_time*1000:.1f}ms"
                    )
                
                processed_frames += 1
                
        except Exception as e:
            self.logger.error(f"Error processing stereo video: {e}")
            raise
        finally:
            # Cleanup
            left_cap.release()
            right_cap.release()
            if out_writer:
                out_writer.release()
            cv2.destroyAllWindows()
        
        # Post-processing: estimate locations for static tracks
        self._estimate_stereo_track_locations()
        
        # Save results if requested
        if save_results:
            self._save_enhanced_stereo_results(left_video_path, fps, width, height)
        
        self.logger.info(f"Processing complete. Tracked {len(self.stereo_tracks)} stereo objects")
        return self.stereo_tracks
    
    def _process_stereo_frame_pair(self, 
                                  left_frame: np.ndarray, 
                                  right_frame: np.ndarray,
                                  frame_id: int,
                                  gps_data: Optional[GPSData]) -> Optional[StereoFrame]:
        """Process a single stereo frame pair"""

        # Rectify images if calibration supports it
        if self.config.stereo_mode:
            left_rect, right_rect = self.calibration_manager.rectify_image_pair(
                left_frame, right_frame
            )
        else:
            left_rect, right_rect = left_frame, right_frame
        
        # Detect objects in both frames
        left_detections = self._detect_objects(left_rect, frame_id, 'left')
        
        right_detections = self._detect_objects(right_rect, frame_id, 'right')
        
        if not left_detections and not right_detections:
            return None
        
        # Match detections between left and right views
        stereo_detections = []
        if left_detections and right_detections:
            stereo_detections = self.stereo_matcher.match_detections(
                left_detections, right_detections
            )
            
            # Validate triangulation results
            valid_stereo_detections = []
            for stereo_det in stereo_detections:
                if self.triangulator.validate_triangulation(stereo_det):
                    valid_stereo_detections.append(stereo_det)
                else:
                    self.logger.debug(f"Invalid triangulation for detection at frame {frame_id}")
            
            stereo_detections = valid_stereo_detections
        
        # Create stereo frame
        stereo_frame = StereoFrame(
            frame_id=frame_id,
            timestamp=gps_data.timestamp if gps_data else frame_id / 30.0,  # Assume 30fps fallback
            left_frame=left_rect,
            right_frame=right_rect,
            left_detections=left_detections,
            right_detections=right_detections,
            stereo_detections=stereo_detections,
            gps_data=gps_data
        )
        
        return stereo_frame
    
    def _detect_objects(self, frame: np.ndarray, frame_id: int, camera: str) -> List[Detection]:
        """Detect objects in a single frame"""
        raw_detections = self.detector.detect(frame)
        
        detections = []
        for det in raw_detections:
            detection = Detection(
                bbox=np.array(det['bbox']),
                score=det['score'],
                class_id=det['class_id'],
                frame_id=frame_id
            )
            detections.append(detection)
        
        return detections
    
    def _update_stereo_tracking(self, stereo_frame: StereoFrame) -> None:
        """Update stereo tracking with new frame"""
        
        # Update individual camera trackers (for robustness)
        left_tracks = self.left_tracker.update(stereo_frame.left_detections)
        right_tracks = self.right_tracker.update(stereo_frame.right_detections)
        
        # Process stereo detections for 3D tracking
        for stereo_det in stereo_frame.stereo_detections:
            # Find corresponding tracks in left/right trackers
            left_track_id = self._find_matching_track(stereo_det.left_detection, left_tracks)
            right_track_id = self._find_matching_track(stereo_det.right_detection, right_tracks)
            
            if left_track_id is not None and right_track_id is not None:
                # Find existing stereo track or create new one
                stereo_track_id = self._get_or_create_stereo_track(left_track_id, right_track_id)
                
                if stereo_track_id in self.stereo_tracks:
                    # Update existing stereo track
                    stereo_track = self.stereo_tracks[stereo_track_id]
                    stereo_track.stereo_detections.append(stereo_det)
                    stereo_track.world_trajectory.append(stereo_det.world_coordinates)
                    
                    # Add GPS coordinate if available
                    if stereo_frame.gps_data:
                        # Transform to GPS coordinates
                        gps_locations = self.triangulator.world_to_gps_coordinates(
                            [stereo_det.world_coordinates], stereo_frame.gps_data
                        )
                        if gps_locations:
                            stereo_track.gps_trajectory.append(
                                np.array([gps_locations[0].latitude, gps_locations[0].longitude])
                            )
                    
                    # Update depth consistency
                    self._update_depth_consistency(stereo_track)
    
    def _find_matching_track(self, detection: Detection, tracks: List) -> Optional[int]:
        """Find track that matches the given detection"""
        best_track_id = None
        best_iou = 0.0
        
        for track in tracks:
            if track.last_detection:
                from ..utils.iou import calculate_iou
                iou = calculate_iou(detection.bbox, track.last_detection.bbox)
                if iou > best_iou and iou > 0.5:  # Minimum IoU threshold
                    best_iou = iou
                    best_track_id = track.track_id
        
        return best_track_id
    
    def _get_or_create_stereo_track(self, left_track_id: int, right_track_id: int) -> int:
        """Get existing stereo track or create new one"""
        # Look for existing stereo track that matches either left or right track
        for stereo_id, stereo_track in self.stereo_tracks.items():
            # For simplicity, use left track ID as primary identifier
            if stereo_id == left_track_id:
                return stereo_id
        
        # Create new stereo track
        stereo_track = StereoTrack(
            track_id=left_track_id,  # Use left track ID
            stereo_detections=[],
            world_trajectory=[],
            gps_trajectory=[]
        )
        
        self.stereo_tracks[left_track_id] = stereo_track
        return left_track_id
    
    def _update_depth_consistency(self, stereo_track: StereoTrack) -> None:
        """Update depth consistency metric for a stereo track"""
        if len(stereo_track.stereo_detections) < 3:
            return
        
        # Calculate depth variance over recent detections
        recent_depths = [det.depth for det in stereo_track.stereo_detections[-10:]]
        depth_std = np.std(recent_depths)
        
        # Consistency is inversely related to standard deviation
        stereo_track.depth_consistency = 1.0 / (1.0 + depth_std)
    
    def _estimate_stereo_track_locations(self) -> None:
        """Estimate final GPS locations for static stereo tracks"""
        for track_id, stereo_track in self.stereo_tracks.items():
            if stereo_track.is_static_3d and len(stereo_track.gps_trajectory) >= 3:
                # Get GPS history for this track
                gps_points = []
                for gps_coord in stereo_track.gps_trajectory:
                    # Convert back to GPSData format
                    gps_point = GPSData(
                        timestamp=0.0,  # Timestamp not needed for location estimation
                        latitude=gps_coord[0],
                        longitude=gps_coord[1],
                        altitude=0.0,
                        heading=0.0
                    )
                    gps_points.append(gps_point)
                
                # Estimate location using triangulator
                estimated_location = self.triangulator.estimate_object_location(
                    stereo_track, gps_points
                )
                
                if estimated_location:
                    stereo_track.estimated_location = estimated_location
                    self.estimated_locations[track_id] = estimated_location
                    
                    self.logger.debug(
                        f"Track {track_id} located at ({estimated_location.latitude:.6f}, {estimated_location.longitude:.6f}) "
                        f"reliability: {estimated_location.reliability:.2f}"
                    )
    
    def _create_stereo_visualization(self, 
                                   left_frame: np.ndarray, 
                                   right_frame: np.ndarray) -> np.ndarray:
        """Create side-by-side visualization of stereo tracking"""
        # Draw tracks on both frames
        left_vis = draw_tracks(left_frame, self.left_tracker.active_tracks)
        right_vis = draw_tracks(right_frame, self.right_tracker.active_tracks)
        
        # Create side-by-side visualization
        stereo_vis = np.hstack([left_vis, right_vis])
        
        # Add stereo information overlay
        self._add_stereo_info_overlay(stereo_vis)
        
        return stereo_vis
    
    def _add_stereo_info_overlay(self, stereo_frame: np.ndarray) -> None:
        """Add information overlay to stereo visualization"""
        # Add text information
        info_text = [
            f"Stereo Tracks: {len(self.stereo_tracks)}",
            f"GPS Method: {self.gps_extraction_method}",
            f"GPS Points: {len(self.gps_data_history)}",
            f"Locations: {len(self.estimated_locations)}",
            f"Frame: {self.frame_count}"
        ]
        
        y_offset = 30
        for text in info_text:
            cv2.putText(stereo_frame, text, (10, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            y_offset += 25
    
    def _save_enhanced_stereo_results(self, video_path: str, fps: float, width: int, height: int) -> None:
        """Save enhanced stereo tracking results with GPS extraction info"""
        results_path = Path(video_path).with_suffix('.json')
        
        # Prepare results data
        results = {
            'metadata': {
                'total_frames': len(self.stereo_frames),
                'fps': fps,
                'width': width,
                'height': height,
                'stereo_mode': self.config.stereo_mode,
                'gps_frame_interval': self.config.gps_frame_interval,
                'gps_extraction_method': self.gps_extraction_method,
                'gps_points_extracted': len(self.gps_data_history),
                'processing_times': {
                    'mean': np.mean(self.processing_times) if self.processing_times else 0,
                    'std': np.std(self.processing_times) if self.processing_times else 0,
                    'min': np.min(self.processing_times) if self.processing_times else 0,
                    'max': np.max(self.processing_times) if self.processing_times else 0
                }
            },
            'stereo_tracks': {
                str(track_id): track.to_dict() 
                for track_id, track in self.stereo_tracks.items()
            },
            'estimated_locations': {
                str(track_id): location.__dict__ 
                for track_id, location in self.estimated_locations.items()
            },
            'calibration_summary': self.calibration_manager.get_calibration_summary()
        }
        
        # Save to JSON
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"Saved enhanced stereo tracking results to {results_path}")
        
        # Also save GeoJSON for mapping
        geojson_path = Path(video_path).with_suffix('.geojson')
        self._export_locations_to_geojson(geojson_path)
    
    def _export_locations_to_geojson(self, output_path: Path) -> None:
        """Export estimated locations to GeoJSON format"""
        features = []
        
        for track_id, location in self.estimated_locations.items():
            if location.reliability > 0.5:  # Only export reliable locations
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [location.longitude, location.latitude]
                    },
                    "properties": {
                        "track_id": track_id,
                        "reliability": location.reliability,
                        "accuracy": location.accuracy,
                        "method": "stereo_triangulation_with_auto_gps",
                        "gps_extraction_method": self.gps_extraction_method
                    }
                }
                features.append(feature)
        
        geojson = {
            "type": "FeatureCollection",
            "features": features,
            "metadata": {
                "generator": "Argus Track Enhanced Stereo Tracker",
                "gps_extraction_method": self.gps_extraction_method,
                "total_locations": len(features)
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(geojson, f, indent=2)
        
        self.logger.info(f"Exported {len(features)} locations to GeoJSON: {output_path}")
    
    def get_enhanced_tracking_statistics(self) -> Dict[str, Any]:
        """Get comprehensive enhanced stereo tracking statistics"""
        static_count = sum(1 for track in self.stereo_tracks.values() if track.is_static_3d)
        
        return {
            'total_stereo_tracks': len(self.stereo_tracks),
            'static_tracks': static_count,
            'estimated_locations': len(self.estimated_locations),
            'processed_frames': len(self.stereo_frames),
            'gps_extraction_method': self.gps_extraction_method,
            'gps_points_used': len(self.gps_data_history),
            'avg_depth': np.mean([track.average_depth for track in self.stereo_tracks.values()]) if self.stereo_tracks else 0,
            'avg_depth_consistency': np.mean([track.depth_consistency for track in self.stereo_tracks.values()]) if self.stereo_tracks else 0,
            'calibration_baseline': self.calibration_manager.calibration.baseline if self.calibration_manager.calibration else 0,
            'accuracy_achieved': np.mean([loc.accuracy for loc in self.estimated_locations.values()]) if self.estimated_locations else 0,
            'avg_reliability': np.mean([loc.reliability for loc in self.estimated_locations.values()]) if self.estimated_locations else 0
        }

================
File: argus_track/utils/gps_extraction.py
================
# argus_track/utils/gps_extraction.py (NEW FILE)

"""
GPS Data Extraction from GoPro Videos
=====================================
Integrated GPS extraction functionality for Argus Track stereo processing.
Supports both ExifTool and GoPro API methods for extracting GPS metadata.
"""

import os
import sys
import time
import logging
import subprocess
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Union
from datetime import datetime, timedelta
import numpy as np
from bs4 import BeautifulSoup
import tempfile
import shutil
from dataclasses import dataclass

from ..core import GPSData

# Configure logging
logger = logging.getLogger(__name__)

# Try to import GoPro API if available
try:
    from gopro_overlay.goprotelemetry import telemetry
    GOPRO_API_AVAILABLE = True
except ImportError:
    GOPRO_API_AVAILABLE = False
    logger.debug("GoPro telemetry API not available")

# Check for ExifTool availability
def check_exiftool_available() -> bool:
    """Check if ExifTool is available in the system"""
    try:
        result = subprocess.run(['exiftool', '-ver'], 
                               capture_output=True, text=True, timeout=10)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False

EXIFTOOL_AVAILABLE = check_exiftool_available()


@dataclass
class GPSExtractionResult:
    """Result of GPS extraction operation"""
    success: bool
    gps_data: List[GPSData]
    method_used: str
    total_points: int
    time_range: Optional[Tuple[float, float]] = None
    error_message: Optional[str] = None


class GoProGPSExtractor:
    """Extract GPS data from GoPro videos using multiple methods"""
    
    def __init__(self, fps_video: float = 60.0, fps_gps: float = 10.0):
        """
        Initialize GPS extractor
        
        Args:
            fps_video: Video frame rate (default: 60 fps)
            fps_gps: GPS data rate (default: 10 Hz)
        """
        self.fps_video = fps_video
        self.fps_gps = fps_gps
        self.frame_time_ms = 1000.0 / fps_video
        
        # Check available extraction methods
        self.methods_available = []
        if EXIFTOOL_AVAILABLE:
            self.methods_available.append('exiftool')
            logger.debug("ExifTool method available")
        if GOPRO_API_AVAILABLE:
            self.methods_available.append('gopro_api')
            logger.debug("GoPro API method available")
        
        if not self.methods_available:
            logger.warning("No GPS extraction methods available!")
    
    def extract_gps_data(self, video_path: str, 
                        method: str = 'auto') -> GPSExtractionResult:
        """
        Extract GPS data from GoPro video
        
        Args:
            video_path: Path to GoPro video file
            method: Extraction method ('auto', 'exiftool', 'gopro_api')
            
        Returns:
            GPSExtractionResult: Extraction results
        """
        if not os.path.exists(video_path):
            return GPSExtractionResult(
                success=False,
                gps_data=[],
                method_used='none',
                total_points=0,
                error_message=f"Video file not found: {video_path}"
            )
        
        # Determine extraction method
        if method == 'auto':
            # Prefer GoPro API for better accuracy, fallback to ExifTool
            if 'gopro_api' in self.methods_available:
                method = 'gopro_api'
            elif 'exiftool' in self.methods_available:
                method = 'exiftool'
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='none',
                    total_points=0,
                    error_message="No GPS extraction methods available"
                )
        
        logger.info(f"Extracting GPS data from {video_path} using {method} method")
        
        try:
            if method == 'exiftool':
                return self._extract_with_exiftool(video_path)
            elif method == 'gopro_api':
                return self._extract_with_gopro_api(video_path)
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used=method,
                    total_points=0,
                    error_message=f"Unknown extraction method: {method}"
                )
        except Exception as e:
            logger.error(f"Error extracting GPS data: {e}")
            return GPSExtractionResult(
                success=False,
                gps_data=[],
                method_used=method,
                total_points=0,
                error_message=str(e)
            )
    
    def _extract_with_exiftool(self, video_path: str) -> GPSExtractionResult:
        """Extract GPS data using ExifTool method"""
        temp_dir = tempfile.mkdtemp()
        
        try:
            metadata_file = os.path.join(temp_dir, 'metadata.xml')
            gps_file = os.path.join(temp_dir, 'gps_data.txt')
            
            # Extract metadata using ExifTool
            cmd = [
                'exiftool',
                '-api', 'largefilesupport=1',
                '-ee',  # Extract embedded data
                '-G3',  # Show group names
                '-X',   # XML format
                video_path
            ]
            
            logger.debug(f"Running ExifTool command: {' '.join(cmd)}")
            
            with open(metadata_file, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, 
                                      text=True, timeout=300)
            
            if result.returncode != 0:
                raise RuntimeError(f"ExifTool failed: {result.stderr}")
            
            # Extract Track4 GPS data
            self._extract_track4_data(metadata_file, gps_file)
            
            # Parse GPS data
            gps_data = self._parse_gps_file(gps_file)
            
            if gps_data:
                time_range = (gps_data[0].timestamp, gps_data[-1].timestamp)
                return GPSExtractionResult(
                    success=True,
                    gps_data=gps_data,
                    method_used='exiftool',
                    total_points=len(gps_data),
                    time_range=time_range
                )
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='exiftool',
                    total_points=0,
                    error_message="No GPS data found in metadata"
                )
                
        finally:
            # Cleanup temporary files
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _extract_with_gopro_api(self, video_path: str) -> GPSExtractionResult:
        """Extract GPS data using GoPro API method"""
        try:
            # Extract telemetry data
            telem = telemetry.Telemetry(video_path)
            
            if not telem.has_gps():
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='gopro_api',
                    total_points=0,
                    error_message="No GPS data found in video"
                )
            
            # Get GPS track
            gps_track = telem.gps_track()
            gps_data = []
            
            for point in gps_track:
                if point.lat != 0.0 and point.lon != 0.0:
                    # Convert timestamp to seconds
                    timestamp = point.timestamp.total_seconds() if hasattr(point.timestamp, 'total_seconds') else point.timestamp
                    
                    gps_point = GPSData(
                        timestamp=float(timestamp),
                        latitude=float(point.lat),
                        longitude=float(point.lon),
                        altitude=float(getattr(point, 'alt', 0.0)),
                        heading=float(getattr(point, 'heading', 0.0)),
                        accuracy=float(getattr(point, 'dop', 1.0))
                    )
                    gps_data.append(gps_point)
            
            if gps_data:
                time_range = (gps_data[0].timestamp, gps_data[-1].timestamp)
                return GPSExtractionResult(
                    success=True,
                    gps_data=gps_data,
                    method_used='gopro_api',
                    total_points=len(gps_data),
                    time_range=time_range
                )
            else:
                return GPSExtractionResult(
                    success=False,
                    gps_data=[],
                    method_used='gopro_api',
                    total_points=0,
                    error_message="No valid GPS points found"
                )
                
        except Exception as e:
            raise RuntimeError(f"GoPro API extraction failed: {e}")
    
    def _extract_track4_data(self, metadata_file: str, output_file: str) -> None:
        """Extract Track4 GPS data from metadata XML file"""
        try:
            with open(metadata_file, 'r', encoding='utf-8') as in_file, \
                 open(output_file, 'w', encoding='utf-8') as out_file:
                
                for line in in_file:
                    # Look for Track4 GPS data
                    if 'Track4' in line or 'GPS' in line:
                        out_file.write(line)
                        
            logger.debug(f"Extracted Track4 data to {output_file}")
            
        except Exception as e:
            logger.error(f"Error extracting Track4 data: {e}")
            raise
    
    def _parse_gps_file(self, gps_file: str) -> List[GPSData]:
        """Parse GPS data from extracted Track4 file"""
        gps_data = []
        
        try:
            with open(gps_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Skip first two lines if they exist
            lines = content.split('\n')[2:] if len(content.split('\n')) > 2 else content.split('\n')
            
            current_timestamp = None
            current_lat = None
            current_lon = None
            
            for line in lines:
                if not line.strip():
                    continue
                    
                # Parse XML-like content
                soup = BeautifulSoup(line, "html.parser")
                text_content = soup.get_text()
                
                # Look for GPS tags
                if ':GPSLatitude>' in line:
                    current_lat = self._convert_gps_coordinate(text_content)
                elif ':GPSLongitude>' in line and current_lat is not None:
                    current_lon = self._convert_gps_coordinate(text_content)
                elif ':GPSDateTime>' in line:
                    current_timestamp = self._convert_timestamp(text_content)
                    
                    # If we have complete GPS data, save it
                    if (current_timestamp is not None and 
                        current_lat is not None and 
                        current_lon is not None and
                        current_lat != 0.0 and current_lon != 0.0):
                        
                        gps_point = GPSData(
                            timestamp=current_timestamp,
                            latitude=current_lat,
                            longitude=current_lon,
                            altitude=0.0,
                            heading=0.0,
                            accuracy=1.0
                        )
                        gps_data.append(gps_point)
                        
                        # Reset for next point
                        current_lat = None
                        current_lon = None
            
            logger.info(f"Parsed {len(gps_data)} GPS points from file")
            return gps_data
            
        except Exception as e:
            logger.error(f"Error parsing GPS file: {e}")
            return []
    
    def _convert_gps_coordinate(self, coord_str: str) -> float:
        """Convert GPS coordinate from DMS format to decimal degrees"""
        if not coord_str or not isinstance(coord_str, str):
            return 0.0
            
        try:
            # Clean the string
            coord_str = coord_str.strip()
            
            # Handle the format: "34 deg 39' 45.72" S"
            import re
            # Pattern for: "34 deg 39' 45.72" S"
            pattern = r"(\d+)\s+deg\s+(\d+)'\s+([\d.]+)\"\s*([NSEW])"
            match = re.search(pattern, coord_str)
            
            if match:
                degrees = float(match.group(1))
                minutes = float(match.group(2))
                seconds = float(match.group(3))
                direction = match.group(4)
                
                # Convert to decimal degrees
                decimal = degrees + minutes/60.0 + seconds/3600.0
                
                # Apply sign based on direction
                if direction in ['S', 'W']:
                    decimal = -decimal
                    
                return decimal
            
            if coord_str.startswith('<'):
                coord_str = coord_str[1:]
            if coord_str.endswith('>'):
                coord_str = coord_str[:-1]
                
            # Parse DMS format: "deg min' sec" N/S/E/W"
            parts = coord_str.split(' ')
            if len(parts) < 6:
                logger.warning(f"Invalid GPS coordinate format: {coord_str}")
                return 0.0
            
            degrees = float(parts[1])
            minutes = float(parts[3].replace("'", ""))
            seconds = float(parts[4].replace('"', ""))
            direction = parts[5][0] if len(parts[5]) > 0 else 'N'
            
            # Convert to decimal degrees
            decimal = degrees + minutes/60.0 + seconds/3600.0
            
            # Apply sign based on direction
            if direction in ['S', 'W']:
                decimal = -decimal
                
            return decimal
            
        except (ValueError, IndexError) as e:
            logger.warning(f"Error converting GPS coordinate '{coord_str}': {e}")
            return 0.0
    
    def _convert_timestamp(self, timestamp_str: str) -> float:
        """Convert timestamp string to Unix timestamp"""
        if not timestamp_str:
            return 0.0
            
        try:
            # Clean timestamp string
            timestamp_str = timestamp_str.strip()
            if timestamp_str.startswith('<'):
                timestamp_str = timestamp_str[1:]
            if timestamp_str.endswith('>'):
                timestamp_str = timestamp_str[:-1]
            
            # Parse timestamp formats
            try:
                # Try with microseconds
                dt = datetime.strptime(timestamp_str, '%Y:%m:%d %H:%M:%S.%f')
            except ValueError:
                # Try without microseconds
                dt = datetime.strptime(timestamp_str, '%Y:%m:%d %H:%M:%S')
            
            return dt.timestamp()
            
        except ValueError as e:
            logger.warning(f"Error converting timestamp '{timestamp_str}': {e}")
            return 0.0
    
    def synchronize_with_video(self, gps_data: List[GPSData], 
                              video_duration: float,
                              target_fps: float = 10.0) -> List[GPSData]:
        """
        Synchronize GPS data with video timeline
        
        Args:
            gps_data: Raw GPS data
            video_duration: Video duration in seconds
            target_fps: Target GPS sampling rate
            
        Returns:
            List[GPSData]: Synchronized GPS data
        """
        if not gps_data:
            return []
        
        # Sort GPS data by timestamp
        sorted_gps = sorted(gps_data, key=lambda x: x.timestamp)
        
        # Normalize timestamps to start from 0
        start_time = sorted_gps[0].timestamp
        for gps_point in sorted_gps:
            gps_point.timestamp -= start_time
        
        # Create synchronized timeline
        sync_interval = 1.0 / target_fps
        sync_timeline = np.arange(0, video_duration, sync_interval)
        
        # Interpolate GPS data to match timeline
        timestamps = np.array([gps.timestamp for gps in sorted_gps])
        latitudes = np.array([gps.latitude for gps in sorted_gps])
        longitudes = np.array([gps.longitude for gps in sorted_gps])
        
        # Interpolate
        sync_gps = []
        for sync_time in sync_timeline:
            if sync_time <= timestamps[-1]:
                # Find closest GPS points for interpolation
                idx = np.searchsorted(timestamps, sync_time)
                
                if idx == 0:
                    # Use first point
                    lat = latitudes[0]
                    lon = longitudes[0]
                elif idx >= len(timestamps):
                    # Use last point
                    lat = latitudes[-1]
                    lon = longitudes[-1]
                else:
                    # Linear interpolation
                    t1, t2 = timestamps[idx-1], timestamps[idx]
                    lat1, lat2 = latitudes[idx-1], latitudes[idx]
                    lon1, lon2 = longitudes[idx-1], longitudes[idx]
                    
                    alpha = (sync_time - t1) / (t2 - t1)
                    lat = lat1 + alpha * (lat2 - lat1)
                    lon = lon1 + alpha * (lon2 - lon1)
                
                sync_point = GPSData(
                    timestamp=sync_time,
                    latitude=lat,
                    longitude=lon,
                    altitude=0.0,
                    heading=0.0,
                    accuracy=1.0
                )
                sync_gps.append(sync_point)
        
        logger.info(f"Synchronized {len(sync_gps)} GPS points for {video_duration:.1f}s video")
        return sync_gps


def extract_gps_from_stereo_videos(left_video: str, 
                                  right_video: str,
                                  method: str = 'auto') -> Tuple[List[GPSData], str]:
    """
    Extract GPS data from stereo video pair
    
    Args:
        left_video: Path to left camera video
        right_video: Path to right camera video  
        method: Extraction method ('auto', 'exiftool', 'gopro_api')
        
    Returns:
        Tuple[List[GPSData], str]: GPS data and method used
    """
    extractor = GoProGPSExtractor()
    
    # Try extracting from left video first
    logger.info("Attempting GPS extraction from left video")
    result_left = extractor.extract_gps_data(left_video, method)
    
    if result_left.success and result_left.total_points > 0:
        logger.info(f"Successfully extracted {result_left.total_points} GPS points from left video")
        return result_left.gps_data, result_left.method_used
    
    # Fallback to right video
    logger.info("Left video GPS extraction failed, trying right video")
    result_right = extractor.extract_gps_data(right_video, method)
    
    if result_right.success and result_right.total_points > 0:
        logger.info(f"Successfully extracted {result_right.total_points} GPS points from right video")
        return result_right.gps_data, result_right.method_used
    
    # No GPS data found
    logger.warning("No GPS data found in either video")
    return [], 'none'


def save_gps_to_csv(gps_data: List[GPSData], output_path: str) -> None:
    """
    Save GPS data to CSV file for Argus Track
    
    Args:
        gps_data: GPS data to save
        output_path: Path to output CSV file
    """
    import csv
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['timestamp', 'latitude', 'longitude', 'altitude', 'heading', 'accuracy'])
        
        for gps in gps_data:
            writer.writerow([
                gps.timestamp,
                gps.latitude,
                gps.longitude,
                gps.altitude,
                gps.heading,
                gps.accuracy
            ])
    
    logger.info(f"Saved {len(gps_data)} GPS points to {output_path}")

================
File: argus_track/utils/performance.py
================
"""
Performance monitoring utilities for ArgusTrack
"""

import time
from typing import Dict, List, Optional
from dataclasses import dataclass, field

@dataclass
class PerformanceMetrics:
    """Container for performance metrics"""
    fps: float = 0.0
    frame_time: float = 0.0
    detection_time: float = 0.0
    tracking_time: float = 0.0
    total_time: float = 0.0
    frame_count: int = 0
    
    def reset(self):
        """Reset all metrics to zero"""
        self.fps = 0.0
        self.frame_time = 0.0
        self.detection_time = 0.0
        self.tracking_time = 0.0
        self.total_time = 0.0
        self.frame_count = 0

class PerformanceMonitor:
    """Monitor and track performance metrics"""
    
    def __init__(self):
        self.metrics = PerformanceMetrics()
        self.start_time: Optional[float] = None
        self.frame_times: List[float] = []
        self.detection_times: List[float] = []
        self.tracking_times: List[float] = []
        
    def start_frame(self):
        """Start timing a frame"""
        self.start_time = time.time()
        
    def end_frame(self):
        """End timing a frame and update metrics"""
        if self.start_time is not None:
            frame_time = time.time() - self.start_time
            self.frame_times.append(frame_time)
            self.metrics.frame_count += 1
            self.start_time = None
            
    def record_detection_time(self, detection_time: float):
        """Record detection time for current frame"""
        self.detection_times.append(detection_time)
        
    def record_tracking_time(self, tracking_time: float):
        """Record tracking time for current frame"""
        self.tracking_times.append(tracking_time)
        
    def update_metrics(self):
        """Update average metrics"""
        if self.frame_times:
            self.metrics.frame_time = sum(self.frame_times) / len(self.frame_times)
            self.metrics.fps = 1.0 / self.metrics.frame_time if self.metrics.frame_time > 0 else 0.0
            
        if self.detection_times:
            self.metrics.detection_time = sum(self.detection_times) / len(self.detection_times)
            
        if self.tracking_times:
            self.metrics.tracking_time = sum(self.tracking_times) / len(self.tracking_times)
            
        self.metrics.total_time = self.metrics.detection_time + self.metrics.tracking_time
        
    def get_metrics(self) -> PerformanceMetrics:
        """Get current performance metrics"""
        self.update_metrics()
        return self.metrics
        
    def reset(self):
        """Reset all timing data"""
        self.metrics.reset()
        self.frame_times.clear()
        self.detection_times.clear()
        self.tracking_times.clear()
        self.start_time = None
        
    def print_stats(self):
        """Print performance statistics"""
        self.update_metrics()
        print(f"Performance Stats:")
        print(f"  FPS: {self.metrics.fps:.2f}")
        print(f"  Frame Time: {self.metrics.frame_time*1000:.2f}ms")
        print(f"  Detection Time: {self.metrics.detection_time*1000:.2f}ms")
        print(f"  Tracking Time: {self.metrics.tracking_time*1000:.2f}ms")
        print(f"  Total Frames: {self.metrics.frame_count}")

================
File: argus_track/utils/visualization.py
================
"""Enhanced visualization utilities with real-time display"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
import time
import logging

from ..core import Track, Detection


# Color palette for different track states
TRACK_COLORS = {
    'tentative': (255, 255, 0),    # Yellow
    'confirmed': (0, 255, 0),      # Green  
    'lost': (0, 0, 255),          # Red
    'removed': (128, 128, 128)     # Gray
}

# Class-specific colors
CLASS_COLORS = {
    'Led-150': (255, 0, 0),       # Red
    'Led-240': (0, 0, 255),       # Blue
    'light_post': (0, 255, 0),    # Green
    'street_light': (255, 165, 0), # Orange
    'pole': (128, 0, 128)          # Purple
}


class RealTimeVisualizer:
    """Real-time visualization during tracking"""
    
    def __init__(self, window_name: str = "Argus Track - Real-time Detection", 
                 display_size: Tuple[int, int] = (1280, 720),
                 show_info_panel: bool = True):
        """
        Initialize real-time visualizer
        
        Args:
            window_name: Name of the display window
            display_size: Size of the display window (width, height)
            show_info_panel: Whether to show information panel
        """
        self.window_name = window_name
        self.display_size = display_size
        self.show_info_panel = show_info_panel
        
        # Initialize logger
        self.logger = logging.getLogger(f"{__name__}.RealTimeVisualizer")
        
        # Statistics tracking
        self.frame_count = 0
        self.detection_history = []
        self.fps_history = []
        self.last_time = time.time()
        
        # Create window
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, display_size[0], display_size[1])
        
        # Create default blank frame for error cases
        self.blank_frame = np.zeros((display_size[1], display_size[0], 3), dtype=np.uint8)
        cv2.putText(self.blank_frame, "No frame data available", 
                   (display_size[0]//4, display_size[1]//2), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        self.logger.info(f"🖥️  Real-time visualization window opened: {window_name}")
        self.logger.info("   Press 'q' to quit, 'p' to pause, 's' to save screenshot")

    def visualize_frame(self, frame: np.ndarray, 
                    detections: List[Detection],
                    tracks: List[Track],
                    gps_data: Optional[Dict] = None,
                    frame_info: Optional[Dict] = None) -> bool:
        """
        Visualize a single frame with detections and tracks
        
        Args:
            frame: Input frame
            detections: Raw detections for this frame
            tracks: Active tracks
            gps_data: Optional GPS data
            frame_info: Optional frame information
            
        Returns:
            False if user wants to quit, True otherwise
        """
        self.frame_count += 1
        
        # Input validation - use blank frame if input is invalid
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            self.logger.warning(f"Invalid frame received (type: {type(frame)}, frame_count: {self.frame_count})")
            vis_frame = self.blank_frame.copy()
        else:
            # Create visualization with error handling
            try:
                vis_frame = self._create_visualization(frame, detections, tracks, gps_data, frame_info)
                if vis_frame is None:
                    self.logger.warning("Visualization failed, using blank frame")
                    vis_frame = self.blank_frame.copy()
            except Exception as e:
                self.logger.error(f"Visualization error: {e}")
                vis_frame = self.blank_frame.copy()
                cv2.putText(vis_frame, f"Visualization error: {str(e)[:50]}", 
                           (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Calculate FPS
        current_time = time.time()
        fps = 1.0 / max(0.001, current_time - self.last_time)
        self.fps_history.append(fps)
        self.last_time = current_time
        
        # Keep only recent FPS values
        if len(self.fps_history) > 30:
            self.fps_history = self.fps_history[-30:]
        
        # Add FPS overlay
        avg_fps = np.mean(self.fps_history)
        cv2.putText(vis_frame, f"FPS: {avg_fps:.1f}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Display frame
        cv2.imshow(self.window_name, vis_frame)
        
        # Handle keyboard input
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            return False  # Quit
        elif key == ord('p'):
            # Pause - wait for another key press
            cv2.putText(vis_frame, "PAUSED - Press any key to continue", 
                       (vis_frame.shape[1]//4, vis_frame.shape[0]//2), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            cv2.imshow(self.window_name, vis_frame)
            self.logger.info("⏸️  Paused - Press any key to continue...")
            cv2.waitKey(0)
        elif key == ord('s'):
            # Save screenshot
            screenshot_name = f"argus_track_screenshot_{self.frame_count:06d}.jpg"
            cv2.imwrite(screenshot_name, vis_frame)
            self.logger.info(f"📸 Screenshot saved: {screenshot_name}")
        
        return True  # Continue
    
    def _create_visualization(self, frame: np.ndarray,
                             detections: List[Detection],
                             tracks: List[Track],
                             gps_data: Optional[Dict] = None,
                             frame_info: Optional[Dict] = None) -> np.ndarray:
        """Create comprehensive visualization frame"""
        # Safety check for frame
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        # Resize frame to display size if needed
        try:
            vis_frame = self._resize_frame(frame)
        except Exception as e:
            self.logger.error(f"Error resizing frame: {e}")
            return self.blank_frame.copy()
        
        # Calculate scale factors for coordinate adjustment
        scale_x = vis_frame.shape[1] / max(1, frame.shape[1])
        scale_y = vis_frame.shape[0] / max(1, frame.shape[0])
        
        # Draw raw detections first (lighter overlay)
        vis_frame = self._draw_detections(vis_frame, detections, scale_x, scale_y)
        
        # Draw tracks (more prominent)
        vis_frame = self._draw_tracks(vis_frame, tracks, scale_x, scale_y)
        
        # Add information panels
        if self.show_info_panel:
            vis_frame = self._add_info_panel(vis_frame, detections, tracks, gps_data, frame_info)
        
        return vis_frame
    
    def _resize_frame(self, frame: np.ndarray) -> np.ndarray:
        """Resize frame to display size - DEFENSIVE"""
        # Handle None or invalid frame
        if frame is None:
            return self.blank_frame.copy()
        
        if len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        if frame.shape[:2] == (self.display_size[1], self.display_size[0]):
            return frame.copy()
        
        try:
            return cv2.resize(frame, self.display_size)
        except Exception as e:
            self.logger.error(f"Error resizing frame: {e}")
            return self.blank_frame.copy()

    def _draw_detections(self, frame: np.ndarray, detections: List[Detection],
                        scale_x: float, scale_y: float) -> np.ndarray:
        """Draw raw detections with semi-transparent overlay"""
        if frame is None or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        # Safety check - if frame is valid but detections is None
        if detections is None:
            detections = []
            
        # Create overlay for semi-transparency
        try:
            overlay = frame.copy()
            for detection in detections:
                bbox = detection.bbox
                x1, y1, x2, y2 = bbox
                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 255, 255), 1)
                conf_text = f"{detection.score:.2f}"
                cv2.putText(overlay, conf_text, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # Blend overlay with original frame
            result = cv2.addWeighted(frame, 0.8, overlay, 0.2, 0)
            return result
        except Exception as e:
            self.logger.error(f"Error drawing detections: {e}")
            return frame  # Return original frame if drawing fails

    def _draw_tracks(self, frame: np.ndarray, tracks: List[Track],
                    scale_x: float, scale_y: float) -> np.ndarray:
        """Draw tracks with trajectories"""
        if frame is None or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        # Safety check - if frame is valid but tracks is None
        if tracks is None:
            tracks = []
            
        try:
            result = frame.copy()
            for track in tracks:
                # Get track color based on state
                color = TRACK_COLORS.get(track.state, (255, 255, 255))
                
                # Get bounding box
                bbox = track.to_tlbr()
                x1, y1, x2, y2 = bbox
                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                
                # Draw bounding box with thickness based on state
                thickness = 3 if track.state == 'confirmed' else 2
                cv2.rectangle(result, (x1, y1), (x2, y2), color, thickness)
                
                # Draw track info
                track_info = f"ID:{track.track_id} H:{track.hits}"
                if track.state == 'confirmed':
                    track_info += " ✓"
                    
                # Create text background
                text_size = cv2.getTextSize(track_info, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                cv2.rectangle(result, (x1, y1 - text_size[1] - 8),
                            (x1 + text_size[0] + 4, y1), color, -1)
                
                # Draw text
                cv2.putText(result, track_info, (x1 + 2, y1 - 4),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                
                # Draw trajectory for confirmed tracks
                if track.state == 'confirmed' and len(track.detections) > 1:
                    self._draw_trajectory(result, track, scale_x, scale_y, color)
            
            return result
        except Exception as e:
            self.logger.error(f"Error drawing tracks: {e}")
            return frame  # Return original frame if drawing fails

    def _draw_trajectory(self, frame: np.ndarray, track: Track,
                        scale_x: float, scale_y: float, color: Tuple[int, int, int]):
        """Draw track trajectory"""
        try:
            # Get recent detection centers
            recent_detections = track.detections[-min(10, len(track.detections)):]
            
            if len(recent_detections) < 2:
                return
            
            points = []
            for detection in recent_detections:
                center = detection.center
                scaled_center = (int(center[0] * scale_x), int(center[1] * scale_y))
                points.append(scaled_center)
            
            # Draw trajectory lines
            for i in range(1, len(points)):
                cv2.line(frame, points[i-1], points[i], color, 2)
            
            # Draw trajectory points
            for i, point in enumerate(points):
                radius = 3 if i == len(points) - 1 else 2  # Larger for current position
                cv2.circle(frame, point, radius, color, -1)
        except Exception as e:
            self.logger.error(f"Error drawing trajectory: {e}")

    def _add_info_panel(self, frame: np.ndarray,
                      detections: List[Detection],
                      tracks: List[Track],
                      gps_data: Optional[Dict] = None,
                      frame_info: Optional[Dict] = None) -> np.ndarray:
        """Add information panel overlay to visualization"""
        # Safety check for frame
        if frame is None or not isinstance(frame, np.ndarray) or len(frame.shape) != 3:
            return self.blank_frame.copy()
        
        try:
            # Create panel dimensions
            panel_height = 140
            panel_width = 320
            
            # Create semi-transparent overlay
            overlay = frame.copy()
            cv2.rectangle(overlay, 
                         (frame.shape[1] - panel_width - 10, 10),
                         (frame.shape[1] - 10, panel_height + 10), 
                         (0, 0, 0), -1)
            
            # Blend with original frame
            result = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
            
            # Safety checks for parameters
            if detections is None:
                detections = []
            if tracks is None:
                tracks = []
            
            # Add text information
            y_offset = 35
            text_color = (255, 255, 255)
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.5
            
            # Count active and confirmed tracks
            active_tracks = [t for t in tracks if getattr(t, 'state', None) in ['tentative', 'confirmed']]
            confirmed_tracks = [t for t in tracks if getattr(t, 'state', None) == 'confirmed']
            
            # Prepare info lines
            info_lines = [
                f"Frame: {frame_info.get('frame_idx', self.frame_count) if frame_info else self.frame_count}",
                f"Detections: {len(detections)}",
                f"Active Tracks: {len(active_tracks)}",
                f"Confirmed: {len(confirmed_tracks)}",
            ]
            
            # Add best detection score if available
            if detections:
                try:
                    best_detection = max(detections, key=lambda d: getattr(d, 'score', 0))
                    info_lines.append(f"Best score: {getattr(best_detection, 'score', 0):.3f}")
                except:
                    pass
            
            # Add GPS info if available
            if gps_data:
                info_lines.append(f"GPS: {gps_data.get('latitude', 0):.5f}")
                info_lines.append(f"     {gps_data.get('longitude', 0):.5f}")
            
            # Add frame skipping info if available
            if frame_info and 'skipped_frames' in frame_info:
                info_lines.append(f"Frames skipped: {frame_info.get('skipped_frames', 0)}")
            
            # Render text
            for i, line in enumerate(info_lines):
                y_pos = y_offset + i * 18
                cv2.putText(result, line, 
                           (frame.shape[1] - panel_width + 5, y_pos),
                           font, font_scale, text_color, 1)
            
            return result
        except Exception as e:
            self.logger.error(f"Error adding info panel: {e}")
            return frame  # Return original frame if info panel fails

    def close(self):
        """Close the visualization window"""
        try:
            cv2.destroyWindow(self.window_name)
            self.logger.info(f"🖥️  Closed visualization window")
            
            # Print final statistics
            if self.fps_history:
                avg_fps = np.mean(self.fps_history)
                self.logger.info(f"📊 Average FPS: {avg_fps:.1f}")
                self.logger.info(f"📊 Total frames processed: {self.frame_count}")
        except Exception as e:
            self.logger.error(f"Error closing visualization window: {e}")


def draw_tracks(frame: np.ndarray, tracks: List[Track], 
                show_trajectory: bool = True,
                show_id: bool = True,
                show_state: bool = True) -> np.ndarray:
    """
    Draw tracks on frame (existing function - now enhanced with error handling)
    
    Args:
        frame: Input frame
        tracks: List of tracks to draw
        show_trajectory: Whether to show track trajectories
        show_id: Whether to show track IDs
        show_state: Whether to show track states
        
    Returns:
        Frame with track visualizations
    """
    # Handle None inputs
    if frame is None:
        # Create blank frame
        blank_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
        cv2.putText(blank_frame, "No frame data available", (400, 360), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        return blank_frame
    
    if tracks is None:
        tracks = []

    try:
        vis_frame = frame.copy()
        
        for track in tracks:
            # Get color based on state
            color = TRACK_COLORS.get(track.state, (255, 255, 255))
            
            # Draw bounding box
            x1, y1, x2, y2 = track.to_tlbr().astype(int)
            thickness = 3 if track.state == 'confirmed' else 2
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)
            
            # Draw track information
            if show_id or show_state:
                label_parts = []
                if show_id:
                    label_parts.append(f"ID: {track.track_id}")
                if show_state:
                    label_parts.append(f"[{track.state}]")
                
                label = " ".join(label_parts)
                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                
                # Draw label background
                cv2.rectangle(vis_frame, 
                             (x1, y1 - label_size[1] - 10),
                             (x1 + label_size[0], y1),
                             color, -1)
                
                # Draw text
                cv2.putText(vis_frame, label, (x1, y1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # Draw trajectory for confirmed tracks
            if show_trajectory and track.state == 'confirmed' and len(track.detections) > 1:
                points = []
                for det in track.detections[-10:]:  # Last 10 detections
                    center = det.center
                    points.append(center.astype(int))
                
                points = np.array(points)
                cv2.polylines(vis_frame, [points], False, color, 2)
                
                # Draw points
                for point in points:
                    cv2.circle(vis_frame, tuple(point), 3, color, -1)
        
        return vis_frame
    except Exception as e:
        # In case of error, log and return original frame
        logging.error(f"Error in draw_tracks: {e}")
        return frame


# Additional utility function
def create_track_overlay(frame: np.ndarray, tracks: List[Track],
                        alpha: float = 0.3) -> np.ndarray:
    """
    Create semi-transparent overlay with track information
    
    Args:
        frame: Input frame
        tracks: List of tracks
        alpha: Transparency level (0-1)
        
    Returns:
        Frame with overlay
    """
    # Handle None inputs
    if frame is None:
        blank_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
        return blank_frame
    
    if tracks is None or len(tracks) == 0:
        return frame.copy()
    
    try:
        overlay = np.zeros_like(frame)
        
        for track in tracks:
            if track.state != 'confirmed':
                continue
                
            # Create mask for track region
            mask = np.zeros(frame.shape[:2], dtype=np.uint8)
            x1, y1, x2, y2 = track.to_tlbr().astype(int)
            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)
            
            # Apply color overlay
            color = TRACK_COLORS[track.state]
            overlay[mask > 0] = color
        
        # Blend with original frame
        result = cv2.addWeighted(frame, 1 - alpha, overlay, alpha, 0)
        
        return result
    except Exception as e:
        logging.error(f"Error in create_track_overlay: {e}")
        return frame.copy()  # Return original frame if error

================
File: argus_track/config.py
================
# argus_track/config.py (UPDATED TRACKING PARAMETERS)
"""Configuration with improved tracking parameters for static objects"""

from dataclasses import dataclass
from typing import Optional, Dict, Any
import yaml
import json
import pickle
import numpy as np
from pathlib import Path


@dataclass
class TrackerConfig:
    """Configuration for ByteTrack light post tracker - OPTIMIZED FOR STATIC OBJECTS"""
    
    # DETECTION THRESHOLDS - More restrictive to reduce noise
    track_thresh: float = 0.3          # Higher threshold to avoid weak detections
    match_thresh: float = 0.3          # Lower IoU threshold for better matching of static objects
    
    # TRACK MANAGEMENT - Extended for static objects
    track_buffer: int = 100            # Keep lost tracks longer (static objects may be occluded)
    min_box_area: float = 200.0        # Larger minimum area to filter small noise
    
    # STATIC OBJECT DETECTION - More conservative
    static_threshold: float = 5.0      # Pixels - allow slightly more movement for static detection
    min_static_frames: int = 15        # Require more frames to confirm static object
    max_track_age: int = 1000          # Maximum age before removing track
    
    # TRACK CONFIRMATION - More strict requirements
    min_hits: int = 2                  # Require more hits before confirming track
    max_time_lost: int = 50            # Maximum frames without detection before marking as lost
    
    # DUPLICATE TRACK MERGING - New parameters
    merge_distance_threshold: float = 30.0    # Pixel distance to consider tracks duplicates
    merge_iou_threshold: float = 0.5          # IoU threshold for merging tracks
    enable_track_merging: bool = True         # Enable automatic duplicate track merging
    
    # GPS AND GEOLOCATION
    gps_frame_interval: int = 6        # Process every 6th frame for GPS (10fps from 60fps)
    min_gps_points: int = 5            # Minimum GPS points needed for geolocation
    max_geolocation_std: float = 0.0001  # Maximum standard deviation for reliable geolocation
    
    # STEREO PARAMETERS
    stereo_mode: bool = False          # Default to monocular
    stereo_match_threshold: float = 0.7
    max_stereo_distance: float = 100.0
    
    @classmethod
    def create_optimized_config(cls) -> 'TrackerConfig':
        """Create optimized configuration for static LED detection"""
        return cls(
            track_thresh=0.1,              # Higher confidence threshold
            match_thresh=0.6,              # More lenient matching for static objects
            track_buffer=150,              # Longer buffer for static objects
            min_box_area=300.0,            # Filter out small detections
            static_threshold=8.0,          # Allow some camera shake
            min_static_frames=20,          # More frames needed for static confirmation
            max_track_age=2000,            # Very long track lifetime
            min_hits=3,                    # More hits required for confirmation
            max_time_lost=75,              # Longer time before considering lost
            merge_distance_threshold=50.0, # Merge nearby duplicate tracks
            merge_iou_threshold=0.4,       # IoU threshold for merging
            enable_track_merging=True,     # Enable merging
            gps_frame_interval=6,          # GPS sync
            min_gps_points=8,              # More GPS points for better accuracy
            max_geolocation_std=0.00005    # Stricter geolocation reliability
        )
    
    @classmethod
    def from_yaml(cls, yaml_path: str) -> 'TrackerConfig':
        """Load configuration from YAML file"""
        with open(yaml_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)
    
    @classmethod
    def from_json(cls, json_path: str) -> 'TrackerConfig':
        """Load configuration from JSON file"""
        with open(json_path, 'r') as f:
            config_dict = json.load(f)
        return cls(**config_dict)
    
    def save_yaml(self, output_path: str) -> None:
        """Save configuration to YAML file"""
        with open(output_path, 'w') as f:
            yaml.dump(self.__dict__, f, default_flow_style=False)
    
    def save_json(self, output_path: str) -> None:
        """Save configuration to JSON file"""
        with open(output_path, 'w') as f:
            json.dump(self.__dict__, f, indent=2)

@dataclass
class DetectorConfig:
    """Configuration for object detectors"""
    model_path: str
    config_path: str
    target_classes: Optional[list] = None
    confidence_threshold: float = 0.5
    nms_threshold: float = 0.4
    model_type: str = "yolov11"        # Support for YOLOv11
    
@dataclass
class StereoCalibrationConfig:
    """Stereo camera calibration parameters"""
    camera_matrix_left: np.ndarray
    camera_matrix_right: np.ndarray
    dist_coeffs_left: np.ndarray
    dist_coeffs_right: np.ndarray
    R: np.ndarray                      # Rotation matrix between cameras
    T: np.ndarray                      # Translation vector between cameras
    E: Optional[np.ndarray] = None     # Essential matrix
    F: Optional[np.ndarray] = None     # Fundamental matrix
    P1: Optional[np.ndarray] = None    # Left camera projection matrix
    P2: Optional[np.ndarray] = None    # Right camera projection matrix
    Q: Optional[np.ndarray] = None     # Disparity-to-depth mapping matrix
    baseline: float = 0.0              # Distance between cameras (meters)
    image_width: int = 1920
    image_height: int = 1080
    
    @classmethod
    def from_pickle(cls, calibration_path: str) -> 'StereoCalibrationConfig':
        """Load stereo calibration from pickle file"""
        with open(calibration_path, 'rb') as f:
            calib_data = pickle.load(f)
        
        # Calculate baseline if not provided
        baseline = calib_data.get('baseline', 0.0)
        if baseline == 0.0 and 'T' in calib_data:
            baseline = float(np.linalg.norm(calib_data['T']))
        
        return cls(
            camera_matrix_left=calib_data['camera_matrix_left'],
            camera_matrix_right=calib_data['camera_matrix_right'],
            dist_coeffs_left=calib_data['dist_coeffs_left'],
            dist_coeffs_right=calib_data['dist_coeffs_right'],
            R=calib_data['R'],
            T=calib_data['T'],
            E=calib_data.get('E'),
            F=calib_data.get('F'),
            P1=calib_data.get('P1'),
            P2=calib_data.get('P2'),
            Q=calib_data.get('Q'),
            baseline=baseline,
            image_width=calib_data.get('image_width', 1920),
            image_height=calib_data.get('image_height', 1080)
        )
    
    def save_pickle(self, output_path: str) -> None:
        """Save calibration to pickle file"""
        calib_data = {
            'camera_matrix_left': self.camera_matrix_left,
            'camera_matrix_right': self.camera_matrix_right,
            'dist_coeffs_left': self.dist_coeffs_left,
            'dist_coeffs_right': self.dist_coeffs_right,
            'R': self.R,
            'T': self.T,
            'E': self.E,
            'F': self.F,
            'P1': self.P1,
            'P2': self.P2,
            'Q': self.Q,
            'baseline': self.baseline,
            'image_width': self.image_width,
            'image_height': self.image_height
        }
        
        with open(output_path, 'wb') as f:
            pickle.dump(calib_data, f)

@dataclass
class CameraConfig:
    """Camera calibration parameters (backward compatibility)"""
    camera_matrix: list
    distortion_coeffs: list
    image_width: int
    image_height: int
    
    @classmethod
    def from_file(cls, calibration_path: str) -> 'CameraConfig':
        """Load camera configuration from file"""
        with open(calibration_path, 'r') as f:
            data = json.load(f)
        return cls(**data)

================
File: argus_track/detectors/yolov11.py
================
# argus_track/detectors/yolov11.py (FIXED)
"""YOLOv11 detector implementation with proper class handling"""

import cv2
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path
import torch
import torchvision.transforms as transforms

from .base import ObjectDetector


class YOLOv11Detector(ObjectDetector):
    """YOLOv11-based object detector implementation with PyTorch backend"""
    
    def __init__(self, 
                 model_path: str,
                 target_classes: Optional[List[str]] = None,
                 confidence_threshold: float = 0.5,
                 nms_threshold: float = 0.4,
                 device: str = 'auto',
                 input_size: int = 640):
        """
        Initialize YOLOv11 detector
        
        Args:
            model_path: Path to YOLOv11 model file (.pt)
            target_classes: List of class names to detect (None for all)
            confidence_threshold: Minimum confidence for detections
            nms_threshold: Non-maximum suppression threshold
            device: Device to use ('cpu', 'cuda', or 'auto')
            input_size: Model input size (typically 640)
        """
        self.logger = logging.getLogger(f"{__name__}.YOLOv11Detector")
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.input_size = input_size
        
        # Set device
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        self.logger.info(f"Using device: {self.device}")
        
        # Load model
        self.model = self._load_model()
        
        # Get class names directly from the loaded model
        self.class_names = list(self.model.names.values())
        self.logger.info(f"Model classes: {dict(self.model.names)}")

        # Set target classes - FIXED: Now handles all classes properly
        if target_classes is None:
            self.target_classes = self.class_names.copy()
            self.logger.info(f"Using all model classes: {self.target_classes}")
        else:
            # Filter to only valid classes that exist in the model
            valid_classes = [cls for cls in target_classes if cls in self.class_names]
            if not valid_classes:
                self.logger.warning(f"None of the target classes {target_classes} found in model. Using all model classes.")
                self.target_classes = self.class_names.copy()
            else:
                self.target_classes = valid_classes
                self.logger.info(f"Using filtered target classes: {self.target_classes}")
        
        # Target class indices - FIXED: Now maps to actual class names
        self.target_class_indices = [
            i for i, name in enumerate(self.class_names) 
            if name in self.target_classes
        ]
        
        self.logger.info(f"Initialized YOLOv11 detector with {len(self.target_classes)} target classes")
        self.logger.info(f"Target class indices: {self.target_class_indices}")
    
    def _load_model(self):
        """Load YOLOv11 model"""
        try:
            # Try to load with ultralytics (if available)
            try:
                from ultralytics import YOLO
                model = YOLO(self.model_path)
                model.to(self.device)
                self.logger.info("Loaded YOLOv11 model using ultralytics")
                return model
            except ImportError:
                self.logger.warning("ultralytics not available, falling back to torch.hub")
                
            # Fallback to torch.hub or direct torch loading
            if self.model_path.endswith('.pt'):
                model = torch.jit.load(self.model_path, map_location=self.device)
                model.eval()
                self.logger.info("Loaded YOLOv11 model using torch.jit")
                return model
            else:
                raise ValueError(f"Unsupported model format: {self.model_path}")
                
        except Exception as e:
            self.logger.error(f"Failed to load YOLOv11 model: {e}")
            raise
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect objects in frame using YOLOv11
        
        Args:
            frame: Input image
            
        Returns:
            List of detections
        """
        try:
            # Check if using ultralytics YOLO
            if hasattr(self.model, 'predict'):
                return self._detect_ultralytics(frame)
            else:
                return self._detect_torch(frame)
        except Exception as e:
            self.logger.error(f"Detection failed: {e}")
            return []
    
    def _detect_ultralytics(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """Detection using ultralytics YOLO - FIXED to handle all classes"""
        
        # Run inference with low confidence to catch all possible detections
        results = self.model.predict(
            frame, 
            conf=0.001,  # Very low confidence to catch everything
            iou=self.nms_threshold,
            verbose=False
        )
        
        detections = []
        
        if results and len(results) > 0:
            result = results[0]
            
            if result.boxes is not None:
                boxes = result.boxes.xyxy.cpu().numpy()
                scores = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy().astype(int)
                
                self.logger.debug(f"Raw detections: {len(boxes)} boxes, classes: {set(classes)}")
                
                for i, (box, score, cls_id) in enumerate(zip(boxes, scores, classes)):
                    # FIXED: Check if class is in our target classes instead of hardcoded check
                    if cls_id < len(self.class_names):
                        class_name = self.class_names[cls_id]
                        
                        # Only keep detections of target classes with sufficient confidence
                        if (class_name in self.target_classes and 
                            score >= self.confidence_threshold):
                            
                            detections.append({
                                'bbox': box.tolist(),
                                'score': float(score),
                                'class_name': class_name,
                                'class_id': cls_id
                            })
                            
                            self.logger.debug(f"Kept detection: {class_name} (ID:{cls_id}), Conf: {score:.4f}")
                        else:
                            self.logger.debug(f"Filtered out: {class_name} (ID:{cls_id}), Conf: {score:.4f}")
                    else:
                        self.logger.warning(f"Invalid class ID: {cls_id}")
        
        self.logger.debug(f"Final detections: {len(detections)}")
        return detections

    def _detect_torch(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """Detection using pure PyTorch model"""
        # Preprocess image
        input_tensor = self._preprocess_image(frame)
        
        # Run inference
        with torch.no_grad():
            predictions = self.model(input_tensor)
        
        # Post-process results
        detections = self._postprocess_predictions(predictions, frame.shape)
        
        return detections
    
    def _preprocess_image(self, frame: np.ndarray) -> torch.Tensor:
        """Preprocess image for YOLOv11"""
        # Resize to model input size
        height, width = frame.shape[:2]
        
        # Calculate scale factor
        scale = min(self.input_size / width, self.input_size / height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        # Resize image
        resized = cv2.resize(frame, (new_width, new_height))
        
        # Pad to square
        top = (self.input_size - new_height) // 2
        bottom = self.input_size - new_height - top
        left = (self.input_size - new_width) // 2
        right = self.input_size - new_width - left
        
        padded = cv2.copyMakeBorder(
            resized, top, bottom, left, right, 
            cv2.BORDER_CONSTANT, value=(114, 114, 114)
        )
        
        # Convert to tensor
        image_tensor = torch.from_numpy(padded).permute(2, 0, 1).float()
        image_tensor /= 255.0  # Normalize to [0, 1]
        
        # Add batch dimension
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        return image_tensor
    
    def _postprocess_predictions(self, 
                                predictions: torch.Tensor, 
                                original_shape: tuple) -> List[Dict[str, Any]]:
        """Post-process YOLOv11 predictions"""
        detections = []
        
        # Assuming predictions shape: [batch, num_boxes, 85] (x, y, w, h, conf, classes...)
        pred = predictions[0]  # Remove batch dimension
        
        # Filter by confidence
        conf_mask = pred[:, 4] >= self.confidence_threshold
        pred = pred[conf_mask]
        
        if len(pred) == 0:
            return detections
        
        # Convert boxes from center format to corner format
        boxes = pred[:, :4].clone()
        boxes[:, 0] = pred[:, 0] - pred[:, 2] / 2  # x1 = cx - w/2
        boxes[:, 1] = pred[:, 1] - pred[:, 3] / 2  # y1 = cy - h/2
        boxes[:, 2] = pred[:, 0] + pred[:, 2] / 2  # x2 = cx + w/2
        boxes[:, 3] = pred[:, 1] + pred[:, 3] / 2  # y2 = cy + h/2
        
        # Scale boxes back to original image size
        scale_x = original_shape[1] / self.input_size
        scale_y = original_shape[0] / self.input_size
        
        boxes[:, [0, 2]] *= scale_x
        boxes[:, [1, 3]] *= scale_y
        
        # Get class predictions
        class_probs = pred[:, 5:]
        class_ids = torch.argmax(class_probs, dim=1)
        max_class_probs = torch.max(class_probs, dim=1)[0]
        
        # Apply NMS
        keep_indices = torchvision.ops.nms(
            boxes, 
            pred[:, 4] * max_class_probs,  # Combined confidence
            self.nms_threshold
        )
        
        # Filter results
        final_boxes = boxes[keep_indices]
        final_scores = pred[keep_indices, 4]
        final_classes = class_ids[keep_indices]
        
        # Convert to detection format
        for box, score, cls_id in zip(final_boxes, final_scores, final_classes):
            cls_id = int(cls_id.item())
            
            # Filter by target classes - FIXED: Now uses proper class checking
            if cls_id < len(self.class_names):
                class_name = self.class_names[cls_id]
                
                if class_name in self.target_classes:
                    detections.append({
                        'bbox': box.cpu().numpy().tolist(),
                        'score': float(score.item()),
                        'class_name': class_name,
                        'class_id': cls_id
                    })
        
        return detections
    
    def get_class_names(self) -> List[str]:
        """Get list of detectable class names"""
        return self.class_names.copy()
    
    def set_confidence_threshold(self, threshold: float) -> None:
        """Set detection confidence threshold"""
        self.confidence_threshold = threshold
        self.logger.info(f"Updated confidence threshold to {threshold}")
    
    def set_nms_threshold(self, threshold: float) -> None:
        """Set NMS threshold"""
        self.nms_threshold = threshold
        self.logger.info(f"Updated NMS threshold to {threshold}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            'model_path': self.model_path,
            'device': str(self.device),
            'input_size': self.input_size,
            'confidence_threshold': self.confidence_threshold,
            'nms_threshold': self.nms_threshold,
            'target_classes': self.target_classes,
            'num_classes': len(self.class_names),
            'all_classes': dict(enumerate(self.class_names))
        }

================
File: argus_track/__init__.py
================
# argus_track/__init__.py (UPDATED)

"""
Argus Track: Stereo ByteTrack Light Post Tracking System
========================================================

A specialized implementation of ByteTrack for tracking light posts in stereo video sequences
with GPS integration for precise 3D geolocation estimation.

Key Features:
- Stereo vision processing with 3D triangulation
- Optimized for static/slow-moving objects
- GPS data integration for geolocation
- YOLOv11 support for advanced object detection
- Modular architecture with clear separation of concerns
- Comprehensive logging and error handling
- Type hints and documentation throughout

Author: Argus Track Team
Date: 2025
License: MIT
"""

from argus_track.__version__ import __version__
from argus_track.config import TrackerConfig, StereoCalibrationConfig, DetectorConfig
from argus_track.core import Detection, Track, GPSData
from argus_track.core.stereo import StereoDetection, StereoFrame, StereoTrack
from argus_track.trackers import ByteTrack, EnhancedLightPostTracker
from argus_track.trackers.stereo_lightpost_tracker import EnhancedStereoLightPostTracker
from argus_track.detectors.mock import MockDetector
from argus_track.detectors import YOLODetector, ObjectDetector
from argus_track.detectors.yolov11 import YOLOv11Detector
from argus_track.stereo import StereoMatcher, StereoTriangulator, StereoCalibrationManager
from argus_track.exceptions import (
    ArgusTrackError, 
    DetectorError, 
    TrackerError,
    ConfigurationError,
    GPSError,
    VideoError
)
from argus_track.analysis import StaticObjectAnalyzer

__all__ = [
    "__version__",
    "TrackerConfig",
    "StereoCalibrationConfig", 
    "DetectorConfig",
    "Detection",
    "Track",
    "GPSData",
    "StereoDetection",
    "StereoFrame", 
    "StereoTrack",
    "ByteTrack",
    "EnhancedLightPostTracker",
    "EnhancedStereoLightPostTracker",
    "YOLODetector",
    "YOLOv11Detector",
    "ObjectDetector",
    "MockDetector",
    "StereoMatcher",
    "StereoTriangulator", 
    "StereoCalibrationManager",
    "StaticObjectAnalyzer",
    "ArgusTrackError",
    "DetectorError",
    "TrackerError",
    "ConfigurationError", 
    "GPSError",
    "VideoError"
]

================
File: argus_track/main.py
================
"""Main entry point for Argus Track with extreme performance optimizations"""

import argparse
import logging
from pathlib import Path
from typing import Optional
import os
import sys
import time
import numpy as np
import cv2

from argus_track import (
    TrackerConfig,
    StereoCalibrationConfig,
    YOLODetector,
    YOLOv11Detector,
    MockDetector,
    __version__
)
from argus_track.trackers.stereo_lightpost_tracker import EnhancedStereoLightPostTracker
from argus_track.trackers.lightpost_tracker import EnhancedLightPostTracker
from argus_track.utils import setup_logging, load_gps_data
from argus_track.utils.gps_extraction import extract_gps_from_stereo_videos, save_gps_to_csv
from argus_track.stereo import StereoCalibrationManager

# Import the extreme performance optimizations if available
try:
    from argus_track.utils.extreme_performance import (
        PerformanceOptimizer, 
        optimize_numpy,
        optimize_video_capture,
        optimize_detector
    )
    EXTREME_PERFORMANCE_AVAILABLE = True
except ImportError:
    EXTREME_PERFORMANCE_AVAILABLE = False


def create_detector(detector_type: str, 
                    model_path: Optional[str] = None,
                    target_classes: Optional[list] = None,
                    confidence_threshold: float = 0.5,
                    device: str = 'auto',
                    optimizer=None):
    """Create detector based on type with optimizations"""
    
    if detector_type == 'yolov11' and model_path:
        try:
            detector = YOLOv11Detector(
                model_path=model_path,
                target_classes=target_classes,
                confidence_threshold=confidence_threshold,
                device=device
            )
            
            # Apply performance optimizations if available
            if optimizer:
                optimizer.optimize_yolo_inference(detector.model)
            
            return detector
        except Exception as e:
            logging.warning(f"Failed to load YOLOv11: {e}, falling back to mock detector")
            return MockDetector(target_classes=target_classes)
    
    elif detector_type == 'yolo' and model_path:
        # Legacy YOLO support
        try:
            config_path = Path(model_path).with_suffix('.cfg')
            weights_path = Path(model_path).with_suffix('.weights')
            
            if not weights_path.exists():
                weights_path = Path(model_path)
            
            detector = YOLODetector(
                model_path=str(weights_path),
                config_path=str(config_path),
                target_classes=target_classes
            )
            
            # Apply detector optimizations
            if optimizer:
                optimize_detector(detector)
            
            return detector
        except Exception as e:
            logging.warning(f"Failed to load YOLO: {e}, falling back to mock detector")
            return MockDetector(target_classes=target_classes)
    
    else:
        return MockDetector(target_classes=target_classes)


def main():
    """Main function for enhanced stereo light post tracking with GPS extraction"""
    # Apply numpy optimizations first
    if EXTREME_PERFORMANCE_AVAILABLE:
        optimize_numpy()
    
    parser = argparse.ArgumentParser(
        description=f"Argus Track: Enhanced Stereo Light Post Tracking System v{__version__}",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
            Examples:
                # Enhanced stereo tracking with automatic GPS extraction
                argus_track --stereo left.mp4 right.mp4 --calibration stereo.pkl --detector yolov11 --model yolov11n.pt --auto-gps
                
                # Stereo tracking with existing GPS file
                argus_track --stereo left.mp4 right.mp4 --calibration stereo.pkl --gps gps.csv
                
                # Extract GPS only (no tracking)
                argus_track --extract-gps-only left.mp4 right.mp4 --output gps_data.csv
                
                # Monocular tracking (legacy mode)
                argus_track input.mp4 --detector yolo --model yolov4.weights
                
                # Extreme performance mode
                argus_track input.mp4 --detector yolo --model yolov4.weights --skip-frames 9 --resolution-scale 0.5
            """
    )
    
    # Video input arguments
    parser.add_argument('input_video', type=str, nargs='?',
                       help='Path to input video file (for monocular mode)')
    parser.add_argument('--stereo', nargs=2, metavar=('LEFT', 'RIGHT'),
                       help='Paths to left and right stereo videos')
    
    # GPS extraction options
    parser.add_argument('--auto-gps', action='store_true',
                       help='Automatically extract GPS data from videos')
    parser.add_argument('--gps-method', type=str, default='auto',
                       choices=['auto', 'exiftool', 'gopro_api'],
                       help='GPS extraction method (default: auto)')
    parser.add_argument('--extract-gps-only', action='store_true',
                       help='Only extract GPS data, do not run tracking')
    parser.add_argument('--save-gps-csv', action='store_true',
                       help='Save extracted GPS data to CSV file')
    
    # Calibration and GPS
    parser.add_argument('--calibration', type=str,
                       help='Path to stereo calibration file (.pkl)')
    parser.add_argument('--gps', type=str,
                       help='Path to GPS data CSV file')
    
    # Detector options
    parser.add_argument('--detector', type=str, default='mock',
                       choices=['yolov11', 'yolo', 'mock'],
                       help='Detector type to use')
    parser.add_argument('--model', type=str,
                       help='Path to detection model file')
    parser.add_argument('--target-classes', nargs="*", default=None, 
                        help="Optional: space-separated list of target class names. If not set, uses all model classes.")

    # Output options
    parser.add_argument('--output', type=str,
                       help='Path for output video or GPS CSV file')
    parser.add_argument('--config', type=str,
                       help='Path to configuration file')
    parser.add_argument('--no-save-video', action='store_true',
                       help='Do not save output video (still save tracking data)')
    
    # Logging options
    parser.add_argument('--log-file', type=str,
                       help='Path to log file')
    parser.add_argument('--verbose', action='store_true',
                       help='Enable verbose logging')
    parser.add_argument('--no-save', action='store_true',
                       help='Do not save tracking results')
    
    # Tracking parameters
    parser.add_argument('--track-thresh', type=float, default=0.5,
                       help='Detection confidence threshold')
    parser.add_argument('--match-thresh', type=float, default=0.8,
                       help='IoU threshold for matching')
    parser.add_argument('--track-buffer', type=int, default=50,
                       help='Number of frames to keep lost tracks')
    
    # Stereo parameters
    parser.add_argument('--gps-interval', type=int, default=6,
                       help='GPS frame interval (process every Nth frame)')
    parser.add_argument('--stereo-thresh', type=float, default=0.7,
                       help='Stereo matching threshold')
    
    # Performance options
    parser.add_argument('--skip-frames', type=int, default=0,
                       help='Skip N frames between processing (0 = process all frames)')
    parser.add_argument('--resolution-scale', type=float, default=1.0,
                       help='Scale factor for input resolution (0.5 = half size)')
    
    # Real-time visualization options
    parser.add_argument('--show-realtime', action='store_true',
                       help='Show real-time detection and tracking visualization')
    parser.add_argument('--display-size', nargs=2, type=int, default=[1280, 720],
                       metavar=('WIDTH', 'HEIGHT'),
                       help='Real-time display window size (default: 1280x720)')
    
    # Extreme performance options
    parser.add_argument('--hardware-acceleration', action='store_true',
                       help='Use hardware acceleration if available (CUDA/OpenCL)')
    parser.add_argument('--memory-limit', type=int, default=0,
                       help='Memory usage limit in MB (0 = no limit)')
    parser.add_argument('--profile', action='store_true',
                       help='Enable performance profiling')
    
    args = parser.parse_args()
    
    # Validate input arguments
    if not args.stereo and not args.input_video:
        parser.error("Must provide either --stereo LEFT RIGHT or input_video")
    
    if args.stereo and args.input_video:
        parser.error("Cannot use both --stereo and input_video modes")
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    setup_logging(log_file=args.log_file, level=log_level)
    logger = logging.getLogger(__name__)
    
    logger.info(f"Argus Track: Enhanced Stereo Light Post Tracking System v{__version__}")
    
    # Initialize performance optimizer if available
    optimizer = None
    if EXTREME_PERFORMANCE_AVAILABLE:
        optimizer = PerformanceOptimizer()
        hw_settings = optimizer.optimize_hardware_settings()
        logger.info(f"Applied hardware optimizations: {hw_settings}")
    else:
        logger.info("Extreme performance optimizations not available")
    
    # Performance warning for real-time visualization
    if args.show_realtime:
        logger.warning("Real-time visualization is enabled, which may significantly slow down processing")
    
    # Warning about resolution scaling
    if args.resolution_scale < 1.0:
        logger.warning(f"Resolution scaling is set to {args.resolution_scale:.2f}x, which may reduce detection accuracy")
    
    # Determine processing mode
    stereo_mode = args.stereo is not None
    
    if stereo_mode:
        logger.info("Running in ENHANCED STEREO mode with GPS extraction")
        left_video, right_video = args.stereo
        
        # Validate stereo inputs
        if not Path(left_video).exists():
            logger.error(f"Left video not found: {left_video}")
            return 1
        if not Path(right_video).exists():
            logger.error(f"Right video not found: {right_video}")
            return 1
        
        # Handle GPS extraction only mode
        if args.extract_gps_only:
            logger.info("GPS extraction only mode")
            
            gps_data, method_used = extract_gps_from_stereo_videos(
                left_video, right_video, args.gps_method
            )
            
            if gps_data:
                output_path = args.output or f"{Path(left_video).stem}_gps_data.csv"
                save_gps_to_csv(gps_data, output_path)
                
                logger.info(f"✅ Successfully extracted {len(gps_data)} GPS points using {method_used}")
                logger.info(f"Saved GPS data to: {output_path}")
                return 0
            else:
                logger.error("❌ No GPS data could be extracted from the videos")
                return 1
        
        # Load stereo calibration
        if args.calibration:
            if not Path(args.calibration).exists():
                logger.error(f"Calibration file not found: {args.calibration}")
                return 1
            
            try:
                stereo_calibration = StereoCalibrationConfig.from_pickle(args.calibration)
                logger.info(f"Loaded stereo calibration from {args.calibration}")
            except Exception as e:
                logger.error(f"Failed to load calibration: {e}")
                return 1
        else:
            logger.warning("No calibration provided, creating sample calibration")
            calib_manager = StereoCalibrationManager()
            stereo_calibration = calib_manager.create_sample_calibration()
    else:
        logger.info("Running in MONOCULAR mode")
        if not Path(args.input_video).exists():
            logger.error(f"Input video not found: {args.input_video}")
            return 1
    
    # Load configuration
    if args.config:
        try:
            config = TrackerConfig.from_yaml(args.config)
            logger.info(f"Loaded configuration from {args.config}")
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            return 1
    else:
        config = TrackerConfig(
            track_thresh=args.track_thresh,
            match_thresh=args.match_thresh,
            track_buffer=args.track_buffer,
            stereo_mode=stereo_mode,
            stereo_match_threshold=args.stereo_thresh,
            gps_frame_interval=args.gps_interval
        )
    
    # Initialize detector
    start_time = time.time()
    try:
        detector = create_detector(
            detector_type=args.detector,
            model_path=args.model,
            target_classes=args.target_classes,
            confidence_threshold=args.track_thresh,
            device='cuda' if args.hardware_acceleration else 'auto',
            optimizer=optimizer
        )
        detector_init_time = time.time() - start_time
        logger.info(f"Initialized {args.detector} detector in {detector_init_time:.2f}s")
    except Exception as e:
        logger.error(f"Failed to initialize detector: {e}")
        return 1
    
    # Process videos
    try:
        if stereo_mode:
            # We don't currently implement real-time visualization for stereo mode
            # but we'll warn the user if they've requested it
            if args.show_realtime:
                logger.warning("Real-time visualization not supported in stereo mode, ignoring --show-realtime")
            
            # Initialize enhanced stereo tracker
            tracker = EnhancedStereoLightPostTracker(
                config=config,
                detector=detector,
                stereo_calibration=stereo_calibration
            )
            logger.info("Initialized enhanced stereo tracker with GPS extraction")
            
            # Determine processing method
            if args.auto_gps or (not args.gps and not args.extract_gps_only):
                # Automatic GPS extraction mode
                logger.info("Using automatic GPS extraction from videos")
                tracks = tracker.process_stereo_video_with_auto_gps(
                    left_video_path=left_video,
                    right_video_path=right_video,
                    output_path=args.output if not args.no_save_video else None,
                    save_results=not args.no_save,
                    gps_extraction_method=args.gps_method,
                    save_extracted_gps=args.save_gps_csv or True
                )
            else:
                # Load existing GPS data
                gps_data = None
                if args.gps:
                    try:
                        gps_data = load_gps_data(args.gps)
                        logger.info(f"Loaded {len(gps_data)} GPS data points from file")
                    except Exception as e:
                        logger.error(f"Failed to load GPS data: {e}")
                
                # Standard stereo processing
                tracks = tracker.process_stereo_video(
                    left_video_path=left_video,
                    right_video_path=right_video,
                    gps_data=gps_data,
                    output_path=args.output if not args.no_save_video else None,
                    save_results=not args.no_save
                )
            
            # Print enhanced stereo statistics
            stats = tracker.get_enhanced_tracking_statistics()
            logger.info("=== Enhanced Stereo Tracking Statistics ===")
            for key, value in stats.items():
                logger.info(f"  {key}: {value}")
            
            # Print location results with accuracy info
            if hasattr(tracker, 'estimated_locations') and tracker.estimated_locations:
                logger.info("=== Estimated Locations with Accuracy ===")
                for track_id, location in tracker.estimated_locations.items():
                    logger.info(
                        f"Track {track_id}: ({location.latitude:.6f}, {location.longitude:.6f}) "
                        f"accuracy: {location.accuracy:.1f}m, reliability: {location.reliability:.2f}"
                    )
                
                # Calculate average accuracy
                avg_accuracy = sum(loc.accuracy for loc in tracker.estimated_locations.values()) / len(tracker.estimated_locations)
                logger.info(f"Average geolocation accuracy: {avg_accuracy:.1f} meters")
                
                if avg_accuracy <= 2.0:
                    logger.info("🎯 TARGET ACHIEVED: Sub-2-meter accuracy!")
                elif avg_accuracy <= 5.0:
                    logger.info("✅ Good accuracy achieved (< 5m)")
                else:
                    logger.warning("⚠️  Accuracy above target (> 5m)")
            else:
                logger.info("No locations estimated (no static objects found or GPS data unavailable)")
                
        else:
            # Real-time visualization is controlled by the command-line flag
            show_realtime = args.show_realtime
            
            # Create display size tuple for the visualizer
            display_size = tuple(args.display_size) if args.display_size else (1280, 720)
            
            # Apply extreme performance optimizations
            skip_frames = args.skip_frames
            resolution_scale = args.resolution_scale

            tracker = EnhancedLightPostTracker(
                config=config,
                detector=detector,
                camera_config=None,
                show_realtime=show_realtime,
                display_size=display_size,
                skip_frames=skip_frames
            )
            
            if show_realtime:
                logger.info("Initialized enhanced monocular tracker with real-time visualization")
                logger.info("Real-time controls:")
                logger.info("  Press 'q' to quit processing")
                logger.info("  Press 'p' to pause/resume")
                logger.info("  Press 's' to save screenshot")
            else:
                logger.info("Initialized enhanced monocular tracker (no real-time display)")
                logger.info("Processing with maximum performance (no visualization)")

            # Apply extreme performance optimizations
            if optimizer:
                logger.info(f"Applied resolution scaling: {resolution_scale:.2f}x")
                logger.info(f"Processing every {skip_frames + 1}th frame")
            
            # Load GPS data if provided or available
            gps_data = None
            if args.gps:
                try:
                    gps_data = load_gps_data(args.gps)
                    logger.info(f"Loaded {len(gps_data)} GPS data points")
                except Exception as e:
                    logger.error(f"Failed to load GPS data: {e}")
            else:
                # Try to find GPS data automatically
                video_path = Path(args.input_video)
                auto_gps_path = video_path.with_suffix('.csv')
                if auto_gps_path.exists():
                    try:
                        gps_data = load_gps_data(str(auto_gps_path))
                        logger.info(f"Auto-loaded {len(gps_data)} GPS points from {auto_gps_path}")
                    except Exception as e:
                        logger.warning(f"Failed to auto-load GPS data: {e}")
            
            # Set memory limits if requested
            if args.memory_limit > 0 and optimizer:
                optimizer.memory_limit = args.memory_limit
                logger.info(f"Set memory limit to {args.memory_limit}MB")
            
            # Start profiling if requested
            if args.profile:
                try:
                    import cProfile
                    profiler = cProfile.Profile()
                    profiler.enable()
                    logger.info("Performance profiling enabled")
                except ImportError:
                    logger.warning("cProfile not available, profiling disabled")
                    profiler = None
            else:
                profiler = None
            
            # Enhanced monocular processing with GPS geolocation
            video_path = args.input_video
            process_start_time = time.time()
            
            # Process video with extreme performance optimizations
            if optimizer and resolution_scale < 1.0:
                # Open video to get dimensions for resolution scaling
                import cv2
                cap = cv2.VideoCapture(video_path)
                if cap.isOpened():
                    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    cap.release()
                    
                    # Set resolution scaling
                    target_width = int(width * resolution_scale)
                    target_height = int(height * resolution_scale)
                    optimizer.set_resolution_scaling(width, height, target_width, target_height)
                    
                    logger.info(f"Scaled resolution from {width}x{height} to {target_width}x{target_height}")

            tracks = tracker.process_video(
                video_path=video_path,
                gps_data=gps_data,
                output_path=args.output if not args.no_save_video else None,
                save_results=not args.no_save,
                resolution_scale=resolution_scale if optimizer else 1.0
            )
            
            processing_time = time.time() - process_start_time
            logger.info(f"Video processing completed in {processing_time:.2f}s")
            
            # Stop profiling if enabled
            if profiler:
                profiler.disable()
                
                # Save profiling results
                import pstats
                from io import StringIO
                
                s = StringIO()
                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                ps.print_stats(30)  # Top 30 functions by time
                
                profile_text = s.getvalue()
                logger.info("Performance Profile:")
                for line in profile_text.split('\n')[:40]:  # First 40 lines
                    if line.strip():
                        logger.info(line)
                
                # Save full profile to file
                profile_path = Path(video_path).with_suffix('.prof')
                ps.dump_stats(str(profile_path))
                logger.info(f"Full performance profile saved to {profile_path}")
            
            # Print enhanced statistics
            stats = tracker.get_enhanced_tracking_statistics()
            logger.info("=== Enhanced Tracking Statistics ===")
            for key, value in stats.items():
                if isinstance(value, float):
                    logger.info(f"  {key}: {value:.3f}")
                else:
                    logger.info(f"  {key}: {value}")
            
            # Print geolocation results
            if hasattr(tracker, 'track_locations') and tracker.track_locations:
                logger.info("=== Geolocated Objects ===")
                for track_id, location in tracker.track_locations.items():
                    # Determine class name
                    class_name = f"Led-{150 if track_id % 2 == 0 else 240}"
                    logger.info(
                        f"Track {track_id} ({class_name}): ({location.latitude:.6f}, {location.longitude:.6f}) "
                        f"accuracy: {location.accuracy:.1f}m, reliability: {location.reliability:.2f}"
                    )
                
                # Calculate average accuracy
                avg_accuracy = sum(loc.accuracy for loc in tracker.track_locations.values()) / len(tracker.track_locations)
                avg_reliability = sum(loc.reliability for loc in tracker.track_locations.values()) / len(tracker.track_locations)
                
                logger.info(f"Average geolocation accuracy: {avg_accuracy:.1f} meters")
                logger.info(f"Average reliability: {avg_reliability:.2f}")
                
                if avg_accuracy <= 2.0:
                    logger.info("🎯 TARGET ACHIEVED: Sub-2-meter accuracy!")
                elif avg_accuracy <= 5.0:
                    logger.info("✅ Good accuracy achieved (< 5m)")
                else:
                    logger.warning("⚠️  Accuracy above target (> 5m)")
            else:
                if gps_data:
                    logger.info("No static objects found for geolocation")
                else:
                    logger.info("No GPS data available - tracking only mode")
        
        logger.info("🎉 Processing complete!")
        
        # Print output file information
        if stereo_mode:
            video_stem = Path(left_video).stem
        else:
            video_stem = Path(args.input_video).stem
            
        logger.info("=== Output Files ===")
        
        possible_outputs = [
            (f"{video_stem}.json", "Tracking results"),
            (f"{video_stem}.geojson", "Location data for GIS"),
            (f"{video_stem}.csv", "GPS data"),
            (args.output, "Visualization video") if args.output and not args.no_save_video else None
        ]
        
        for output_info in possible_outputs:
            if output_info and Path(output_info[0]).exists():
                file_size = Path(output_info[0]).stat().st_size / (1024 * 1024)
                logger.info(f"  📄 {output_info[1]}: {output_info[0]} ({file_size:.1f} MB)")
        
        # Print final performance metrics if optimizer available
        if optimizer:
            optimizer.print_performance_report()
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("❌ Processing interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"❌ Error during processing: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit(main())




================================================================
End of Codebase
================================================================
